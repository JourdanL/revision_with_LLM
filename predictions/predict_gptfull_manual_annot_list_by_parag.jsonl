{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.09", "instruction": "Rewrite the middle part of the paragraph to make it more better. Replace some words in the paragraph.", "revised_paragraph": "Revised paragraph: \"The task was designed with reference to a previous study [25]. Figure 3 illustrates a schematic representation of the task. A pink circular start area (251-pixel radius) and a green target were displayed against a gray backdrop. Initially, participants clicked on the start area, which resulted in the cursor being fixed at its center. Given that the starting position of the cursor may influence its trajectory and impact the effectiveness of pointing, we strictly maintained a consistent starting location throughout the trial. Participants clicked once more at the starting position to commence the trial. The start area then vanished as feedback indicating the trial had begun. Participants subsequently aimed for the target and concluded the trial with their next click. If participants successfully clicked on the target, we recorded the trial as a success; otherwise, it was recorded as a failure (error). Auditory feedback was provided in response to the outcome of each trial, indicating either success or failure.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.09", "instruction": "Slightly revise for readability, you can reorganise ideas in sentences if necessary.", "revised_paragraph": "Revised paragraph: The task was developed based on a previous study [25]. Figure 3 illustrates a schematic representation of the task. A pink circular start area with a radius of 251 pixels and a green target were displayed against a gray background. Participants initiated the task by clicking on the start area, which resulted in the cursor being fixed at the center of this area. To ensure consistency, the starting position of the cursor was strictly controlled, as we hypothesized that initial cursor placement could influence the path taken and overall pointing performance. After clicking the start area, participants clicked again at this fixed position to begin the trial. At this point, the start area disappeared, signaling the commencement of the task. Participants aimed for the target and concluded the trial with a subsequent click. A trial was considered successful if participants clicked correctly on the target; otherwise, it was classified as a failure (error). Sound feedback was provided to participants, indicating the outcome of their trial\u2014success or failure.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "instruction": "Rearrange the structure to make the structure clearer.", "revised_paragraph": "Revised paragraph: \"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The process begins with the extraction of feature maps from each uni-modal branch, which are then converted into vector representations using global average pooling over spatial dimensions. After obtaining these representations, we concatenate them and apply a linear transformation to derive a cross-modal context representation. Based on this context representation, we predict channel-wise weights for each modality through two independent fully connected layers. Lastly, these predicted weights are utilized to re-calibrate the channel-wise features for each modality.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "instruction": "Rewrite this paragraph completely to make it clearer.", "revised_paragraph": "Revised paragraph: \"We introduce the fusion module as a multi-modal transfer module (MMTM), following the framework established by Joze et al. (2020). The initial phase of MMTM involves compressing feature maps from each unimodal branch into vector representations using global average pooling across the spatial dimensions. Subsequently, these vector representations are concatenated, and a linear transformation is applied to generate a cross-modal context representation. Based on this context representation, we predict channel-wise weights for each modality through two separate fully-connected layers. These weights are then utilized to re-calibrate the channel-wise features for each modality.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "uJRtLYIOIq.e9xxGlB_c.00", "instruction": "Rewrite some formulations, giving preference to shorter ones.", "revised_paragraph": "Revised paragraph: \"Lemma 1 shows that the CPD kernels in Corollary 1 can be made positive definite (PD) by adding large enough constants, such as c \u2212 \u2225 x \u2212 x \u2032 \u2225 p for sufficiently large c. While Lemma 1 does not provide a specific c, the shift-invariant property of the Softmax normalization allows us to treat it as an under-determined constant in our positional embedding design, detailed in Eq. (1) of Section 4. For a given set of test points { x i } Ni =1, one can perform a geometric sequence search to find a c so that the matrix c + \u02dc k ( x i , x j )] Ni,j =1 is PD. Thus, while we do not require the value of c in this work, we can compute it if necessary, for instance, when deriving the feature map of c + \u02dc k.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "uJRtLYIOIq.e9xxGlB_c.00", "instruction": "Shorten this paragraph a bit while keeping all the informations.", "revised_paragraph": "Revised paragraph: \"Lemma 1 indicates that the CPD kernels in Corollary 1 can be made positive definite (PD) by adding sufficiently large constants, such as c \u2212 \u2225 x \u2212 x \u2032 \u2225 p for a sufficiently large c. Although Lemma 1 does not explicitly define c, the shift-invariant property of Softmax normalization allows us to treat it as an under-determined constant in our positional embedding design, as shown in Eq. (1) in Section 4. Given a set of test points { x i } Ni =1 , a geometric sequence search can be employed to find a c such that the N \u00d7 N matrix c + \u02dc k ( x i , x j )] Ni,j =1 is PD. Therefore, while we do not require the value of c in this work, we can compute it if necessary, such as when deriving the feature map of c + \u02dc k.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "xV0XmrSMtk.sYfR73R9z.02", "instruction": "Make this paragraph more concise by introducing acronyms earlier.", "revised_paragraph": "Revised paragraph: \"Discrete Variational Auto-Encoder (DVAE). In a DVAE (Rolfe, 2016), the network layers preceding the sampling solver function as the encoder, while those following it serve as the decoder. We focus on training a DVAE on the MNIST dataset, where the encoder maps the input image to a discrete distribution of k-hot binary vectors of length 20 in the latent space, and the decoder reconstructs the image.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "xV0XmrSMtk.sYfR73R9z.02", "instruction": "Introduce the acronym DVAE earlier to avoid repeating it.", "revised_paragraph": "Revised paragraph: \"Discrete Variational Auto-Encoders (DVAEs) are a class of models used for various tasks. In a discrete variational autoencoder (DVAE) (Rolfe, 2016), the network layers before the sampling solver represent the encoder, while the layers after the sampling solver function as the decoder. We consider the task of training a DVAE on the MNIST dataset, where the encoder maps the input image to a discrete distribution of k-hot binary vectors of length 20 in the latent space, and the decoder reconstructs the image.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "E2pFUCGYZ1.5hMS4Fg2b_b.00", "instruction": "Use \"supplemental materials\" instead of \"Appendix\"", "revised_paragraph": "Revised paragraph: \"ADO iterations in the Bayesian framework are shown in Sec. 3.3 and supplemental materials A.3. Finally, with the estimated posterior, the predictive uncertainty can be quantified by evaluating the identified system with an ensemble of parameters. To further improve the prediction capability, especially for chaotic systems, we propose to leverage data assimilation techniques, which is shown in the green box and discussed in Sec. 3.4 and supplemental materials A.5.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "E2pFUCGYZ1.5hMS4Fg2b_b.00", "instruction": "Lightly revise for readability.", "revised_paragraph": "Revised paragraph: \"ADO iterations within the Bayesian framework are detailed in Section 3.3 and Appendix A.3. After estimating the posterior, we can quantify predictive uncertainty by evaluating the identified system using an ensemble of parameters. To enhance prediction capabilities, particularly for chaotic systems, we propose utilizing data assimilation techniques. This approach is outlined in the green box and discussed in Section 3.4 and Appendix A.5.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.14", "instruction": "Remove unnecessary words and fix the words if they are not in the correct form", "revised_paragraph": "Revised paragraph: \"AGILE outperforms all baselines, demonstrating that relational knowledge of available actions is crucial for an optimal policy. RecSim and Real RecSys show consistent result trends with CREATE, though less pronounced for Real RecSys. Additionally, DQN performs worse than CDQN-based architectures because the top-K greedy list action building ignores list interdependence.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.14", "instruction": "Remove terms that might be considered biased. Make the writing more clear.", "revised_paragraph": "Revised paragraph: \"AGILE demonstrates superior performance compared to all baseline models, indicating that understanding the relationships among available actions is essential for developing an optimal policy. The result trends for RecSim and Real RecSys align with those of CREATE, although they are less prominent for Real RecSys. Furthermore, DQN underperforms compared to CDQN-based architectures due to the top-K greedy list action approach, which does not account for interdependencies among the actions.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ByZyHzZC-.HktKf7-AW.01", "instruction": "Remove unnecessary content in the last sentence.", "revised_paragraph": "Revised paragraph: \"Our work is also related to other research on the importance of noise in SGDs. The main inspiration for incorporating a learning rate schedule is to anneal noise (Bottou, 1998). Neelakantan et al. (2015) empirically observe that adding noise can aid the optimization of very deep networks. Our analysis allows us to derive the impact of gradient noise on the SGD stationary distribution. Moreover, our study provides insight into the recently proposed Cyclic Learning Rate (CLR) schedule (Smith, 2015). While CLR schedules demonstrate good optimization and generalization performance, they are primarily based on empirical observation rather than theoretical understanding. We show that a learning rate annealing can be replaced with an equivalent batch size schedule, suggesting that the benefits of CLR relate to the noise it induces, akin to mixing in Monte Carlo Markov Chain (MCMC) methods. In the MCMC context, annealing processes facilitate better mixing (Graham & Storkey, 2017).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ByZyHzZC-.HktKf7-AW.01", "instruction": "Make the last sentence shorter, only keep the main idea. Slightly concise this paragraph and improve the english.", "revised_paragraph": "Revised paragraph: \"Our work is also connected to existing research on the significance of noise in SGD, which has been previously examined. The primary motivation for implementing a learning rate schedule is to reduce noise (Bottou, 1998). Neelakantan et al. (2015) empirically demonstrate that introducing noise can enhance the optimization of very deep networks. Our analysis derives the effect of gradient noise on the stationary distribution of SGD. Furthermore, we provide insight into the recently proposed Cyclic Learning Rate (CLR) schedule (Smith, 2015). While CLR schedules have shown strong optimization and generalization performance, they are based more on empirical findings than theoretical foundations. We propose that learning rate annealing can be substituted with an equivalent batch size schedule, suggesting that CLR's benefits stem from the noise it introduces, akin to mixing in Monte Carlo Markov Chain (MCMC) methods. In this context, annealing processes improve mixing (Graham & Storkey, 2017).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.11", "instruction": "Make this paragraph a bit more fluid.", "revised_paragraph": "Revised paragraph: \"Design A accommodates two types of medication entries: drug and physical activity. Each drug entry in the calendar is clearly labeled with the name of the medication, followed by the dosage in brackets. The suffix -WF signifies that the drug should be taken with food. In contrast, physical activity entries are distinguished by a full-color fill, a dashed border, and a label that specifies the name of the activity. All other entries in the calendar are represented by rectangles filled with varying shades of grey.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.11", "instruction": "I want to rewrite the first sentence.", "revised_paragraph": "Revised paragraph: \"Design A accommodates two categories of medication entries: pharmaceuticals and physical activities. Each drug entry in the calendar is labelled with the name of the drug and suffixed with bracketed drug dosage. The suffix -WF indicates that the drug should be administered with food. Physical activity entries have a full-color fill, a dashed border, and a label indicating the name of the activity. All other calendar entries are represented with rectangles filled with different shades of grey.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.04", "instruction": "Add a sentence to explain the last sentence.", "revised_paragraph": "Revised paragraph: \"Because g is permutation-invariant, any ordering for the elements in Y has the same value for L. In the forward pass of the model, the arg min is approximated by running a fixed number of gradient descent steps. In the backward pass, Zhang et al. (2019) backpropagate through the gradient descent iterations in order to compute the gradients of the training objective with respect to the input vector z and the parameters \u03b8 of the encoder. This backpropagation through the iterations allows for the adjustment of the model parameters based on how the inputs influence the objective over the course of the gradient descent process.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.04", "instruction": "Improve the logical flow of the last half of the paragraph.", "revised_paragraph": "Revised paragraph: \"Because g is permutation-invariant, any ordering of the elements in Y yields the same value for L. During the forward pass of the model, the arg min is approximated by executing a fixed number of gradient descent steps. Subsequently, in the backward pass, Zhang et al. (2019) compute the gradients of the training objective with respect to the input vector z and the encoder parameters \u03b8 by backpropagating through the iterations of gradient descent.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "33RNh69fYq.kMvWVl725x.02", "instruction": "Remove some details on model training to make the paragraph more concise.", "revised_paragraph": "Revised paragraph: Setup. Anomaly detection aims to identify anomalous regions in images, with performance evaluated on the MVTec-AD dataset. Images are resized to 224 \u00d7 224, and feature maps are concatenated to form a 272-channel output. The AdamW optimizer is utilized, and the model is trained for 1000 epochs on 8 GPUs. The learning rate is initially set to 1 \u00d7 10 \u2212 4 and reduced after 800 epochs. Evaluation is conducted using 5 random seeds.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "33RNh69fYq.kMvWVl725x.02", "instruction": "Remove unnecessary details to shorten this paragraph.", "revised_paragraph": "Revised paragraph: Anomaly detection aims to identify anomalous regions in images, with performance evaluated on MVTec-AD. Images are resized to 224 \u00d7 224, and feature maps from EfficientNet-b4 stages 1 to 4 are concatenated to form a 272-channel feature map, which is reduced to 256 channels. We employ the AdamW optimizer with a weight decay of 1 \u00d7 10 \u2212 4 and train the model for 1000 epochs on 8 NVIDIA Tesla V100 GPUs with a batch size of 64. The initial learning rate is 1 \u00d7 10 \u2212 4, decreasing by 0.1 after 800 epochs. The neighbor size is set to 7 \u00d7 7, with jittering scale and probability of 20 and 1, respectively. Evaluation is conducted with 5 random seeds.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.21", "instruction": "Make this paragraph shorter and easier to understand", "revised_paragraph": "Revised paragraph: In the experiment shown in Fig. 5, we observed that RecSim made it easy to model item relationships, leading to AGILE not outperforming the ablations. Conversely, in CREATE and Grid World, AGILE successfully utilized action relationships for better decision-making. We hypothesized that the complex relations between actions in these environments, such as tools and activators in CREATE, introduce intricate action dependencies. For example, the appropriate activator-tool pairs depend on the context. Thus, we implemented specific pairings in RecSim, allowing clicks only when the correct items were recommended. Given the complexity of these action relations, AGILE was expected to perform better than the ablations. Figure 14 shows AGILE outperforming baselines, and in another figure, it consistently edges out the ablations. Additionally, in Fig. 16, AGILE's superior performance over AGILEGCN indicates that a GAT effectively models action relations, while AGILE's faster convergence compared to AGILE Only Action highlights the importance of intermediate list information in learning effective item pairings.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.21", "instruction": "Simplify the less essential ideas of the paragraph to make it more concise.", "revised_paragraph": "Revised paragraph: In the experiment depicted in Fig. 5, we found that RecSim effectively models item relationships, preventing AGILE from outperforming the ablations. However, AGILE excelled in CREATE and Grid World by effectively utilizing action relationships in decision-making. We hypothesized that complex action relations in the environment, such as tools and activators in CREATE, necessitate appropriate pairings based on situational context. To test this, we implemented predefined item pairings in RecSim that allow clicks only when the correct pairs are recommended. Given the complexity of action relations, AGILE is expected to exceed the performance of the ablations. Figure 14 illustrates AGILE outperforming the baselines, while Fig. 16 shows AGILE surpassing AGILEGCN, indicating that a GAT can effectively model action relations. Additionally, AGILE's faster convergence compared to AGILE Only Action emphasizes the importance of intermediate list information for learning effective item pairings.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NwOG107NKJ.0PPYM22rdB.02", "instruction": "Make the use of a citation in the second sentence correct. Update the third sentence.", "revised_paragraph": "Revised paragraph: \"Influence on the GitHub platform can be quantified by the number of followers, stars, mentions, quotes, and up-votes received from other users. Social network metrics such as centrality indicate how broadly influence extends (e.g., geographic interest) (Weber & Luo, 2014). Other features include project volume, documentation volume, presence of supporting files, code volume, and standard library usage. The popularity velocity can be measured by the ratio of total stars to project life. Few studies have examined the influence of user popularity, repository popularity, and triadic relationships in dynamic graphs.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NwOG107NKJ.0PPYM22rdB.02", "instruction": "Improve the readability of this paragraph.", "revised_paragraph": "Revised paragraph: \"Influence on the GitHub platform can be measured using various metrics, including the number of followers, stars, mentions, quotes, and upvotes received from other users. Social network metrics, such as centrality, provide insights into the extent of influence, including geographic interest (Weber and Luo, 2014). Additional factors that contribute to influence are project volume, documentation volume, presence of supporting files, code volume, and the usage of standard libraries. The speed at which a project gains popularity can be quantified by the formula (Total Stars / project life). However, only a limited number of studies have explored the influence of user popularity, repository popularity, and triadic relationships in dynamic graphs.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.05", "instruction": "Make the sentence understandable.", "revised_paragraph": "Revised paragraph: \"During training, the uni-modal branch primarily concentrates on its respective modality. The fusion modules create cross-modal contextual information by integrating data from the uni-modal branches and relay this information back to them. As a result, both \u02c6 y 0 and \u02c6 y 1 are influenced by information from both modalities. This leads us to develop two functions, f 0 and f 1, which correspond to the two uni-modal branches.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.05", "instruction": "Improve the wording of this paragraph.", "revised_paragraph": "Revised paragraph: \"During the training process, the uni-modal branches predominantly concentrate on their respective modalities. The fusion modules create cross-modal contextual information derived from the uni-modal branches and return it to them. Consequently, both \u02c6y 0 and \u02c6y 1 are informed by data from both modalities. This results in the emergence of two functions, f 0 and f 1, which represent the two uni-modal branches.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.05", "instruction": "Can you make my paragraph more concise?", "revised_paragraph": "Revised paragraph: \"Lightweight Image SR Models. Recent interest in lightweight image super-resolution (SR) models has led to the development of architectures that leverage recursive learning and channel splitting. Kim et al. introduced recursive learning in DRCN to reduce model size (2016), while Ahn et al. created a cascading mechanism in CARN (2018). Hui et al. proposed the lightweight information multi-distillation network (IMDN) (2019), and Luo et al. designed lattice blocks with butterfly structures (2020). Additionally, neural architecture search was implemented for image SR in FALSR (Chu et al., 2019). Model compression techniques such as knowledge distillation have also been explored, with He et al. proposing a feature-affinity approach for efficient image SR (2020), and Lee et al. training a teacher network to distill knowledge to a student (2020). Despite the progress of these lightweight networks, further investigation is needed to develop even more efficient models.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.05", "instruction": "Use shorter formulations and more direct language to make the paragraph more concise.", "revised_paragraph": "Revised paragraph: \"Lightweight Image SR Models. Recent years have seen increased interest in lightweight image super-resolution (SR) models. These models focus on creating lightweight architectures by leveraging recursive learning and channel splitting. Kim et al. first applied recursive learning in DRCN to reduce model size (Kim et al., 2016b). Ahn et al. introduced a cascading mechanism over a residual network in CARN (Ahn et al., 2018). Hui et al. developed a lightweight information multi-distillation network (IMDN) (Hui et al., 2019). Luo et al. created the lattice block with butterfly structures (Luo et al., 2020). Recently, neural architecture search was employed for image SR in FALSR (Chu et al., 2019a). Additionally, model compression techniques, such as knowledge distillation, have been explored for image SR. He et al. proposed feature-affinity knowledge distillation for efficient image SR (He et al., 2020), while Lee et al. trained a teacher network to transfer knowledge to a student (Lee et al., 2020). Despite the progress in lightweight networks, further exploration is needed for more efficient image SR models.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1-LZxvKX.rJ009I8RX.03", "instruction": "Edit the last sentence of this paragraph to make it shorter and remove the reference to Section 5.", "revised_paragraph": "Revised paragraph: \"Most closely related to our work are dynamic sparse reparameterization techniques that emerged only recently. Like ours, these methods adaptively alter, by certain heuristic rules, reparameterization during training. Sparse evolutionary training (Mocanu et al., 2018) used magnitude-based pruning and random growth at the end of each training epoch. NeST (Dai et al., 2017; 2018) iteratively grew and pruned parameters and neurons during training; parameter growth was guided by gradient and pruning by magnitude. Deep rewiring (Bellec et al., 2017) combined sparse reparameterization with stochastic parameter updates for training. These methods were mostly concerned with sparsifying fully connected layers and applied to relatively small and shallow networks. Our method is more scalable and efficient, effectively closing the generalization gap between training a compact sparse network and compressing a large deep CNN.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1-LZxvKX.rJ009I8RX.03", "instruction": "Rewrite the last sentence to make it more concise.", "revised_paragraph": "Revised paragraph: Most closely related to our work are dynamic sparse reparameterization techniques that emerged only recently. Like ours, these methods adaptively alter, by certain heuristic rules, reparameterization during training. Sparse evolutionary training (Mocanu et al., 2018) used magnitude-based pruning and random growth at the end of each training epoch. NeST (Dai et al., 2017; 2018) iteratively grew and pruned parameters and neurons during training; parameter growth was guided by gradient and pruning by magnitude. Deep rewiring (Bellec et al., 2017) combined sparse reparameterization with stochastic parameter updates for training. These methods primarily focused on sparsifying fully connected layers and were applied to relatively small and shallow networks. As discussed in Section 5, our method is more scalable and computationally efficient than these approaches, effectively closing the generalization gap between training a compact sparse network and compressing a large deep CNN for the first time.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aFWzpdwEna.MCecpd3utK.00", "instruction": "Make this paragraph more concise by rewriting the second half.", "revised_paragraph": "Revised paragraph: \"In this paper, we find that the performance of model-based offline reinforcement learning (RL) hinges on the balance between model return and uncertainty, a trade-off that is often difficult to determine without access to the deployment environment. To address this, we propose a bi-objective formulation for model-based offline RL and introduce Pareto policy pool (P3), an efficient method that generates a diverse pool of policies along the Pareto front, enabling the selection of optimal policies for varying environments. P3 effectively tackles the challenges of model exploitation and generalization to unseen states. In experiments on the D4RL benchmark, P3 significantly outperforms several recent baselines across multiple tasks, demonstrating its capability to learn generalizable policies even with low-quality pre-collected experiences.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aFWzpdwEna.MCecpd3utK.00", "instruction": "Make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"This paper demonstrates that the performance of model-based offline reinforcement learning (RL) heavily depends on the trade-off between model return and uncertainty, making it difficult to determine the optimal balance without access to the deployment environment. We propose a bi-objective formulation for model-based offline RL and introduce the Pareto policy pool (P3), an efficient method that generates a diverse set of policies along the Pareto front. This allows for the selection of the best policy suited for various environments. P3 effectively addresses key challenges in model-based offline RL, including model exploitation and generalization to unseen states. Our experiments on the D4RL benchmark reveal that P3 significantly outperforms several recent baseline methods across multiple tasks, highlighting its capability to learn generalizable policies even with low-quality pre-collected experiences.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "YkiRt7L93m.jgDbnUD7s.01", "instruction": "Please, make this paragraph easier to read.", "revised_paragraph": "Revised paragraph: \"We present a new concept of projection between sets of probability measures defined on Euclidean spaces. This definition applies to sets of general probability measures that have different supports and demonstrates strong computational and statistical advantages. It offers a unique solution to the projection problem under mild conditions and can accurately replicate important geometric features of the target measure, including its shape and support. To develop this concept, we utilize the 2-Wasserstein space, which encompasses all probability measures with finite second moments and is equipped with the 2-Wasserstein distance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "YkiRt7L93m.jgDbnUD7s.01", "instruction": "Rewrite and reorganise this paragraph to improve the english and be more convincing, let the last sentence as it is.", "revised_paragraph": "Revised paragraph: We introduce a novel notion of projection that operates between sets of probability measures supported on Euclidean spaces. This definition is applicable to sets of general probability measures with varying supports and demonstrates strong computational and statistical properties. Importantly, it offers a unique solution to the projection problem under mild conditions and is capable of reproducing key geometric characteristics of the target measure, including its shape and support. To develop this framework, we work within the 2-Wasserstein space\u2014defined as the collection of all probability measures with finite second moments, which is equipped with the 2-Wasserstein distance.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.08", "instruction": "Improve the English of this paragraph", "revised_paragraph": "Revised paragraph: \"The display device used was a VZ249HR monitor with a 23.8-inch diagonal and a resolution of 1920 \u00d7 1080 pixels, with a refresh rate configured to 75 Hz. We utilized a Logitech gaming mouse (model G-PPD-002WLr) with a sensitivity of 1600 DPI. The mouse cursor speed was adjusted to the midpoint of the slider in the operating system's control display, and the 'Enhance pointer precision' setting was enabled to align with the participants' typical preferences. The experimental system was implemented using Hot Soup Processor version 3.6 and operated in full-screen mode.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.08", "instruction": "Slightly revise the liking between phrases.", "revised_paragraph": "Revised paragraph: \"The display used in the experiment was a VZ249HR model with a 23.8\u201d diagonal and a resolution of 1920 \u00d7 1080 pixels, with the refresh rate set at 75 Hz. We utilized a Logitech gaming mouse (G-PPD-002WLr) with a sensitivity of 1600 DPI. To ensure consistency with participants' usual settings, the mouse cursor speed was adjusted to the midpoint of the slider in the control display, and the 'Enhance pointer precision' option was activated. The experimental system was executed using Hot Soup Processor version 3.6 and operated in full-screen mode.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.07", "instruction": "Can you paraphrase the last sentence?", "revised_paragraph": "Revised paragraph: \"We first provide a concise overview of the problem setting regarding deep CNN for image super-resolution. Additionally, we note the significant redundancy present within the networks. To achieve more efficient image super-resolution networks, we subsequently introduce the structure-regularized pruning (SRP) method as a means of compression.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.07", "instruction": "Rewrite the last sentence preferring passive voice over active.", "revised_paragraph": "Revised paragraph: \"We first give a brief view of the problem setting about deep CNN for image SR. It is also observed that there exists heavy redundancy in the networks. To pursue more efficient image SR networks, a structure-regularized pruning (SRP) method is proposed to compress them.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.02", "instruction": "Rewrite the following paragraph using a more formal language.", "revised_paragraph": "Revised paragraph: \"In recent years, deep learning has achieved significant advancements in the modeling of proteins, rendering data-driven methodologies increasingly attractive (Rives et al., 2019; Jumper et al., 2021). However, there remain considerable challenges in developing deep learning-based models for predicting the effects of mutations on protein-protein binding. A principal challenge is the limited availability of experimental data; only a few thousand protein mutations accompanied by annotations regarding changes in binding affinity are accessible to the public (Geng et al., 2019b). This scarcity impedes the implementation of supervised learning, as the insufficiency of training data often leads to overfitting. Additionally, the lack of structural information concerning mutated protein-protein complexes presents another obstacle. Mutations in amino acids within a protein complex induce alterations in sidechain conformations (rotamers) (Najmanovich et al., 2000; Gaudreault et al., 2012), which are critical to understanding changes in binding free energy. Nonetheless, the precise nature of conformational changes resulting from mutations remains poorly understood.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.02", "instruction": "Rewrite this paragraph for better readability.", "revised_paragraph": "Revised paragraph: \"Recently, deep learning has achieved remarkable success in protein modeling, making data-driven methods increasingly attractive (Rives et al., 2019; Jumper et al., 2021). However, several challenges persist in developing deep learning models to predict the effects of mutations on protein-protein binding. A primary challenge is the limited availability of experimental data; only a few thousand protein mutations with annotated changes in binding affinity are publicly accessible (Geng et al., 2019b). This scarcity of training data complicates supervised learning and increases the risk of overfitting. Additionally, the lack of structural information for mutated protein-protein complexes presents further difficulties. Mutating amino acids within a protein complex alters sidechain conformations (rotamers) (Najmanovich et al., 2000; Gaudreault et al., 2012), which can impact binding free energy. However, we currently lack detailed knowledge about how these conformational changes occur due to mutations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "SRquLaHRM4.vI2x5N-YHC.00", "instruction": "Remove any unessential information in this paragraph.", "revised_paragraph": "Revised paragraph: \"We solve this problem by introducing optimal transport theory and formulating feature sets as a discrete probability distribution with equal probability values. To reduce computational cost and avoid extra model parameters, we utilize a two-stage optimization strategy. In the first stage, we fix visual and text features and optimize the optimal transport problem using the fast Sinkhorn distances algorithm. In the second stage, we fix optimal transport parameters and back-propagate the gradient to learn the prompts with varying characteristics. Optimal transport aligns different visual features for each local prompt, making it more robust to visual misalignment and feature shifts. We conduct experiments on 11 datasets, following the standard settings of CLIP and CoOp, to evaluate our method in visual classification tasks. The significant improvements in results demonstrate that PLOT effectively learns representative prompts.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "SRquLaHRM4.vI2x5N-YHC.00", "instruction": "Please exclude the content related to optimal transport.", "revised_paragraph": "Revised paragraph: We address this problem by formulating the feature sets as a discrete probability distribution, in which each feature is assigned an equal probability value. To reduce computational cost and minimize additional model parameters, we employ a two-stage optimization strategy for prompt learning. In the first stage, we fix both visual and text features and optimize the feature alignment using a fast algorithm. In the second stage, we fix all other parameters and back-propagate the gradient to learn prompts with varying characteristics. Compared to conventional distance measures, our approach allows for better alignment of different visual features for each local prompt, thereby enhancing robustness to visual misalignment and accommodating feature shifts. We conduct experiments across 11 datasets, adhering to the standard settings of CLIP and CoOp, to evaluate our method. These experiments encompass visual classification tasks for various categories, including generic objects, scenes, and actions. The significant improvement in results demonstrates that our method can effectively learn representative and comprehensive prompts.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.20", "instruction": "Give me a more formal version of ths paragraph", "revised_paragraph": "Revised paragraph: \"Model Size and Mult-Add Operations. In comparison to recent studies such as MemNet, CARN, and IMDN, our proposed SRPN-L exhibits the lowest number of parameters. Additionally, we present the number of operations in terms of Mult-Adds, utilizing an output size of 3 \u00d7 1280 \u00d7 720. Notably, our SRPN-L demonstrates fewer Mult-Add operations than the majority of the methods considered. These comparisons suggest that the SRP framework effectively reduces both parameter count and operational complexity.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.20", "instruction": "Rephrase the text and change SRPN-L to SRPN-Lite", "revised_paragraph": "Revised paragraph: \"Model Size and Mult-Adds. In comparison to recent studies (e.g., MemNet, CARN, and IMDN), our SRPN-Lite exhibits the lowest number of parameters. We also present the count of operations in terms of Mult-Adds, with the output size set to 3 \u00d7 1280 \u00d7 720. Our SRPN-Lite performs fewer Mult-Adds than the majority of the methods being compared. These comparisons demonstrate that SRPN effectively reduces both parameters and operational requirements.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MnewiFDvHZ.iAYttXl-uH.00", "instruction": "Make paragraph more concise", "revised_paragraph": "Revised paragraph: \"\u2022 Fixed constraints g_t(p, x, q) = g(p, x, q), \u2200 t, are the same across time but need not be known when making decisions at round t. This scenario, as discussed in [14, 17, 29, 33], represents a special case of our model. \u2022 Adversarial constraints g_t(p, x, q) involve constraint functions that are unknown at round t and can be chosen arbitrarily and adversarially, as noted in [24, 20, 30].\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MnewiFDvHZ.iAYttXl-uH.00", "instruction": "Make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"\u2022 Fixed constraints g t p x q are equal to g p x q for all t, without requiring prior knowledge at decision round t, as seen in [14, 17, 29, 33]. \u2022 Adversarial constraints g t p x q are unknown during decision-making at round t and can be chosen arbitrarily, as described in [24, 20, 30].\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.08", "instruction": "check the wordings but keep the original content as much as possible", "revised_paragraph": "Revised paragraph: \"Nevertheless, since neural network estimators primarily update parameters using stochastic gradient methods, only a portion of the representation's distribution is available during each iteration. Consequently, one approach (Liuyi et al., 2018) is to compute the group discrepancy at the level of a stochastic mini-batch:\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.08", "instruction": "Improve the language to make it more formal.", "revised_paragraph": "Revised paragraph: \"Nonetheless, given that neural network estimators predominantly update parameters using stochastic gradient methods, only a limited subset of the representation's distribution is accessible during each iteration. Consequently, an efficient approach, as outlined by Liuyi et al. (2018), involves calculating the group discrepancy at the level of a stochastic mini-batch:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "YCmehaMzt.kHwUIOFr_.00", "instruction": "Change the idea of \"composition\" to \"ensemble\" if this paragraph. Fix any spelling mistake.", "revised_paragraph": "Revised paragraph: \"In addition, we combine EM and our proposed OPS together to craft a kind of unlearnable examples in an ensemble. Since OPS only modifies a single pixel, after being applied to EM-perturbed images, the imperceptibility can still be guaranteed. We evaluate the effectiveness of this ensemble method under different training strategies and find that it consistently maintains efficacy. Even when we use adversarial training and strong data augmentation like RandAugment, it is still able to degrade test accuracy to a relatively low level. Based on this property, we introduce CIFAR-10-S, where all the images are perturbed by the EM-OPS-composed noises. It can serve as a new benchmark to evaluate the ability to learn critical information under the disturbance of non-semantic representations.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "YCmehaMzt.kHwUIOFr_.00", "instruction": "Rewrite the first sentence. Improve English in this paragraph.", "revised_paragraph": "Revised paragraph: \"Furthermore, we integrate EM with our proposed OPS to create a novel set of composite unlearnable examples. Since OPS modifies only a single pixel, the imperceptibility is preserved when applied to EM-perturbed images. We evaluate the effectiveness of this compositing method across various training strategies and find that it consistently maintains efficacy. Even when employing adversarial training and robust data augmentation techniques such as RandAugment, it still manages to reduce test accuracy to a relatively low level. Building on this characteristic, we introduce CIFAR-10-S, a dataset where all images are perturbed by the EM-OPS-composed noises. This dataset serves as a new benchmark for assessing the capacity to learn critical information in the presence of composite non-semantic representations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "mS4xvgSiEH.i-a3xp3usm.00", "instruction": "Make this paragraph more concise.", "revised_paragraph": "Revised paragraph: \"Our work's key innovation lies in recognizing that learning a discrete representation enables structured search on two levels. To validate the necessity of the discrete latent space, we introduce two ablative baselines that replace the VQ-VAE with either a generic autoencoder or a VAE.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "mS4xvgSiEH.i-a3xp3usm.00", "instruction": "Make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"Our work's central novelty lies in demonstrating that learning a discrete representation enables structured search on two levels. To establish the necessity of the discrete latent space, we introduce two ablative baselines that replace the VQ-VAE with either a generic autoencoder or a VAE.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "sIqSoZ9KiO.KLlOZMoJ9G.01", "instruction": "Make sentence precise.", "revised_paragraph": "Revised paragraph: \"To investigate its performance impact in a more constrained setting, SDN was paired with a simpler VAE model than IAF-VAE. In addition to its implementation simplicity and reduced training time, a non-hierarchical VAE is more effective for representation learning due to its single stochastic vector, which allows for better control of the latent space. Specifically, the performance enhancements from using SDN were assessed through: (a) the evidence lower bound (ELBO) as a metric for how accurately the image distribution is approximated; and (b) the disentanglement of latent codes, using corresponding metrics to evaluate the influence of the SDN decoder on the quality of the learned latent representations.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "sIqSoZ9KiO.KLlOZMoJ9G.01", "instruction": "Rephrase the second sentence, mostly focusing on the second half.", "revised_paragraph": "Revised paragraph: \"To study its performance impact in a more constrained setting, SDN was paired with a VAE architecturally much simpler than IAF-VAE. Beyond its implementation simplicity and reduced training duration, a non-hierarchical VAE is better suited for representation learning due to its reliance on a single stochastic vector rather than a complex hierarchy of feature maps, facilitating improved control of the latent space. Specifically, the performance enhancements achieved with SDN were assessed in terms of: (a) the evidence lower bound (ELBO), serving as a proxy to evaluate the fidelity of the image distribution approximation; (b) the disentanglement of latent codes, using corresponding metrics to analyze the influence of the SDN decoder on the quality of the learned latent representations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.04", "instruction": "Be more concise.", "revised_paragraph": "Revised paragraph: \"SRCNN was followed by the introduction of the memory block in MemNet for deeper networks (Tai et al., 2017b). Lim et al. simplified the residual block and built deeper, wider networks with numerous parameters (Lim et al., 2017). Zhang et al. proposed the residual channel attention network (RCAN), incorporating an attention mechanism in image super-resolution (Zhang et al., 2018b). Liu et al. developed FRANet to enhance focus on critical spatial features (Liu et al., 2020). Subsequently, Zhang et al. introduced residual non-local attention for image restoration, including super-resolution (Zhang et al., 2019). Mei et al. proposed CSNLN by integrating local and non-local feature correlations (Mei et al., 2020). Despite achieving state-of-the-art results, these approaches often face challenges related to large model sizes and heavy computational demands.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.04", "instruction": "Use shorter formulations to make some sentences more concise.", "revised_paragraph": "Revised paragraph: \"SRCNN. Tai et al. introduced a memory block in MemNet (2017b) to deepen network structures. Lim et al. (2017) simplified the residual block (He et al., 2016) and built deeper, wider networks with many parameters. Zhang et al. (2018b) proposed the residual channel attention network (RCAN), the first to incorporate an attention mechanism in image super-resolution (SR). Liu et al. (2020) developed FRANet to enhance the focus of residual features on critical spatial content. Subsequently, Zhang et al. (2019) introduced residual non-local attention for image restoration, including SR. Mei et al. (2020) proposed CSNLN, combining local, in-scale/cross-scale non-local feature correlations and external statistics. While most of these methods achieved state-of-the-art results with deeper and wider networks, they also faced challenges with large model sizes (i.e., network parameters) and heavy computational demands (i.e., FLOPs).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.16", "instruction": "Make this paragraph better. Rewrite a sentece about the Grid World", "revised_paragraph": "Revised paragraph: In Figure 6, we provide a qualitative analysis of the agent's performance. (a) In the CREATE environment, at time t = 0, the action selected by AGILE's GAT, which focuses on the spring tool, also considers various other tools, notably covering all the tools that are activated with spring, including the trampoline. By time t = 1, the trampoline tool is chosen with significant attention directed towards the spring. This indicates that, in selecting the trampoline, the agent verifies the presence of the spring, thus enabling placement of the trampoline either before or after it. (b) In the Grid World scenario, we visualize the Summary-GAT ablation to examine how the summarizer effectively utilizes attention. We evaluate a situation where both dig-lava skills are available. The agent moves to the right, digs the orange lava, and approaches the pink lava. At this juncture, the Right action assigns a high weight to the Dig-Pink skill, assessing its presence prior to making the irreversible decision to enter the lava. Conversely, the Utility Policy consistently opts for the safer suboptimal route, lacking awareness of the dig-skills before encountering the lava. Ultimately, in the RecSim environment, we observe the agent successfully maximizing the CPR score by selecting 5 out of 6 items from the same primary category. In contrast, the Utility Policy struggles to identify the most prevalent category and fails to optimize CPR effectively.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.16", "instruction": "Improve the clarity in this paragraph.", "revised_paragraph": "Revised paragraph: \"In Figure 6, we provide a qualitative analysis of agent performance. (a) In the CREATE environment, at time t = 0, the action selected by the spring tool in AGILE\u2019s Graph Attention Network (GAT) focuses on various other tools, particularly those activated alongside the spring, such as the trampoline. At time t = 1, the trampoline tool is chosen, with significant attention directed towards the spring. This indicates that when selecting the trampoline, the agent verifies the availability of the spring, allowing for placement either before or after utilizing the trampoline. (b) In the Grid World scenario, we visualize the Summary-GAT ablation to examine how the summarizer employs attention. We analyze a situation in which both dig-lava skills are accessible. The agent moves to the right, digs the orange lava, and prepares to enter the pink lava. At this juncture, the Right action exhibits a strong attention weighting towards the Dig-Pink skill, assessing its availability prior to making the irreversible decision to enter the lava. In contrast, the Utility Policy consistently opts for a safe suboptimal path, as it lacks awareness of the dig-skills before entering the lava. Lastly, in RecSim, we observe that the agent maximizes the CPR score by selecting 5 out of 6 items from the same primary category. In comparison, the Utility Policy struggles to identify the most prevalent category and is less effective in maximizing the CPR.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aFzc_2nNz.WIdHkazOg.00", "instruction": "Make the ideas in these paragraph more modular and easier to understand.", "revised_paragraph": "Revised paragraph: \"Further improvement in model performance can be achieved by selecting \u03b3 independently for each training sample, as demonstrated by the Sample-Dependent Focal Loss (FLSD-53) introduced in [19]. However, FLSD-53 relies on heuristics and, as this paper shows, fails to generalize effectively. To address this issue, we propose a novel calibration-aware adaptive focal loss called AdaFocal. AdaFocal leverages the calibration properties of both focal loss and inverse-focal loss by adaptively adjusting \u03b3_t for different groups of samples. This adjustment is based on two factors: (1) the \u03b3_t from the previous training step, and (2) the magnitude of the model\u2019s underconfidence or overconfidence. We evaluate the performance of AdaFocal across various image recognition tasks and one natural language processing task, utilizing a range of network architectures. The results confirm that AdaFocal improves calibration while maintaining similar levels of accuracy. Additionally, models trained with AdaFocal exhibit a significant enhancement in their ability to detect out-of-distribution samples.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aFzc_2nNz.WIdHkazOg.00", "instruction": "Concise this academic paragraph a bit and smooth out the writing.", "revised_paragraph": "Revised paragraph: \"Further improvement is anticipated when \u03b3 is selected independently for each training sample, as demonstrated by the Sample-Dependent Focal Loss (FLSD-53) introduced in [19]. However, this approach relies on heuristics and, as shown in this paper, lacks strong generalizability. We propose a calibration-aware adaptive focal loss, termed AdaFocal, which leverages the calibration properties of focal and inverse-focal losses. AdaFocal adaptively modifies \u03b3 t for different sample groups based on (1) \u03b3 t from the previous step and (2) the model\u2019s under/over-confidence levels. We evaluate AdaFocal across various image recognition tasks and one NLP task, encompassing diverse network architectures, to demonstrate improved calibration without sacrificing accuracy. Furthermore, models trained with AdaFocal exhibit a notable enhancement in out-of-distribution detection capabilities.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "isfcBsgB-H.SBe0hOLmg9.00", "instruction": "Remove unnecessary details on specific numerical performance of the model. Link to https://github.com/hwwang55/MolR instead of supplementary material.", "revised_paragraph": "Revised paragraph: \"Line-Entry System or GNN-based (Graph Neural Networks) MRL methods often rely on SMILES strings as input, which struggle to effectively represent molecular structure, or place excessive emphasis on GNN architectures while overlooking their generalization capabilities. In response, we propose a method that utilizes chemical reactions to enhance molecule representation learning. Our approach focuses on maintaining the equivalence of molecules concerning chemical reactions in the embedding space, specifically by ensuring that the sums of reactant and product embeddings are equal for each chemical equation. This constraint effectively organizes the embedding space and enhances the generalization ability of molecule embeddings. Additionally, our model is compatible with any GNN as the molecule encoder, making it agnostic to specific GNN architectures. For further details and experimental code, please refer to our repository at https://github.com/hwwang55/MolR.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "isfcBsgB-H.SBe0hOLmg9.00", "instruction": "Make the second last sentence from the end of this paragraph more concise by removing too precise details.  For the last sentence, the code is now provided on github.", "revised_paragraph": "Revised paragraph: \"Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their generalization ability. Here we propose using chemical reactions to assist learning molecule representation. The key idea of our approach is to preserve the equivalence of molecules with respect to chemical reactions in the embedding space, i.e., forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. This constraint is proven effective to 1) keep the embedding space well-organized and 2) improve the generalization ability of molecule embeddings. Moreover, our model can use any GNN as the molecule encoder and is thus agnostic to GNN architectures. Experimental results demonstrate that our method achieves state-of-the-art performance across various downstream tasks, including significant gains in chemical reaction prediction, molecule property prediction, and graph-edit-distance prediction compared to the best baseline method. All experimental code is now available on GitHub.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "zzdwUcxTjWY.rVxmgW1FRK.00", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"Modern deep neural networks have achieved unprecedented success in the contexts for which they are trained; however, they often lack awareness of their limitations (Nguyen et al., 2015). Specifically, neural networks have demonstrated a propensity to assign high posterior probabilities to out-of-distribution (OOD) test inputs, which should not be predicted by the model. For instance, in the case of self-driving cars, an object detection model trained to recognize in-distribution objects (e.g., cars, stop signs) may produce a high-confidence prediction for an unseen object, such as a moose (see Figure 1(a)). This failure scenario raises significant concerns regarding model reliability and, more critically, may lead to catastrophic consequences when deployed in safety-critical applications.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "zzdwUcxTjWY.rVxmgW1FRK.00", "instruction": "Make this paragraph more formal and fitting to academic style.", "revised_paragraph": "Revised paragraph: \"Contemporary deep neural networks have attained remarkable success within the specific contexts for which they have been trained; however, they often exhibit a lack of awareness regarding their limitations (Nguyen et al., 2015). Notably, neural networks have demonstrated a tendency to assign high posterior probabilities to out-of-distribution (OOD) test inputs, which ideally should not fall within the prediction capabilities of the model. For instance, in the context of autonomous vehicles, an object detection model trained exclusively on in-distribution objects (e.g., cars, stop signs) may generate a confident prediction for an unseen object, such as a moose (refer to Figure 1(a)). This scenario highlights significant concerns regarding the reliability of such models, which, in safety-critical applications, could potentially result in catastrophic consequences.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "jP_amc4U0A.Y2t7AFVo5Z.00", "instruction": "Remove details which are unnecessary for the overall paragraph. Fix any spelling mistakes.", "revised_paragraph": "Revised paragraph: \"We proposed a new inference algorithm for distributions parametrized by normalizing flow models. The need for approximate inference is motivated by our theoretical hardness result for exact inference, which applies to invertible models. We presented an empirical evaluation of our method with results on a range of tasks and datasets. Overall, we believe that the idea of a pre-generator creating structured noise is a useful method for leveraging pre-trained generators to solve generative problems.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "jP_amc4U0A.Y2t7AFVo5Z.00", "instruction": "Correct and concise the two first sentences.", "revised_paragraph": "Revised paragraph: \"We proposed a new inference algorithm for distributions parameterized by normalizing flow models. The necessity for approximate inference arises from our theoretical hardness result for exact inference, which is particularly noteworthy given its applicability to invertible models. We also presented a detailed empirical evaluation of our method with both quantitative and qualitative results on a wide range of tasks and datasets. Overall, we believe that the concept of a pre-generator creating structured noise is a useful and general method for leveraging pre-trained generators to address new generative problems.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "tOMAf1V5dI.SNeLZ71pb5.00", "instruction": "Make this paragraph more concise.", "revised_paragraph": "Revised paragraph: \"CNN-based Architectures. Since AlexNet (Krizhevsky et al., 2012) won the ImageNet competition in 2012, CNN architectures have replaced hand-crafted features with automatic image feature extraction. The VGG network (Simonyan & Zisserman, 2015) introduced a series of 3 \u00d7 3 convolutions and fully connected layers, achieving notable success in image classification. ResNet (He et al., 2016) further improved performance by using residual connections to mitigate gradient vanishing, establishing the residual module as a key element in network design and later transformer- and MLP-based architectures. Enhancements like dilated convolution (Yu & Koltun, 2016) and deformable convolution (Dai et al., 2017) have advanced CNN capabilities, while EfficientNet (Tan & Le, 2019; 2021) applied neural architecture search for optimal structure design.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "tOMAf1V5dI.SNeLZ71pb5.00", "instruction": "Remove the sentence about the residual module. Make the paragraph more concise.", "revised_paragraph": "Revised paragraph: \"CNN-based Architectures. Since AlexNet (Krizhevsky et al., 2012) won the ImageNet competition, CNN-based architectures have increasingly replaced hand-crafted features with automatic image feature extraction. The VGG network (Simonyan & Zisserman, 2015) further refined this approach by utilizing only 3 \u00d7 3 convolutional and fully connected layers to achieve remarkable image classification performance. Following this, ResNet (He et al., 2016) introduced residual connections to mitigate gradient vanishing issues, leading to enhanced performance. Subsequent studies have improved convolution operations, incorporating methods such as dilated convolution (Yu & Koltun, 2016) and deformable convolution (Dai et al., 2017). EfficientNet (Tan & Le, 2019; 2021) integrates neural architecture search to identify optimal CNN structures.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.13", "instruction": "Be clear about references.", "revised_paragraph": "Revised paragraph: \"In this section, we evaluate three distinct aspects of our contributions: the usefulness of exclusive multiset-equivariance (Subsection 4.1), the differences between our implicit and automatic differentiation techniques (Subsection 4.2), and the applicability of iDSPN to a larger-scale dataset (Subsection 4.3). For comprehensive descriptions of the experimental procedures, refer to Appendix D; examples of inputs and outputs are presented in Appendix E. Additionally, we have made the code available for reproducing all experiments at the following GitHub repository: https://github.com/<redacted>/<redacted>.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.13", "instruction": "Lightly clarify the text. Add a reference to appendix at the end.", "revised_paragraph": "Revised paragraph: \"In this section, we assess three distinct aspects of our contributions: the usefulness of exclusive multiset-equivariance (Subsection 4.1), the differences between our implicit and automatic differentiation approaches (Subsection 4.2), and the applicability of iDSPN to a larger-scale dataset (Subsection 4.3). Detailed descriptions of the experimental procedures can be found in Appendix D, example inputs and outputs are presented in Appendix E, and we have made the code to reproduce all experiments available at https://github.com/<redacted>/<redacted>. For further details, please refer to the appendices.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "KUhhOtV2Yw.nPdxbHsbU.00", "instruction": "Prefer extended forms over abbreviations of words.", "revised_paragraph": "Revised paragraph: \"Generally, those statistical notions can be expressed in terms of different (conditional) independence statements between the involved random variables (Barocas et al., 2019): the average outcome variable is independent of the sensitive attribute (equation 5), the average outcome variable is independent of the sensitive attribute given the outcome variable (equations 6 to 7), and the outcome variable is independent of the sensitive attribute given the average outcome variable (equations 8 to 9). If our training set has no positive outcome for the demographic group where the sensitive attribute equals zero, that is, the set of instances for the average outcome variable where the sensitive attribute equals zero is empty, the true positive rate for this group will suffer. Therefore, we will likely not be able to satisfy, among other conditions, the equality of true positive rate as indicated in equation 6.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "KUhhOtV2Yw.nPdxbHsbU.00", "instruction": "Write the abbreviation in their full form.", "revised_paragraph": "Revised paragraph: \"Generally, those statistical notions can be expressed in terms of different (conditional) independence statements between the involved random variables (Barocas et al., 2019): the mean of y is independent of s (equation 5), the mean of y is independent of s given y (equations 6\u20137), and y is independent of s given the mean of y (equations 8\u20139). If our training set has no positive outcome for the demographic s = 0, that is, the set of instances where y equals 1 and s equals 0 is empty, the true positive rate for this group will suffer, and therefore we will likely not be able to satisfy, among others, the equality of true positive rate (equation 6).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "8_oadXCaRE.Kt4-LpYuM.01", "instruction": "Make this paragraph shorter by removing details.", "revised_paragraph": "Revised paragraph: \"The SoftHebb layer features a normalization of activations and an optional temperature-scaling mechanism, enabling a soft winner-take-all (WTA) approach. This setup allows us to demonstrate that the SoftHebb layer functions as a generative mixture model that minimizes Kullback-Leibler divergence from the input distribution through Bayesian inference. We back our theoretical findings with small-scale experiments on recognizing MNIST digits and Fashion-MNIST products, confirming that SoftHebb is more accurate than a hard-WTA model and effectively minimizes cross-entropy loss without access to labels. Additionally, the unsupervised WTA model outperforms a supervised perceptron in learning speed, accuracy, and robustness against noise and adversarial attacks like projected gradient descent, while also demonstrating properties that deflect such attacks.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "8_oadXCaRE.Kt4-LpYuM.01", "instruction": "Summarize the middle of the paragraph to make it shorter and more concise. Remove unnecessary details.", "revised_paragraph": "Revised paragraph: \"The SoftHebb layer features a simple normalization of activations and an optional temperature-scaling mechanism, enabling a soft winner-takes-all approach rather than selecting a single neuron. This framework facilitates the formal demonstration that SoftHebb acts as a generative mixture model, minimizing Kullback-Leibler divergence from the input distribution through Bayesian inference. While the theoretical results are complemented by small-scale experiments, we focus on the model's application in recognizing MNIST handwritten digits and Fashion-MNIST products. Our findings confirm that SoftHebb outperforms hard-WTA models in accuracy and effectively minimizes a cross-entropy loss function without direct access to labels. Additionally, due to its Bayesian properties, SoftHebb shows advantages over a supervised two-layer perceptron in learning speed, accuracy, and robustness against noise and adversarial attacks. Notably, the model also demonstrates deflection properties against adversarial attacks and generates object interpolations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.12", "instruction": "Replace every apparition of \"\"amino acids\" or \"amino acids in the protein complex\" by \"residues\"", "revised_paragraph": "Revised paragraph: \"The prior rotamers \u02dc \u03c7 j are inaccurate or unknown in many cases. For example, if we mutate some residues, the rotamers of the mutated residues are unknown, and the rotamers of residues nearby the mutated ones are inaccurate because they are affected by the mutation. The probability density is defined over the d-dimensional torus T D = ( S 1 ) D , and we show below our proposed flow-based architecture to model the density.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.12", "instruction": "Replace occurrences of amino acids by residues. Make this paragraph a lit bit more concise.", "revised_paragraph": "Revised paragraph: \"The prior rotamers \u02dc \u03c7 j are often inaccurate or unknown. For instance, mutating certain residues in a protein complex results in unknown rotamers for the mutated residues and inaccuracies in the rotamers of nearby residues due to the mutation. The probability density is defined over the d-dimensional torus T D = ( S 1 ) D, and we present our proposed flow-based architecture to model this density.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "p8yrWJS4W.eHA5NswPr.02", "instruction": "Make the concepts a bit more specific, such that some vague ideas are more clear.", "revised_paragraph": "Revised paragraph: \"Results. Figure 4 demonstrates that specific modifications\u2014such as the complete removal of articles from the assessed text\u2014have a negligible effect on the divergence measurements between our reference and test corpora across various thresholds (\u2206). Notably, text devoid of any articles is consistently rated as superior to outputs generated by the GPT-2 XL model according to most cluster-based divergence metrics. Furthermore, although this alteration clearly diminishes the text's overall fluency, it exerts less influence on divergence results compared to other modifications like truncating the text. This situation raises concerns, as an effective metric for text quality should prioritize fluency more heavily than superficial metrics, such as text length.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "p8yrWJS4W.eHA5NswPr.02", "instruction": "Revise the writing for better readability.", "revised_paragraph": "Revised paragraph: \"Results. Figure 4 illustrates that certain modifications, such as the complete removal of articles from the evaluated text, have minimal impact on the divergence between our reference and test corpora across various \u2206 values. In fact, text that lacks articles is often rated more favorably than GPT-2 XL based on most cluster-based divergence measures. Although this alteration does affect the text's fluency, its influence on divergence is less significant compared to other changes, such as text truncation. This outcome is arguably concerning; a metric for text quality should prioritize fluency over superficial statistics, such as text length.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "B1SkMaDvr.W2MCLgZGr.00", "instruction": "Make the ideas more concise.", "revised_paragraph": "Revised paragraph: \"This paper presents new generalization bounds for convolutional networks that account for the effects observed in earlier analyses of fully connected networks. Our bounds depend on the distance from the initial weights and the number of parameters, and they are 'size-free', meaning they are independent of the input pixel count and the dimensions of the hidden feature maps.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "B1SkMaDvr.W2MCLgZGr.00", "instruction": "Remove unnecessary details.", "revised_paragraph": "Revised paragraph: \"In this paper, we prove new generalization bounds for convolutional networks. Our bounds depend on the distance from the initial weights and the number of parameters, and they are independent of the number of pixels in the input and the dimensions of the hidden feature maps.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "W6V9WgTOwm.kXnvpTSqMp.00", "instruction": "Rename this section to a more approapiate title.", "revised_paragraph": "Revised paragraph: \"Implications of ID-Calibrated Ensembles for Robustness in Machine Learning. In this paper, we show that ID-calibrated ensembles, a simple method of calibrating a standard and robust model only on ID data and then ensembling them, can eliminate the tradeoff between in-distribution (ID) and out-of-distribution (OOD) accuracy on a wide range of natural shifts. We hope that this leads to more widespread use and deployment of robustness interventions.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "W6V9WgTOwm.kXnvpTSqMp.00", "instruction": "Rename the section \"Conclusion and Future Work\"", "revised_paragraph": "Revised paragraph: \"Discussion. In this paper, we demonstrate that ID-calibrated ensembles\u2014a straightforward approach to calibrating a robust model exclusively on in-distribution (ID) data and subsequently ensembling them\u2014effectively mitigates the tradeoff between ID and out-of-distribution (OOD) accuracy across various natural shifts. We anticipate that this finding will encourage broader adoption and implementation of robustness interventions.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.11", "instruction": "Generate a more formal version of this paragraph", "revised_paragraph": "Revised paragraph: \n\nOverview: The proposed methodology comprises three integral components. The primary element is the rotamer density estimator (RDE), a conditional normalizing flow that effectively models the probability density of sidechain conformations (rotamers) contingent upon the amino acid type and its environmental context (refer to Section 3.2). The subsequent component encompasses an algorithm designed to estimate the entropy associated with the distribution characterized by the normalizing flow (see Section 3.3). Lastly, we elucidate the methodology for utilizing the entropy of both mutated and wild-type protein-protein interfaces in their bound and unbound states to evaluate the change in binding free energy (\u2206\u2206G) consequent to mutation. Additionally, we demonstrate the application of neural networks to enhance the accuracy of \u2206\u2206G predictions using the unsupervised representations derived from the RDE (as detailed in Section 3.4). A protein-protein complex is defined as a multi-chain protein structure, which can be categorized into two distinct groups. Each group is constituted by a minimum of one protein chain, with each chain comprising multiple amino acids. In the context of a protein complex containing n amino acids, we assign numerical identifiers ranging from 1 to n. The two groups within the complex are represented by disjoint sets of indices A and B, where A, B \u2282 {1 . . . n}. Each amino acid is characterized by its type, position, orientation, and sidechain conformation. We denote the type, position, and orientation of the i-th amino acid (i \u2208 {1 . . . n}) as a_i \u2208 {1 . . . 20}, p_i \u2208 R^3, and O_i \u2208 SO(3), respectively. The sidechain conformation of the amino acid is referred to as a rotamer. Given that the conformational degree of freedom of the sidechain is governed by rotatable bonds, a rotamer can be sufficiently parameterized using torsional angles in relation to these bonds. The quantity of torsional angles varies from 0 to 4, depending on the specific amino acid type. For an amino acid characterized by d torsional angles, the k-th (k \u2208 {1 . . . 4}) torsional angle is denoted as \u03c7^(k)_i \u2208 [0, 2\u03c0). Collectively, all torsional angles are represented by the vector \u03c7_i = (\u03c7^(k)_i)_{k=1}^d. In geometric terms, an angle can be represented as a point on the unit circle S^1. Consequently, a vector comprising d angular values exists on the product of d unit circles, which is designated as the d-dimensional torus T^D = (S^1)^D.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.11", "instruction": "Replace all mentions of amino acid by 'residue'. Revise this paragraph for clarity.", "revised_paragraph": "Revised paragraph: \"Overview Our method consists of three parts. The core component is the rotamer density estimator (RDE), a conditional normalizing flow that models the probability density of sidechain conformations (rotamers) given the residue type and environments (Section 3.2). Next is the algorithm for estimating the entropy of the distribution parameterized by the normalizing flow (Section 3.3). Finally, we show how to use the entropy of the mutated and wild-type protein-protein interfaces at both bound and unbound states to estimate the change in binding free energy (\u2206\u2206G) upon mutation, and how to use neural networks to predict \u2206\u2206G more accurately using the unsupervised representations from the RDE (Section 3.4). A protein-protein complex is a multi-chain protein structure that can be divided into two groups. Each group contains at least one protein chain, and each chain consists of multiple residues. For a protein complex containing n residues, we number them from 1 to n. The two groups of the complex can be represented by two disjoint sets of indices A, B \u2282 {1 . . . n}. A residue is characterized by its type, position, orientation, and sidechain conformation. We denote the type, position, and orientation of the i-th (i \u2208 {1 . . . n}) residue as a_i \u2208 {1 . . . 20}, p_i \u2208 R^3, and O_i \u2208 SO(3) respectively. The sidechain conformation of the residue is called a rotamer. As the conformational degree of freedom of the sidechain is defined by rotatable bonds, a rotamer can be sufficiently parameterized by torsional angles with respect to the rotatable bonds. The number of torsional angles varies between 0 to 4 depending on the residue type. For a residue with d torsional angles, we denote the k-th (k \u2208 {1 . . . 4}) torsional angle by \u03c7^(k)_i \u2208 [0, 2\u03c0). Collectively, all the torsional angles are denoted by a vector \u03c7_i = (\u03c7^(k)_i)^(d)_k=1. Using the language of geometry, an angle can be represented by a point on the unit circle S^1. A vector consisting of d angular values resides on the product of d unit circles, known as the d-dimensional torus T^D = (S^1)^D.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.00", "instruction": "Replace all occurrences of SRPN-L with SRPN-Lite. Improve the english of this paragraph.", "revised_paragraph": "Revised paragraph: \"Lightweight image super-resolution (SR) networks have demonstrated promising results while maintaining a moderate model size. However, they are often deemed impractical or overlooked for scaling to larger networks. Concurrently, model compression techniques, such as neural architecture search and knowledge distillation, typically require significant computational resources. In contrast, network pruning is a cost-effective and efficient model compression approach. Nonetheless, applying pruning directly to SR networks presents challenges, particularly because filter pruning for residual blocks is notoriously difficult. To tackle these issues, we propose structure-regularized pruning (SRP), which imposes regularization on the pruned structure to ensure that the locations of pruned filters are consistent across different layers. Specifically, for layers connected by the same residual, we designate the filters with the same indices as unimportant filters. To transfer the expressive power of these unimportant filters to the remainder of the network, we apply L2 regularization to drive the weights toward zero, ensuring that their eventual absence results in minimal performance degradation. We implement SRP to train efficient image SR networks, resulting in a lightweight version, SRPN-Lite, and a deeper variant, SRPN. We conduct extensive comparisons with both lightweight and larger image SR networks. Both SRPN-Lite and SRPN demonstrate superior performance gains over recent methods, both quantitatively and visually.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.00", "instruction": "Replace SRPN-L by SPRN-Lite. Make the first and last sentence more fitting to the academic style.", "revised_paragraph": "Revised paragraph: \"Lightweight image super-resolution (SR) networks have demonstrated promising results while maintaining a moderate model size. Nevertheless, their adaptation to larger networks remains impractical or has been largely overlooked. Concurrently, model compression techniques, such as neural architecture search and knowledge distillation, often require substantial computational resources. In contrast, network pruning stands out as a cost-effective and efficient model compression method. However, its direct application to SR networks is challenging due to the complexities associated with filter pruning in residual blocks. To tackle these issues, we introduce structure-regularized pruning (SPRP), which incorporates regularization on the pruned structure to ensure alignment of pruned filter locations across various layers. Specifically, we designate filters with the same indices as unimportant across layers connected by the same residual. To facilitate the transfer of expressive power from these unimportant filters to the remainder of the network, we implement L2 regularization to drive their weights toward zero, thereby minimizing performance degradation resulting from their removal. We employ SPRP to develop efficient image SR networks, yielding the lightweight network SPRN-L and a significantly deeper variant, SPRN. Our extensive evaluations reveal that both SPRN-L and SPRN outperform contemporary lightweight and larger image SR networks, demonstrating superior performance gains in both quantitative and visual assessments.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.14", "instruction": "Make this paragraph much more concise.", "revised_paragraph": "Revised paragraph: \"A total of A participants used Design A, three used Design B, and ten used Design C. Participants could reference the bars indicating allowed medication intake times with Designs A and C. In Design B, a marker on the medication entry specified the allowed time; however, most participants struggled to complete the task and provided significant feedback. Five participants noted that Design B does not effectively support the task, with P1 stating, \u201cit doesn\u2019t show any other time of the day that you can take it.\u201d Three participants (P5, P11, P12) commented on the flexibility of rescheduling, with P9 noting, \u201c7am is a possibility because there is no other and no indication of conflicts.\u201d P10 mentioned that they would test for conflicts by moving the time, saying, \u201cI think I would just move it and see if a conflict came up.\u201d P8 suggested that the bar indicating allowed schedule times could misleadingly imply extended release, stating, \u201cThat would mean that it\u2019s an extended release. Warfarin is not an extended release.\u201d P10 and P6 argued that a medication requiring specific timing should not occupy a full hour on the calendar, with P10 expressing discomfort about the hour block labeled at 6am.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.14", "instruction": "Please write more concisely about design B.", "revised_paragraph": "Revised paragraph: \"Design B was less effective, as evidenced by participant feedback. While Designs A and C used bars to indicate allowed medication intake times, Design B relied on a marker, which participants found inadequate. Five participants noted that this design did not support the task, with one stating, \u201cit doesn\u2019t show any other time of the day that you can take it.\u201d Others felt that rescheduling was confusing, with comments like, \u201c7am is a possibility because there is no other and there is no indication of conflicts\u201d and \u201cI think I would just move it and see if a conflict came up.\u201d Additionally, some participants misinterpreted the marker as indicating extended release, contrary to the nature of the medication. One participant remarked, \u201cThat would mean that it\u2019s something that it\u2019s an extended release. Warfarin is not an extended release,\u201d while others expressed concern over the representation of specific medication times as occupying a full hour on the calendar, as noted by one participant: \u201cI really don\u2019t like this fact that it says 6am on the side and then it makes it a block of time.\u201d\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.09", "instruction": "Rewrite this paragraph to be more concise.", "revised_paragraph": "Revised paragraph: \"Semantic co-occurrence refers to the relationships between different objects, enabling pixel grouping and separation. We define semantic context as the union of object classes in each image. By leveraging semantic context, we can impose global regularization in the latent feature space, ensuring that pixel-wise feature embeddings are distinct from pixels in images without overlapping object categories, even in the absence of label locations.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.09", "instruction": "Split the last sentence and make it slightly shorter. Improve the english.", "revised_paragraph": "Revised paragraph: \"Semantic co-occurrence. Semantic context characterizes the co-occurrences of different objects and can be used to group and separate pixels. We define semantic context as the union of object classes in each image. Even without the location of labels, we can leverage semantic context to impose global regularization in the latent feature space. This means that the pixel-wise feature embeddings should be distinct from all other pixels in images that do not share overlapping object categories.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.14", "instruction": "Simplify the second sentence.", "revised_paragraph": "Revised paragraph: \"Results Table 2 shows our results. iDSPN approaches problem-solving effectively across various training set sizes due to its exclusive multiset-equivariance, similar to DSPN. In contrast, set-equivariant models fail to tackle this task since they cannot differentiate equal elements in the input from distinct elements in the output. This limitation holds true even for transformers with random position embeddings, which, like TSPN (Kosiorek et al., 2020) and Slot Attention (Locatello et al., 2020), utilize noise to differentiate elements. In comparison, transformers with position encoding and BiLSTM require at least 100 times more training samples to match the performance of iDSPN, due to their insufficient structural bias against relying on the order of elements, making them less sample-efficient. Furthermore, non-equivariant models are unlikely to gain significantly from increased training data, as they overfit with 1\u00d7 and 10\u00d7 but not with 100\u00d7.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.14", "instruction": "Rewrite the 2nd sentence to make it easier to read and less confusing.", "revised_paragraph": "Revised paragraph: \"Results Table 2 shows our findings. iDSPN effectively addresses the problem across various training set sizes due to its exclusive multiset-equivariance, similar to DSPN. As anticipated, the set-equivariant models struggle with this task because they are unable to differentiate between equal elements in the input when producing different elements in the output. This limitation also extends to transformers with random position embeddings, which, like TSPN (Kosiorek et al., 2020) and Slot Attention (Locatello et al., 2020), introduce noise to mitigate the exact equality of elements. In contrast, transformers that utilize position encoding and BiLSTM require at least 100 times more training samples to approach the performance of iDSPN. This inefficiency is attributed to their dependence on the order of elements, which hinders their sample efficiency. It is noteworthy that the non-equivariant models are unlikely to gain significant advantages from additional training data, as they experience overfitting at 1\u00d7 and 10\u00d7, but this does not occur at 100\u00d7.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.19", "instruction": "Rephrase the paragraph", "revised_paragraph": "Revised paragraph: \"In this experiment, twelve local university students from a different participant group than that of Experiment 1 took part. The participants had an average age of 22.3 years (SD = 1.67). All individuals demonstrated proficiency in mouse operation and utilized their dominant right hand.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.19", "instruction": "Revise this text to make it more clear.", "revised_paragraph": "Revised paragraph: \"Twelve local university students, distinct from the participant group in Experiment 1, took part in this experiment. The average age of the participants was 22.3 years (SD = 1.67). All participants demonstrated proficiency in mouse operation and used their dominant right hand.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.18", "instruction": "Use formal words in the last sentence.", "revised_paragraph": "Revised paragraph: \"Reweighting methods assign weights to individuals based on a balanced score to achieve globally balanced distributions, represented by the inverse propensity score (IPS) approach (Rosenbaum & Rubin, 1983a) and its doubly robust variant (Robins et al., 1994). Imai & Ratkovic (2014) and Fong et al. (2018) propose calculating the balancing score through an optimization problem. Kuang et al. (2017b) and Kuang et al. (2017a) further examine non-confounding factors within the covariates. Nevertheless, these methodologies exhibit high variance and are susceptible to issues arising from the presence of non-overlapping units.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.18", "instruction": "Reorder the last sentence arguments. Make this paragraph a bit more precise.", "revised_paragraph": "Revised paragraph: \"Reweighting methods ensure balanced distributions by weighting individuals based on a balanced score, utilizing the inverse propensity score (IPS) approach (Rosenbaum & Rubin, 1983a) and its doubly robust variant (Robins et al., 1994). Imai & Ratkovic (2014) and Fong et al. (2018) introduce an optimization problem for calculating the balancing score. Additionally, Kuang et al. (2017b) and Kuang et al. (2017a) address the inclusion of non-confounding factors in covariates. Nonetheless, these methods are prone to high variance and are susceptible to issues arising from non-overlapping units.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "X50LVGSli.jqJzurpUu.02", "instruction": "Fuse the last two sentences for conciseness.", "revised_paragraph": "Revised paragraph: \"In both the MVC and MC problems across the five datasets, Meta-EGN significantly outperforms EGN and RUN-CSP before and after the fine-tuning step, while also narrowing the gap against Gurobi9.5 on real small graphs, particularly outperforming it on RB graphs for the MC problem and on RB500 for the MVC problem.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "X50LVGSli.jqJzurpUu.02", "instruction": "Merge the two last sentences to make it shorter.", "revised_paragraph": "Revised paragraph: \"In both the MVC and MC problems across the five datasets, Meta-EGN significantly outperforms EGN and RUN-CSP, both before and after the fine-tuning step, while narrowing the gap from Gurobi9.5 on small real graphs; specifically, it surpasses Gurobi9.5 for the MC problem on RB graphs and the MVC problem on RB500.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.19", "instruction": "Split first sentence in two and delete the third sentence", "revised_paragraph": "Revised paragraph: \"Results: As shown in Figure 3, |d util(f)| increases along \u03bb. This increase is particularly evident when log(\u03bb) \u2265 \u22125; furthermore, |d util(f)| is positively correlated with R(f). We see |d speed| follows the same trend as |d util|. Again, it supports our choice of using the conditional learning speed to predict the conditional utilization rate.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.19", "instruction": "Exclude redundant expression.", "revised_paragraph": "Revised paragraph: \"Results: As shown in Figure 3, | d util ( f ) | increases with \u03bb, particularly when log( \u03bb ) \u2265 \u2212 5, and is positively correlated with R ( f ). Thus, stronger regularization results in a larger imbalance in utilization between modalities. This supports the second conjecture in \u00a73.2, which posits that increased regularization leads to a greedier multi-modal learning process. Additionally, | d speed | exhibits a similar trend as | d util |, further validating our choice of using conditional learning speed to predict conditional utilization rate.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.02", "instruction": "Remove unnecessary details.", "revised_paragraph": "Revised paragraph: \"First, it is the tightest topological property of state spaces studied in model checking and planning. Secondly, while computing the diameter for a factored transition system is \u03a0 P 2 -hard, practical methods exist to compute upper bounds on the diameter.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.02", "instruction": "Concise by removing unnecessary details.", "revised_paragraph": "Revised paragraph: \"First, it represents the tightest topological property of state spaces studied in model checking and planning literature. Secondly, while computing the diameter for a factored transition system is \u03a0 P 2 -hard (Hemaspaandra et al. 2010), practical methods exist to compositionally compute upper bounds on the diameter (Baumgartner et al. 2002; Rintanen and Gretton 2013).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Rd7TGMaUy.dkY5HcKwZ1.00", "instruction": "Rephrase the paragraph", "revised_paragraph": "Revised paragraph: \"In Markov Chain Monte Carlo (MCMC), a transition kernel is selected that maintains the invariance of the target distribution, and a Markov Chain is formed by continuously applying this kernel. The effectiveness of the MCMC method depends solely on the assumption of ergodicity. Beyond this requirement, the method is highly versatile; with sufficient computational effort, the Markov Chain can produce accurate samples from any target distribution, regardless of its complexity.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Rd7TGMaUy.dkY5HcKwZ1.00", "instruction": "Balance sentences length.", "revised_paragraph": "Revised paragraph: \"In MCMC, a transition kernel is chosen that leaves the target distribution invariant, allowing for the construction of a Markov Chain through repeated application of the kernel. This method relies solely on the assumption of ergodicity. Beyond this, MCMC remains a general approach; with sufficient computation, the Markov Chain can generate accurate samples from any target distribution, regardless of its complexity.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1BhqsOsB.1mgtDFRDc.03", "instruction": "Remove second part of last sentence and Replace \"grids\" by \"volumes\" ", "revised_paragraph": "Revised paragraph: \"We show a visualization of the occupancy volumes in Figure 9 (right). We visualize the occupancy volumes by converting them to heightmaps. This is achieved by multiplying each voxel\u2019s occupancy value by its height coordinate in the volume, and then taking a max along the volume\u2019s height axis. The visualizations show that the occupancy module learns to fill the \u201choles\u201d of the partial view.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1BhqsOsB.1mgtDFRDc.03", "instruction": "Delete unnecessary details. Make the text more formal.", "revised_paragraph": "Revised paragraph: \"Figure 9 (right) presents a visualization of the occupancy grids, transformed into heightmaps. This transformation involves multiplying each voxel's occupancy value by its height coordinate and subsequently determining the maximum along the height axis of the grid. The visualizations demonstrate that the occupancy module effectively fills the gaps in the partial view, thereby reconstructing a complete 3D scene.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.07", "instruction": "Change some words in this paragraph for the better ", "revised_paragraph": "Revised paragraph: \"The pointing operation towards the edge target leverages the cursor halting at the screen's boundary to facilitate pointing with reduced precision control. However, the pushing-edge behavior (pushing the cursor to the screen's edge) results in an increased travel distance for the mouse, consequently lengthening the movement duration. Yamanaka [25] introduced PE (Path Efficiency) to assess the effectiveness of cursor movements (Eq. 7).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.07", "instruction": "Improve the linking between ideas to make the paragraph more precise and readable.", "revised_paragraph": "Revised paragraph: \"The operation of pointing to an edge target utilizes the cursor's ability to halt at the screen's edge, allowing for the completion of pointing tasks without requiring precise control. Nevertheless, the behavior known as 'pushing-edge,' which involves moving the cursor to the edge of the screen, results in greater distance traveled by the mouse. This increase in distance inevitably extends the movement time required for the task. To quantify this, Yamanaka [25] introduced the concept of Path Efficiency (PE), which is used to evaluate the efficiency of cursor movements (see Eq. 7).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "9wfZbn73om.FhHH15YtKt.00", "instruction": "Use accurate words.", "revised_paragraph": "Revised paragraph: \"To this end, we introduce a specific type of (\u03c3, \u03b4)-measure to quantitatively evaluate data augmentation, followed by an upper bound for the downstream classification error rate informed by this measure. Our findings indicate that the generalization capability of contrastive self-supervised learning is contingent upon three critical factors: the alignment of positive samples, the divergence between class centers, and the concentration of augmented data. The first two factors pertain to the characteristics of learned representations, while the latter is dictated by the predetermined data augmentation strategy. Building on these theoretical insights, we examine two fundamental contrastive loss functions, InfoNCE and cross-correlation, to assess their compliance with the first two factors. Additionally, we conduct a series of experiments to explore the third factor and find a strong correlation between downstream performance and the concentration of augmented data.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "9wfZbn73om.FhHH15YtKt.00", "instruction": "Make the second half of this paragraph more precise and direct.", "revised_paragraph": "Revised paragraph: \"To this end, we define a specific (\u03c3, \u03b4)-measure to quantitatively assess data augmentation and derive an upper bound for the downstream classification error rate based on this measure. Our findings indicate that the generalization capacity of contrastive self-supervised learning depends on three key factors: the alignment of positive samples, the divergence of class centers, and the concentration of augmented data. The first two factors relate to properties of learned representations, while the third factor is contingent upon the predefined data augmentation strategy. Building on these theoretical insights, we investigate two established contrastive losses, InfoNCE and cross-correlation, to evaluate their effectiveness regarding the first two factors. Additionally, we conduct a series of experiments focused on the third factor, revealing a strong correlation between downstream performance and the concentration of augmented data.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "5t8NvKONr.tls-ZX2iE.01", "instruction": "Remove redundant details. Use more precise words.", "revised_paragraph": "Revised paragraph: \"We present a theorem that provides guidelines for neural network architecture in operator learning. It states that if the architecture can be substituted with a fully connected neural network, substantial complexity is necessary for training. Additionally, it establishes that the lower bound for a universal activation function serves as a strict limit on the number of parameters. An assumption is noted here, which is a sufficient condition for proving the theorem.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "5t8NvKONr.tls-ZX2iE.01", "instruction": "Make it more precise when necessary.", "revised_paragraph": "Revised paragraph: \"We now present the theorem that provides guidelines for neural network architecture in operator learning. It posits that if the entire architecture can be substituted with a fully connected neural network, substantial complexity is necessary for effective training. Furthermore, it establishes that the lower bound for a universal activation function serves as a precise limit on the number of parameters. Additionally, we highlight a key assumption that acts as a sufficient condition for the theorem's proof.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Byyb66j52G.hR5KKRfhQm.13", "instruction": "Add missing spaces.", "revised_paragraph": "Revised paragraph: \"On the contrary, random convolution can induce a growing difficulty by increasing the number of factors on a single background. Therefore, the generalization rapidly decreases after augmentation is interrupted when training with a single background because the learning direction toward generalization about various backgrounds is not helpful for training. On the other hand, the training can have help when their difficulty is solved by augmentation, such as in Figure 2(b) and Figure 2(c). Thus, in deep RL, neural networks maintain regularization when augmentation helps the training.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Byyb66j52G.hR5KKRfhQm.13", "instruction": "Improve the english in the paragraph, make it slightly more formal.", "revised_paragraph": "Revised paragraph: \"Conversely, random convolution can lead to an increased level of difficulty due to the proliferation of factors within a single background. Consequently, generalization tends to decline rapidly following augmentation interruptions during training with a single background, as the learning trajectory aimed at generalizing across diverse backgrounds proves ineffective. In contrast, training may benefit when the difficulty is alleviated through augmentation, as illustrated in Figures 2(b) and 2(c). Therefore, in deep reinforcement learning, neural networks uphold regularization when augmentation contributes positively to the training process.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.06", "instruction": "Summarize this:", "revised_paragraph": "Revised paragraph: Traditional methods for predicting mutation effects on protein binding can be categorized into two main types: biophysical and statistical methods. Biophysical methods concentrate on modeling inter-atomic interactions such as hydrogen bonding and electrostatic forces using mechanical and statistical energy functions. These approaches employ energy functions to sample conformations of mutated protein complexes and estimate changes in binding free energy (Schymkowitz et al., 2005; Park et al., 2016; Alford et al., 2017; Steinbrecher et al., 2017). In contrast, statistical methods rely on feature engineering, utilizing descriptors that encapsulate the geometric, physical, evolutionary, and motif properties of proteins to create models predicting the impact of mutations on binding free energy (Geng et al., 2019a; Zhang et al., 2020). However, traditional methods often encounter a trade-off between computational speed and accuracy due to the time-intensive sampling process. Moreover, the sophisticated energy functions and feature engineering methods are heavily dependent on human expertise, which restricts their ability to keep pace with the rapidly expanding databases of available protein structures.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.06", "instruction": "Make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"Traditional approaches to predicting the effects of mutations on protein binding are categorized into biophysical and statistical methods. Biophysical methods model inter-atomic interactions like hydrogen bonding and electrostatic forces to sample mutated protein conformations and predict changes in binding free energy (Schymkowitz et al., 2005; Park et al., 2016; Alford et al., 2017; Steinbrecher et al., 2017). In contrast, statistical methods utilize feature engineering, employing descriptors of geometric, physical, evolutionary, and motif properties to predict mutation effects on binding free energy (Geng et al., 2019a; Zhang et al., 2020). These traditional methods often trade off speed for accuracy due to time-intensive sampling processes and their reliance on human knowledge, which limits adaptation to the rapidly increasing availability of protein structures.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.01", "instruction": "Rewrite the latter half of this paragraph to make it more concise.", "revised_paragraph": "Revised paragraph: \"Electronic calendars have become instrumental in the management of daily activities [16\u201318]. They coordinate interactions among family or team members' schedules and convey the values behind scheduling priorities [19]. These calendars visualize temporal trends in daily activities such as energy use, fitness tracking, and work routines [20\u201322]. In healthcare, packaging with date features aids in managing prescriptions and improves adherence by prompting patients to follow dosing schedules [23]. We explore integrating prescription management into patients' existing calendars, a process that presents two key challenges. First, entries must be designed to distinguish between regular calendar events and prescription-related ones. Second, it is crucial to ensure patients can reschedule prescription entries while remaining within safe dosing parameters.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.01", "instruction": "Merge the two sentences in the middle  about integrating prescription magagement in a new shorter sentence. Improve the english in the last sentence.", "revised_paragraph": "Revised paragraph: \"Electronic calendars have become instrumental in the management of daily activities [16\u201318]. They are used to coordinate interactions among individual schedules of family or team members and can convey meaning and values behind the priorities of scheduling [19]. Calendars have been used to visualize temporal trends that include everyday activities such as energy use in workplaces, fitness tracking, and work routines [20\u201322]. In healthcare, unit-of-use packaging that incorporates a simple day or date feature has been utilized to help manage prescriptions and enhance adherence by prompting patients to follow the prescribed dosing schedule [23]. While these prescription managers often function as stand-alone tools, we explore the potential for integrating prescription management into the main calendar used by patients, which presents challenges such as distinguishing between regular calendar entries and those related to prescriptions, as well as ensuring that patients can reschedule prescription entries safely within the prescribed parameters.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.16", "instruction": "Restructured some sentences in this paragraph and merge the last two sentences ", "revised_paragraph": "Revised paragraph: \"Table 1 presents the results of model fitting using all 120 data points. In addition to comparing the adjusted R\u00b2 values, we included the Akaike information criterion (AIC) to account for the varying number of constants in the models [2]. A model is deemed better if it exhibits a higher R\u00b2 and a lower AIC. A difference in the AIC values greater than 2 is considered notable, while a difference exceeding 10 is deemed significant. For experiment 1, we established the starting position of the trials and excluded Eq. 5 and Eq. 6 from the comparison due to the inability to accurately obtain the values of B in these models.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.16", "instruction": "Improve the liking between phrases.", "revised_paragraph": "Revised paragraph: \"Table 1 presents the results of model fitting utilizing all 120 data points. In addition to comparing the adjusted R\u00b2 values, we also reported the Akaike Information Criterion (AIC) values to account for the varying number of parameters included in each model [2]. A model is deemed superior if it exhibits a higher R\u00b2 and a lower AIC. Notably, if the difference between AIC values exceeds 2, that difference merits consideration; a difference greater than 10 is regarded as significant. In Experiment 1, we established the starting position of the trials. It is important to note that we excluded Equation 5 and Equation 6 from this comparison, as the values of B in these models could not be accurately determined.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.02", "instruction": "Improve English in this paragraph. Explain more about the experiments", "revised_paragraph": "Revised paragraph: \"Pointing towards targets, such as buttons or icons, should be achieved both quickly and accurately. The two primary factors that influence movement time are the size of the target and the distance between the initial cursor position and the target. Research indicates that as the distance increases or the target size decreases, the movement time correspondingly increases. Additionally, the presence of distractors, which do not obscure the cursor, along the path to the target further contributes to prolonging movement time. For instance, when a notch is introduced in the user's path, the cursor may become obscured within the notch, leading to potential misalignments or loss of visual contact with the cursor. This scenario can result in increased movement times, as users may resort to careful navigation strategies to avoid the notch. In Experiment 1, longer movement times were observed because the cursor was concealed by the notch while users pointed to a target positioned at the top edge from another target also located at the top edge. The experiment's design highlighted the challenges associated with cursor visibility and precision in targeting under these specific conditions.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.02", "instruction": "Add a sentence to introduce the experiment. Improve the paragraph for better readability.", "revised_paragraph": "Revised paragraph: \"To investigate the factors influencing movement time in pointing tasks, we conducted an experiment focusing on the effects of target size, distance, and the presence of distractors. Pointing at targets, such as buttons or icons, requires both speed and accuracy. Two significant factors affecting movement time are the size of the target and the distance from the cursor's initial position to the target [11,19]. As the distance increases or the target size decreases, the movement time tends to increase. Additionally, the introduction of distractors, which do not obscure the cursor, along the path to the target also prolongs movement time [6,23]. When a notch is placed in the environment, users may misjudge the cursor's position within it or lose sight of the cursor, leading them to either avoid the notch or move the cursor cautiously around it, further increasing movement time. In Experiment 1, we recorded longer movement times when the cursor was obscured by the notch while attempting to point to a target at the top edge from another target, which was also positioned at the top edge.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.20", "instruction": "Heavily remove details from this paragraph to make it more concise.", "revised_paragraph": "Revised paragraph: Medication entries should include a marker indicating the exact time a reminder will be triggered, rather than a time range. Daily summaries of medications should display only their names and allow users to manage their visibility. In cases of conflicts, the focus should be on the position of conflicting entries rather than their connectors, with lesser emphasis on connecting lines. On-calendar conflict indicators should not serve as primary error notifications after rescheduling; users should be informed of potential conflicts beforehand. Participants preferred clear, dismissible error messages that detail conflicts from actions. When rescheduling medication entries, cells should be highlighted to indicate safety (green for safe, yellow for warning). While some participants expressed a desire to prevent scheduling that might cause conflicts, there could be scenarios where this is acceptable, in which case users should be guided on alternative moves to resolve conflicts through shading or outlining potential rescheduling options, along with customizable warning and error notifications.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.20", "instruction": "Please condense my paragraph related to medication conflicts.", "revised_paragraph": "Revised paragraph: Medication entries should indicate the precise time for reminders, rather than a time range. Daily summaries in the calendar should list medication names and allow users to modify their display. In conflict scenarios, the focus should be on the positioning of conflicting entries rather than the connectors, with less emphasis on connecting lines. The primary notification of conflicts should be proactive, alerting users before they arise, rather than relying predominantly on on-calendar error representation post-rescheduling. Participants recommended displaying normal, dismissible error messages that provide detailed information about any conflicts. When rescheduling, safe or unsafe cells should be highlighted in intuitive colors, like green for safe and yellow for warnings. While some participants preferred restrictions on scheduling that would avoid conflicts, there are circumstances where such flexibility is beneficial; in such cases, guidance should be offered on how to resolve conflicts by visually outlining potential rescheduling options and allowing users to set their preferences for warnings and error messages.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.14", "instruction": "Please, review this paragraph, modify only if necessary", "revised_paragraph": "Revised paragraph: \"In practice, we stack multiple bijective transformations to enable more complex transformations. The derivative of the composite function can be computed efficiently by applying the chain rule. At inference time, the inverse mapping \\( f^{-1}(y) \\) can also be computed efficiently (Rezende et al., 2020). To find the solution for \\( f^{-1}(y) \\), the first step is to locate the unique bin that contains \\( y \\). Assuming \\( y \\) belongs to the \\( k \\)-th bin, finding its corresponding \\( x \\) involves solving the root of the quadratic equation \\( f_k(x | x_{k,k+1}, y_{k,k+1}, \\delta_{k,k+1}) = y \\) within the interval \\([x_k, x_{k+1}]\\), where the solution is in closed form.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.14", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"In practice, we stack multiple bijective functions to facilitate more complex transformations. The derivative of the composite function can be computed efficiently by applying the chain rule. During inference, the inverse mapping f\u207b\u00b9(y) can also be calculated efficiently (Rezende et al., 2020). To determine the solution for f\u207b\u00b9(y), the first step is to locate the unique bin containing y. Assuming that y belongs to the k-th bin, finding the corresponding x involves solving the root of the quadratic equation f\u2096(x | x\u2096\u208b\u2081, y\u2096\u208b\u2081, \u03b4\u2096\u208b\u2081) = y within the interval [x\u2096, x\u2096\u208a\u2081], for which a closed-form solution exists.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "jzQGmT-R1q.ugUt9B3XaO.00", "instruction": "Rewrite the first sentence. Remove the example to make it shorter.", "revised_paragraph": "Revised paragraph: \"This section shows that neural networks progressively lose their ability to quickly fit new targets when trained on sequential prediction tasks. We find that capacity loss is particularly pronounced in sparse prediction tasks, where many of the target values the agent seeks to predict are zero. To study the effect of extreme capacity loss on performance in greater depth, we present a special case of the target-fitting capacity measure, which is efficient to compute and has the intuitive interpretation of measuring the ability of the representation to linearly disentangle states. We find evidence that agents with greater capacity according to this metric tend to achieve better performance in challenging environments from the Atari suite, where agents fail to match human performance, and that those suffering from representation collapse, according to this metric, fail to make any learning progress at all.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "jzQGmT-R1q.ugUt9B3XaO.00", "instruction": "Revise the first sentence in a more academic style. Remove unnecessary details.", "revised_paragraph": "Revised paragraph: This section demonstrates that neural networks exhibit a diminishing capacity to adaptively fit new targets when engaged in sequential prediction tasks. We observe that this capacity loss is particularly significant in sparse prediction contexts, where many target values are zero. To explore the impact of severe capacity loss on performance more thoroughly, we introduce a special case of the target-fitting capacity measure that is computationally efficient and intuitively gauges the representation's capability to linearly disentangle states. Our findings indicate that agents with greater capacity, as measured by this metric, tend to perform better in challenging environments within the Atari suite, where human performance remains unachieved, while those experiencing representation collapse according to this metric do not progress in learning.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.01", "instruction": "Replace the citation to (Tai et al., 2017b) with a citation to (Zhang et al., 2018c; 2020; 2021). Improve the english of this paragraph.", "revised_paragraph": "Revised paragraph: \"Image super-resolution (SR) is a fundamental task in computer vision that seeks to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart. Generally, image SR is considered ill-posed as it represents a many-to-one mapping problem. To address this challenge, numerous deep convolutional networks (CNNs) have been developed to facilitate accurate mapping from LR images to their corresponding HR targets (Dong et al., 2014; 2016; Kim et al., 2016b; Zhang et al., 2018c; 2020; 2021).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.01", "instruction": "Make the langage of this paragraph a bit more simple.", "revised_paragraph": "Revised paragraph: \"Image super-resolution (SR) is an important task in computer vision that aims to create a high-resolution (HR) image from a low-resolution (LR) image. Typically, image SR is tricky because it involves mapping many low-resolution images to one high-resolution image. To address this challenge, many deep convolutional neural networks (CNNs) have been explored to effectively translate LR images into their HR versions.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.25", "instruction": "Make this paragraph more fluid.", "revised_paragraph": "Revised paragraph: \"The rotamer density estimator (RDE) serves as a generative model specifically designed for predicting sidechain structures. By sampling from the estimated distribution, it effectively predicts sidechain conformations. In our experiments utilizing the RDE, we sample sidechain torsional angles\u2014referred to as rotamers\u2014from structures with sidechains removed in our test split of PDB-REDO. For each amino acid, we independently sample 10 rotamers and select the one with the highest probability as our final prediction. To evaluate the performance of RDE, we compare it against two baseline methods: Rosetta (fixbb) (Leman et al., 2020) and SCWRL4 (Krivov et al., 2009). As shown in Table 4, RDE demonstrates superior performance compared to both baselines across all four torsional angles, as indicated by lower angular errors. Additionally, detailed accuracy metrics for each amino acid can be found in Table 11 in the appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.25", "instruction": "Make the beginning of the paragraph more concise. Make the end of the paragraph more fitting to the academic style.", "revised_paragraph": "Revised paragraph: \"The rotamer density estimator (RDE) is a generative model for predicting sidechain conformations by sampling from an estimated distribution. In our test split of PDB-REDO, we utilized the RDE to sample sidechain torsional angles (rotamers) for structures with removed sidechains. For each amino acid, we sampled 10 rotamers independently and selected the one with the highest probability as the final prediction. We compared the performance of RDE with two baseline methods, Rosetta (fixbb) (Leman et al., 2020) and SCWRL4 (Krivov et al., 2009). Results, as shown in Table 4, indicate that RDE consistently outperforms the baselines across all four torsional angles regarding angular errors. Detailed per-amino-acid accuracy is further elaborated in Table 11 in the appendix.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "VRrAfKMSF8.g8rfar4U7.00", "instruction": "Remove unnecessary details.", "revised_paragraph": "Revised paragraph: \"In addition to AT and RND, several defenses have been proposed. DENT optimizes the model at test time to learn the AE distribution, while PNI injects noise during training to reduce sensitivity to input perturbations. TRS ensembles three models with low attack transferability and offers some protection against SQAs. However, as shown in Table 3, their performance in accuracy, calibration, and defense is inferior to that of AAA-linear in real cases. We also test the strong SQA QueryNet, which uses three architecture-alterable models to exploit DNN vulnerabilities. Despite its extensive use of testing samples that significantly impacts DNNs, AAA remains the most effective defense for the model.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "VRrAfKMSF8.g8rfar4U7.00", "instruction": "Delete unnecessary details, mostly in the two first sentences.", "revised_paragraph": "Revised paragraph: \"Diverse defenses have been proposed, including DENT, which optimizes the model at test-time to learn the AE distribution, and PNI, which injects noise during training to reduce sensitivity to input perturbations. TRS ensembles three models with low attack transferability and provides some protection against SQAs, though they do not match AAA-linear in accuracy, calibration, and defense performance, as shown in Table 3. We also test a strong SQA, QueryNet, which employs three architecture-alterable models to compromise DNNs. Despite QueryNet's effectiveness due to its use of large-scale testing samples, AAA remains the best defense for protecting the model.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.13", "instruction": "Please, remove the clarifications that do are not necessary for the development of the idea:", "revised_paragraph": "Revised paragraph: Lightweight Network. We revise the EDSR baseline by removing the final Conv layer to save parameters. Similar to IMDN, the reconstruction is performed within the pixel-shuffle layer. We set the channel number in the revised EDSR baseline to 256 and then prune it to 45. For \u00d7 2, we reduce the number of parameters from 19.5M to 609K, naming the compressed model SRPN-L.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.13", "instruction": "Remove Hui et al. citation and improve writing of the paragraph", "revised_paragraph": "Revised paragraph: \"Lightweight Network. First, we revise the EDSR baseline, which consists of 16 residual blocks (Lim et al., 2017), by removing the final convolutional layer to reduce the number of parameters. The reconstruction is performed within the pixel-shuffle layer (Shi et al., 2016). We initially set the channel number in the revised EDSR baseline to 256 and subsequently prune it to 45. For the \u00d72 scaling factor, we successfully reduce the number of parameters from 19.5 million to 609,000, with the compressed model designated as SRPN-L.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.18", "instruction": "Improve the English in this paragraph by choosing better words", "revised_paragraph": "Revised paragraph: \"We employed different equipment for Experiments 1 and 2; however, this variation did not significantly impact the conclusions of the study. The hardware utilized included a desktop PC equipped with an Intel Core i9-12900KF processor, a GeForce RTX 3070 Ti graphics card, and 32GB of RAM, operating on Windows 10 Home. The display, manufactured by AOPEN, was a model 25XV2QFbmiiprx with a 24.5-inch diagonal and a resolution of 1920 \u00d7 1080 pixels, which was configured with a refresh rate of 360 Hz. Additionally, we used a Logitech Gaming Mouse (G300s) with a sensitivity of 1600 DPI. The mouse cursor speed was adjusted via the operating system settings to the midpoint of the slider in the control-display, and the \u201cEnhance pointer precision\u201d option was enabled to align with the participants' customary preferences. The experimental system was implemented using Hot Soup Processor 3.6 and operated in full-screen mode.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.18", "instruction": "Improve the liking between phrases.", "revised_paragraph": "Revised paragraph: \"In experiments 1 and 2, we utilized different apparatus; however, this variation did not significantly affect the conclusions of this study. Specifically, we employed a desktop PC equipped with an Intel Core i9-12900KF processor, a GeForce RTX 3070 Ti graphics card, and 32GB of RAM, running on Windows 10 Home. The display used was an AOPEN model (25XV2QFbmiiprx), featuring a diagonal size of 24.5 inches and a resolution of 1920 \u00d7 1080 pixels, with a refresh rate set to 360 Hz. For input, we used a Logitech Gaming Mouse (G300s) with a sensitivity of 1600 DPI. The mouse cursor speed was adjusted via the operating system settings to the midpoint of the slider in the control-display, and the \u201cEnhance pointer precision\u201d option was enabled to align with the participants' usual configurations. The experimental system was implemented using Hot Soup Processor 3.6 and was operated in full-screen mode.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "dD-sDO1KaC.N7AsVzdCxV.00", "instruction": "Remove information on why the assumption might not hold.", "revised_paragraph": "Revised paragraph: \"Backdoor-based model watermarking relied on an assumption that the trigger matches hidden backdoors contained in the suspicious model. In this section, we verify this limitation.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "dD-sDO1KaC.N7AsVzdCxV.00", "instruction": "Make the second sentence much shorter, only keep the main idea.", "revised_paragraph": "Revised paragraph: \"Backdoor-based model watermarking relies on the assumption that the trigger matches hidden backdoors in the suspicious model. However, this assumption may not hold if the backdoor changes during the stealing process. In this section, we verify this limitation.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.18", "instruction": "Make this paragraph more concise by using more direct formulations", "revised_paragraph": "Revised paragraph: \"The only exclusively multiset-equivariant prediction model is DSPN (Zhang et al., 2019; Huang et al., 2020). While DESP (Zhang et al., 2021) is also multiset-equivariant, it is not a standard set predictor. DESP employs the Jacobian of sorting to achieve diverse sampling by learning set energies rather than using a traditional set loss, which is why we do not include it in our experiments.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.18", "instruction": "Make the paragraph shorter but don't touch at the first sentence.", "revised_paragraph": "Revised paragraph: \"The only set prediction model we are aware of that can be exclusively multiset-equivariant is DSPN (Zhang et al., 2019; Huang et al., 2020). While DESP (Zhang et al., 2021) is also exclusively multiset-equivariant, it focuses on diverse sampling through learning set energies rather than standard set prediction, making comparison in our experiments unnecessary.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "txe2sPPkO.id6Xr1pUq.00", "instruction": "Rewrite the middle sentence of this paragraph to make it clearer.", "revised_paragraph": "Revised paragraph: \"In this section, we discuss how SafeNet can be instantiated in practice. There are two aspects the data owners need to agree upon before instantiating SafeNet: i) The MPC framework used for secure training and prediction phases and ii) the parameters in Theorem 6 to achieve poisoning robustness. The MPC framework can be established by deciding on three key components: the total number of outsourced servers N that will participate in the MPC, the maximum number of servers T that may be corrupted, and the type of adversary involved (either semihonest or malicious in the SOC paradigm). The owners then agree upon a filtering threshold \u03d5 and the number of poisoned owners t that can be tolerated. Once these parameters are chosen, the maximum allowed error probability of the local models trained by the honest owners is determined based on Lemma 5.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "txe2sPPkO.id6Xr1pUq.00", "instruction": "Rewrite the long sentence in the middle sentence to improve clarity.", "revised_paragraph": "Revised paragraph: In this section, we discuss how SafeNet can be instantiated in practice. There are two key aspects that data owners must agree upon before implementing SafeNet: (i) the multi-party computation (MPC) framework to be utilized for secure training and prediction phases, and (ii) the parameters outlined in Theorem 6 aimed at achieving poisoning robustness. The agreement on the MPC framework involves determining the total number of outsourced servers, denoted as N, participating in the MPC, the number of potentially corrupted servers, designated as T, and the nature of the adversary, which can be either semihonest or malicious within the SOC paradigm. Subsequently, the owners must also settle on a filtering threshold, represented as \u03d5, and the maximum number of poisoned owners, denoted as t, that the system can tolerate. Once these parameters are established, the maximum allowed error probability of the local models trained by the honest owners can be determined based on Lemma 5.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NwOG107NKJ.0PPYM22rdB.03", "instruction": "Remove the information about the code. And remove the last sentence.", "revised_paragraph": "Revised paragraph: TERGM models estimated within the Markov chain assumption are typically incapable of generating and reproducing realistic dynamics observed in real-world online social networks. We hypothesized that increasing the model\u2019s capacity to describe triadic network properties would reduce the error between the model and empirical observations. We propose TTERGM here to sequentially predict network probabilities by integrating the dynamics between influencers and followers. Figure 2 shows the framework of TTERGM that has five major components - data collection module, network processing module, feature extraction module, pattern analysis module, and a generative network module. The data collection module is discussed in Section 3.3.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NwOG107NKJ.0PPYM22rdB.03", "instruction": "Remove the mentions to the code and to other sections.", "revised_paragraph": "Revised paragraph: TERGM models estimated within the Markov chain assumption are typically incapable of generating and reproducing realistic dynamics observed in real-world online social networks. We hypothesized that increasing the model\u2019s capacity to describe triadic network properties would reduce the error between the model and empirical observations. We propose TTERGM here to sequentially predict network probabilities by integrating the dynamics between influencers and followers. TTERGM comprises five major components: data collection module, network processing module, feature extraction module, pattern analysis module, and a generative network module.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.08", "instruction": "Make this paragraph considerably more concise. Remove any unnecessary details that are not essential for the main point of the paragraph.", "revised_paragraph": "Revised paragraph: We propagate labels within visually coherent regions, leveraging the correlation between visual and semantic similarity. To achieve over-segmentation, we use the HED contour detector (Xie & Tu, 2015), pre-trained on the BSDS dataset (Arbelaez et al., 2010), following the gPb-owt-ucm method. This approach utilizes bottom-up segmentation techniques to capture local and global appearance affinity. For learning pixel embeddings at pixel i, we define positive segments as i's own segment and negative segments as all others, denoted as V+ and V\u2212, respectively. This formulation facilitates the learning of embeddings grounded in low-level visual cues. We restrict consideration of segments to the same image as i for positive and negative samples, aligning contour-based over-segmentations with K-Means clustering segmentations as per SegSort (Hwang et al., 2019).", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.08", "instruction": "This paragraph is too long, make it almost 50% shorter but keep the important informations.", "revised_paragraph": "Revised paragraph: \"Low-level image similarity involves propagating labels within visually coherent regions, as visual similarity often aligns with semantic similarity. To achieve over-segmentation, we utilize the HED contour detector (Xie & Tu, 2015), pre-trained on the BSDS dataset (Arbelaez et al., 2010), following the gPb-owt-ucm method. These bottom-up segmentation techniques account for local and global appearance affinity without relying on semantic information. Some segments may include pixels from different categories. For learning pixel embedding at pixel i, we define positive and negative segments corresponding to i\u2019s own segment and all other segments, denoted as V + and V \u2212, respectively. This approach aids in learning embeddings that honor low-level visual cues, considering only segments within the same image as positive and negative samples. We align contour-based over-segmentations with K-Means clustering segmentations using the SegSort method (Hwang et al., 2019).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.23", "instruction": "Make the third sentence shorter and easier to understand", "revised_paragraph": "Revised paragraph: \"The user model takes as input user information (i.e., a concatenation of user attributes and a sequence of user interactions) and a set of item embeddings in the list. The user information is processed by a single-layer gated recurrent network (GRU (Cho et al., 2014)) and then passed to a 2-layer MLP to create a compact representation of the state. Similarly, the item embeddings are processed by a distinct GRU network with a separate initialization to derive the list-embedding. Finally, a 2-layer MLP combines the state-embedding and list-embedding to generate item scores, which are converted to individual click likelihoods using a sigmoid function.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.23", "instruction": "Simplify the convoluted sentences to make the paragraph more concise.", "revised_paragraph": "Revised paragraph: \"The user model inputs user information, which includes a combination of user attributes and a sequence of interactions, along with a set of item embeddings. This user information is processed through a single-layer gated recurrent network (GRU) followed by a two-layer multi-layer perceptron (MLP) to create a compact state representation. The item embeddings are processed using a separate GRU network, initialized differently from the user information model. Finally, a two-layer MLP takes the concatenated state and list embeddings to output item scores, which are then transformed into individual click likelihoods using a sigmoid function.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "fJhx73ErBg.NeKLbmOxG8.03", "instruction": "Make this paragraph easier to read, remove unnecessary details if needed", "revised_paragraph": "Revised paragraph: Temporal logic (TL)-guided policy learning inspires our research. In this approach, TL specifies the ego agent\u2019s desired high-level behavior and generates rewards. Recent surveys, such as those by [17, 18, 19], review the use of TL in reinforcement learning (RL), although exploration challenges remain. Authors [20, 21] have proposed learning finite state automata from demonstrations to aid planning using value iteration networks, thus avoiding exploration issues. However, manually creating a TL formula for satisfactory behavior can be tedious. To address this, some work has focused on making elements of the formula learnable from data; for instance, [22] learns linear temporal logic (LTL) formulas from demonstrations. Given the connection between TL and automata, [24] introduced a method to learn reward machines from demonstrations. A shared feature of these methods is their reliance on propositions, which require either high-level demonstrations or a mapping from low-level states to propositions, limiting their applicability to discrete and finite state-action spaces. In contrast, our approach utilizes signal temporal logic (STL), allowing us to learn from continuous demonstration trajectories that are more common in robotic systems. Additionally, the LogicRiskNet we propose is designed to be learnable from demonstrations.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "fJhx73ErBg.NeKLbmOxG8.03", "instruction": "Summarize the last third of this paragraph in one sentence. Smooth out the writing.", "revised_paragraph": "Revised paragraph: \"Temporal logic (TL)-guided policy learning is an area that we have taken inspiration from. In this area, TL is often used to specify the ego agent\u2019s desired high-level behavior and generate rewards. The authors of [17, 18, 19] provide surveys of recent work on the use of TL in RL. The exploration problem still exists in these methods. The authors of [20, 21] learn finite state automata from demonstrations and use them to guide planning with the value iteration network, which avoids exploration. It can sometimes be tedious to manually design a TL formula that yields satisfying behaviors; hence, research has been directed towards making components of the formula learnable from data. For instance, [22] learns linear temporal logic (LTL) formulas from demonstrations, and given the close relationship between TL and automata, the authors of [24] propose a method for learning reward machines (an automata-like reward representation) from demonstrations. A commonality among these methods is that the LTL they employ operates on propositions (binary variables with values true or false), necessitating either high-level demonstrations (proposition traces) or a mapping from low-level states to propositions, thereby restricting these approaches to discrete and finite state and action spaces. In contrast, our method utilizes signal temporal logic (STL), which operates on continuous signals, allowing us to learn from continuous demonstration trajectories that are more readily available in robotic systems; additionally, the proposed LogicRiskNet is learnable from demonstrations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "QSAQjBO0aj.srl-4uM-pl.00", "instruction": "Make this paragraph more concise.", "revised_paragraph": "Revised paragraph: \"Trigger Feature Hypothesis: We propose that trigger features are sparsely encoded in a limited number of channels, whereas clean image features require broader encoding across many channels for effective classification. This contrasts with typical data features, which are evenly distributed across channels, suggesting differing behaviors in specific situations and highlighting our main technical contribution. Further illustrations of trigger features are available in App. A3.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "QSAQjBO0aj.srl-4uM-pl.00", "instruction": "Make this paragraph a bit shorter.", "revised_paragraph": "Revised paragraph: \"Trigger Feature Hypothesis We hypothesize that trigger features are sparsely encoded in a few channels, while clean image features require encoding across many channels for effective classification. This contrasts with normal data features, which are typically distributed evenly across channels, suggesting that these feature types may behave differently in certain contexts. Additional illustrations of trigger features are available in App. A3.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "RPX7thbt2Mv.PdsbQ4ckYr.01", "instruction": "Remove the fourth sentence", "revised_paragraph": "Revised paragraph: We use the medium-v2, medium-replay-v2, and medium-expert-v2 datasets to construct the expert demonstrations and the unlabeled dataset for each environment in the OpenAI Gym MuJoCo locomotion tasks. For the expert demonstrations, we choose the best episodes from the D4RL dataset based on the episodic return. To obtain the unlabeled dataset, we discard the original reward information in the dataset. We then run OTR to label the dataset based on the optimal coupling between the unlabeled episodes and the chosen expert demonstrations. Afterward, we proceed with running the offline RL algorithm.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "RPX7thbt2Mv.PdsbQ4ckYr.01", "instruction": "Exclude unnecessary information.", "revised_paragraph": "Revised paragraph: \"We utilize the Hopper-v2 environment from the OpenAI Gym MuJoCo locomotion tasks, employing the medium-v2, medium-replay-v2, and medium-expert-v2 datasets to create expert demonstrations and an unlabeled dataset. Expert demonstrations are selected from the D4RL dataset based on episodic return. To obtain the unlabeled dataset, we remove the original reward information and then apply OTR to label it using the optimal coupling with the chosen expert demonstrations, after which we run the offline RL algorithm.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "9ALnOEcGN_.4eEIRZ-dm.02", "instruction": "Make this paragraph more concise.", "revised_paragraph": "Revised paragraph: \"The higher value of \u03b8 i,j indicates a greater probability for the edge from node i to node j to be sampled. Notably, we use the matrix \u03b8 \u2208 R n \u00d7 n to parameterize the probabilistic distribution of n! discrete feasible solutions. This compact, continuous, and differentiable representation of \u03b8 facilitates gradient-based optimization, eliminating the need for the costly MDP-based construction of feasible solutions that has hindered the scalability of representative DRL solvers. Furthermore, we avoid expensive MCMC-based sampling for model optimization by employing chain-rule decomposition. Instead, we utilize autoregressive factorization for sampling from the auxiliary distribution, which is more efficient than MCMC sampling from the distribution defined by the energy function.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "9ALnOEcGN_.4eEIRZ-dm.02", "instruction": "Delete the second sentence. Improve the english in the first sentence.", "revised_paragraph": "Revised paragraph: \"In this context, a higher value of \u03b8 i,j indicates a greater probability that the edge from node i to node j will be sampled. The compact, continuous, and differentiable space of \u03b8 allows us to leverage gradient-based optimization without the costly MDP-based construction of feasible solutions, which has been a bottleneck for scaling up in representative DRL solvers thus far. Furthermore, we can avoid expensive MCMC-based sampling for optimizing our model due to the chain-rule decomposition. Instead, we employ autoregressive factorization for sampling from the auxiliary distribution, which is more efficient than using MCMC to sample from the distribution defined by the energy function.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.22", "instruction": "Please, rewrite this paragraph, make it easier to read", "revised_paragraph": "Revised paragraph: \"We compare our network pruning method with notable model compression techniques used for image super-resolution (SR). Specifically, we evaluate our approach against methods based on neural architecture search (Chu et al., 2019) and knowledge distillation (KD) (Lee et al., 2020). The quantitative results can be found in Table 4. Our SRPN-L achieves the highest performance with the fewest parameters and Mult-Adds. One advantage of our SRP pruning method is that it eliminates the need for searching through numerous architectures or training a teacher network, which would require additional computational resources. These comparisons demonstrate that our SRP method holds significant potential for efficient image super-resolution. Further discussions and comparisons with related works, such as DCP (Zhuang et al., 2018) and DHP (Li et al., 2020), are provided in the appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.22", "instruction": "Write in a more passive style and remove the last sentence", "revised_paragraph": "Revised paragraph: A comparison of the network pruning method was conducted with representative model compression techniques for image super-resolution (SR). In particular, comparisons were made with neural architecture search-based methods (Chu et al., 2019b;a) and knowledge distillation (KD) based methods (Lee et al., 2020). Quantitative results are presented in Table 4, indicating that SRPN-L achieves the best performance with the least number of parameters and Mult-Adds. The SRP pruning method allows for the avoidance of extensive architecture searches or the training of a teacher network, which typically consumes significant computational resources.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.11", "instruction": "Rewrite the majority of the paragraph, avoiding we and writing in a more neutral tone.", "revised_paragraph": "Revised paragraph: Interaction data was collected over a one-month period for a listwise online campaign recommender system. Users are characterized by various attributes, including age, occupation, and localities, while item attributes include text features, image features, and reward points associated with campaigns. A representative reinforcement learning (RL) environment was simulated by training a reward model on a dataset comprising 68,775 users and 57 items to estimate user click likelihood (see Appendix A.4). The user reward model was augmented with the CPR reward to encourage the recommendation of user-relevant items with high CPR. A Variational Autoencoder (VAE) (Kingma & Welling, 2013) was trained to learn action representations, which were then provided as input to the RL agent. The test dataset consisted of 82,445 users and 58 items, with approximately 30 items shared with the training set. Test rewards were reported for models trained using the CDQN algorithm.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.11", "instruction": "Rewrite and reorganize the paragraph to convey the ideas more clearly.", "revised_paragraph": "Revised paragraph: \"In this study, we collected interaction data over the course of one month to evaluate a listwise online campaign recommender system. Users are characterized by various attributes, including age, occupation, and locality, while items are described by their text features, image features, and associated reward points. To simulate a representative reinforcement learning (RL) environment, we trained a reward model using data from 68,775 users and 57 items to estimate user click likelihood (refer to Appendix A.4). Additionally, we enhanced the user reward model with a cost-per-recommendation (CPR) reward to encourage the agent to recommend items that are both high in CPR and relevant to users. We trained a Variational Autoencoder (VAE) (Kingma & Welling, 2013) to derive action representations, which were then provided as input to the RL agent. The test dataset comprised 82,445 users and 58 items, of which approximately 30 items overlap with the training dataset. We present the test rewards obtained for models trained using the CDQN algorithm.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.00", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"A significant gap in the literature pertains to the establishment of a practical completeness threshold for cost-optimal planning problems that involve actions with zero cost. This issue poses a challenge to the application of SAT-based planning to such problems; without an appropriate completeness threshold, optimality can only be demonstrated after resolving the compilation for a horizon equal to the number of states in the state space. This requirement is impractical for most situations, as it can be exponentially larger than the original problem size. It is worth mentioning that some methodologies, such as those proposed by Robinson et al. and Leofante et al., attempt to bypass the necessity for a strict completeness threshold by incorporating an over-approximation of the transition relation that underpins the planning problem into the encoding. In these cases, the optimality of a proposed solution is established when this over-approximation is found to be unsatisfiable. However, these methods still require the computation of compilations for multiple horizons and remain vulnerable to the need for solving compilations for an exponential horizon unless a tighter completeness threshold is identified. This is because the over-approximation acts as a relaxation of the original problem, meaning it may be solvable even if the specific problem is not.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.00", "instruction": "Concise the last sentence of this text.", "revised_paragraph": "Revised paragraph: \"A gap in the literature seems to be a practical completeness threshold for cost optimal planning problems that have actions with 0-cost. This is one hurdle to the application of SAT-based planning to such problems since, without a reasonable completeness threshold, optimality can only be proved after solving the compilation for a horizon that is the number of states in the state space. This is impractical for most problems since it can be exponentially bigger than the size of the given problem. Some approaches, such as those by Robinson et al. and Leofante et al., attempt to address the need for a tight completeness threshold by incorporating an over-approximation of the transition relation into the encoding, allowing optimality to be proved when this over-approximation is unsatisfiable. However, these methods still require computing compilations for multiple horizons and may face exponential complexities unless a tighter completeness threshold is established.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "c8pZvSp-5r.zd4IIIuixp.01", "instruction": "Simplify the conclusions of this paragraph wo make it clearer and more concise.", "revised_paragraph": "Revised paragraph: \"Both DeiT and ViT use a learnable class token for classification (shown as the cls token in Figure 1 (a) and (b)). Although the class token can learn to be translation-invariant, it is not inherently so. An alternative approach is to replace it with a global average pooling (GAP), which is translation-invariant. This results in our CVPT-GAP model, which, combined with translation-equivariant positional encodings, achieves high translation invariance and improves image classification performance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "c8pZvSp-5r.zd4IIIuixp.01", "instruction": "Simplify the last sentence  by removing the notion of translation-equivariant  and just calling it conditional positional encodings.", "revised_paragraph": "Revised paragraph: \"In addition, both DeiT and ViT utilize an extra learnable class token to perform classification (i.e., cls token shown in Figure 1 (a) and (b)). By design, the class token is not translation-invariant although it can learn to be so. A simple alternative is to directly replace it with a global average pooling (GAP), which is inherently translation-invariant, resulting in our CVPT-GAP. Together with the conditional positional encodings, CVPT-GAP is utterly translation-invariant and thus can achieve much better image classification performance.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.19", "instruction": "Rewrite the following paragraph, make it more formal.", "revised_paragraph": "Revised paragraph: \"When juxtaposed with all prior methodologies, our proposed SRPN-L demonstrates superior performance across all datasets and scaling factors. Unlike many of the competing methods that rely on meticulously designed networks, we begin with the existing EDSR baseline (Lim et al., 2017) and subsequently reduce its size through pruning, thereby illustrating the efficacy of our proposed SRP approach.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.19", "instruction": "Improve the writing and change SRPN-L to SRPN-Lite", "revised_paragraph": "Revised paragraph: \"When compared to all prior methods, our SRPN-Lite consistently outperforms them across all datasets and scaling factors. Unlike most of the compared methods that rely on meticulous network designs, our approach begins with the established EDSR baseline (Lim et al., 2017) and prunes it down to a significantly smaller network, thereby demonstrating the effectiveness of our proposed SRP.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.22", "instruction": "Fluidify this paragraph.", "revised_paragraph": "Revised paragraph: \"Shan et al. (2022) identify five single-point mutations in a human antibody targeting SARS-CoV-2 that enhance neutralization effectiveness. In total, there are 494 possible single-point mutations located within the heavy chain CDR region of the antibody. Utilizing the most competitive methods, as benchmarked in Section 4.1, we predict the \u2206\u2206G values for all mutations and rank them in ascending order, with the lowest \u2206\u2206G values positioned at the top. A predictor is deemed more effective if it consistently ranks more favorable mutations higher in this order. Table 2 demonstrates that both RDE-Network and DDGPred successfully identify three mutations that rank within the top 10%, with RDE-Network showing superior performance in the rankings.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.22", "instruction": "Improve the English in this paragraph in an academic style.", "revised_paragraph": "Revised paragraph: Shan et al. (2022) identify five single-point mutations in a human antibody against SARS-CoV-2 that enhance its neutralization efficacy. In total, there are 494 possible single-point mutations within the heavy chain complementarity-determining region (CDR) of the antibody. We employ the most competitive methodologies, as benchmarked in Section 4.1, to predict the change in free energy (\u0394\u0394G) for all mutations and rank them in ascending order, with the lowest \u0394\u0394G values positioned at the top. A predictor is deemed more effective if it successfully ranks a greater number of favorable mutations in the top tier. Table 2 indicates that both RDE-Network and DDGPred correctly identify three mutations ranked within the top 10%, with RDE-Network achieving a higher ranking for these mutations.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.10", "instruction": "I want to use other words in my paragraph.", "revised_paragraph": "Revised paragraph: \"Considering the aforementioned issue, it is imperative to eliminate certain Conv layers within the residual blocks if we aim to enhance practical efficiency. Consequently, a strategy is required to synchronize the pruned indices across all constrained Conv layers. Regularization thus emerges as an intuitive solution, owing to its common application in enforcing prior constraints on the sparsity patterns during pruning (Reed, 1993; Wen et al., 2016; Wang et al., 2021).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.10", "instruction": "Revise this text to make it a little more concise and fitting to the academic style.", "revised_paragraph": "Revised paragraph: \"To address the aforementioned issue, it is essential to prune all convolutional layers in residual blocks to enhance practical efficiency. Consequently, a method for aligning the pruned indices across all constrained convolutional layers is required. Regularization serves as an effective solution, as it is commonly employed to impose priors on the sparsity structure in pruning (Reed, 1993; Wen et al., 2016; Wang et al., 2021).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.06", "instruction": "Change the subject in the first sentence.", "revised_paragraph": "Revised paragraph: \"It is important to note that exclusive multiset-equivariance is not always obtained in DSPN, but depends on the choice of encoder. For instance, a DeepSets encoder (Zaheer et al., 2017) \u2013 which is based on sum pooling \u2013 has the same gradients for equal elements, which would make DSPN set-equivariant. It is specifically the use of the exclusively multiset-equivariant gradient of sorting that makes DSPN exclusively multiset-equivariant.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.06", "instruction": "Lightly revise this paragraph for better readability while trying to make it a little shorter without loosing informations.", "revised_paragraph": "Revised paragraph: \"It is important to note that exclusive multiset-equivariance in DSPN is not guaranteed and depends on the encoder selection. For example, a DeepSets encoder (Zaheer et al., 2017), which utilizes sum pooling, produces identical gradients for equal elements, thus ensuring set-equivariance in DSPN. The key to achieving exclusive multiset-equivariance lies in using the multiset-equivariant gradient derived from sorting.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7VIguXRv9h.yPdniQMisK.00", "instruction": "Remove the less important details in the results.", "revised_paragraph": "Revised paragraph: \"\u2022 Implicit Curricula: We demonstrate that the order in which examples are learned is consistent across various runs and architectures, and that this order can be altered by changing the presentation sequence during training. \u2022 Curricula achieve minimal improvement in the standard setting. Our findings indicate that curriculum learning, random, and anti-curriculum learning perform similarly in this context. \u2022 Curriculum learning shows improvement over standard training when training time is limited. In situations where multiple training epochs are not feasible, our experiments reveal a clear advantage of curriculum learning. \u2022 Curriculum learning outperforms standard training in noisy environments. Our experiments indicate that curriculum learning provides a significant benefit in the presence of label noise.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7VIguXRv9h.yPdniQMisK.00", "instruction": "Remove unnecessary details.", "revised_paragraph": "Revised paragraph: \"Implicit curricula exhibit a consistent learning order across different runs, training methods, and architectures, which can be altered by changing the presentation order of examples. We find that well-known measures of sample difficulty are highly correlated. In a standard setting, curriculum learning shows minimal improvement compared to random and anti-curriculum approaches, and removing examples from the training set does not enhance performance. However, when training time is constrained, curriculum learning demonstrates significant advantages over standard training. Additionally, in a noisy data environment created by label noise, curriculum learning also outperforms standard training and other curricula.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.07", "instruction": "Give me a shorter version of this:", "revised_paragraph": "Revised paragraph: Recently, deep learning approaches have been developed to predict mutational effects on protein binding, categorized into end-to-end models, pre-training-based models, and unsupervised models. End-to-end models input both mutant and wild-type protein structures to directly predict differences in binding free energy. While they demonstrate good correlation with the SKEMPI benchmark overall (Shan et al., 2022), the correlation per structure remains low, limiting practical applicability. In response to data scarcity, some researchers propose pre-training a feature extraction network followed by regression models to leverage learned features for predicting mutation effects (Liu et al., 2021; Yang et al., 2022).", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.07", "instruction": "Make this paragraph twice as short by making the content more concise and deleting unnecessary details.", "revised_paragraph": "Revised paragraph: \"Deep learning-based approaches for predicting mutational effects on protein binding have emerged, categorized into end-to-end models, pre-training-based models, and unsupervised models. End-to-end models, which use both mutant and wild-type protein structures as input, predict binding free energy differences. While they show good correlation with the SKEMPI benchmark (Shan et al., 2022), their per-structure correlation remains low. To address data scarcity, another approach involves pre-training a feature extraction network followed by regression models to predict mutation effects based on learned features (Liu et al., 2021; Yang et al., 2022).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "SyF8k7bCW.HytIRPamf.04", "instruction": "Rewrite this paragraph using more formal language", "revised_paragraph": "Revised paragraph: We embraced the concept articulated by Chen et al. Their objective is to construct a model for the supervised Stanford Natural Language Inference (SNLI) task (Bowman et al., 2015). This model integrates the outputs of global mean-pooling and global max-pooling functions to generate a representation of the sentence, thereby demonstrating an enhancement in performance on the SNLI dataset. Furthermore, Conneau et al. (2017) identified that the model employing the global max-pooling function exhibits superior transferability compared to the model utilizing the global mean-pooling function subsequent to supervised training on SNLI.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "SyF8k7bCW.HytIRPamf.04", "instruction": "Rephrase the text", "revised_paragraph": "Revised paragraph: \"We embraced the concept suggested by Chen et al., who sought to develop a model for the supervised SNLI task (Bowman et al., 2015). This model incorporates outputs from both a global mean-pooling function and a global max-pooling function to create a sentence representation, resulting in enhanced performance on the SNLI dataset. Additionally, Conneau et al. (2017) demonstrated that the model utilizing the global max-pooling function exhibits greater transferability compared to the model employing the global mean-pooling function, following supervised training on SNLI.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "instruction": "Remove a redundant sentence. Use clearer expression.", "revised_paragraph": "Revised paragraph: Generalization of meta learning has been extensively analyzed recently, using excess risk as a metric for assessing generalization ability in gradient-based meta-learning approaches [3,4,9,14,18,42]. In the context of mixed linear regression, the generalization of meta learning has been explored [27], specifically focusing on how numerous tasks with limited data might offset the lack of tasks with larger datasets. Additionally, generalization performance has been examined in relation to representation-based meta learning [13,16]. Information-theoretical bounds have been established to limit the generalization error concerning the mutual information between input training data and the outputs of meta-learning algorithms [10,26]. The PAC-Bayes framework has also been adapted for meta learning, resulting in a PAC-Bayes meta-population risk bound [1,15,19,34]. Most of these investigations are concentrated on cases where the meta-learning model is underparameterized, meaning the overall amount of meta training data across all tasks exceeds the model parameter's dimensionality. Recently, the area of overparameterized meta learning has garnered increased interest. For instance, Bernacchia [6] suggests that a negative learning rate in the inner loop is optimal during the meta-training of linear models with Gaussian data in overparameterized MAML. Furthermore, Sun et al. [39] demonstrate the optimal representation in representation-based meta learning is overparameterized and provides sample complexity for the method of moment estimator.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "instruction": "Improve the English and remove the second sentence.\t", "revised_paragraph": "Revised paragraph: Generalization of meta learning. The excess risk, as a metric of the generalization ability of gradient-based meta learning, has been analyzed recently [3,4,9,14,18,42]. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, focusing on investigating how an abundance of tasks with small data can compensate for a lack of tasks with large data. Generalization performance has also been examined in a relevant but different setting\u2014representation-based meta learning [13,16]. Information theoretical bounds have been proposed in [10,26], which constrain the generalization error in terms of mutual information between the input training data and the output of the meta-learning algorithms. The PAC-Bayes framework has been extended to meta learning, providing a PAC-Bayes meta-population risk bound [1,15,19,34]. These works predominantly focus on cases where the meta learning model is underparameterized; that is, the total number of meta training data from all tasks exceeds the dimension of the model parameters. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML, a negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] demonstrate that the optimal representation in representation-based meta learning is overparameterized and provides sample complexity for the method of moments estimator.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1CMuZFor.H1NchtnoS.01", "instruction": "Please exclude the content that seems unnecessary.", "revised_paragraph": "Revised paragraph: \"The NTK is defined using the gradient of the DNN output with respect to weight parameter space, while the linear approximation in this paper is based on the gradient with respect to input parameter space, indicating that the variables for differentiation differ. Although NTK analysis is typically limited to gradient descent, our analysis can be applied to stochastic gradient descent. The random walk analysis shows that over-parameterized ReLU DNNs interpolate almost linearly between data points. For ReLU activation, since the NTK kernel mapping is 1/2-H\u00f6lder and not Lipschitz, achieving similar results in NTK analysis presents challenges without a balance between smoothness and approximation (Bietti & Mairal, 2019).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1CMuZFor.H1NchtnoS.01", "instruction": "Remove the second item of the list.", "revised_paragraph": "Revised paragraph: \"\u2022 The NTK is de\ufb01ned using the gradient of the DNN output with respect to weight parameter space. In contrast, the linear approximation (Lemma in this paper) is de\ufb01ned using the gradient of the DNN output with respect to input parameter space. In other words, the variables to be differentiated are different. \u2022 The random walk analysis indicates that over-parameterized ReLU DNNs interpolate almost linearly between the data points. For ReLU activation, since the NTK kernel mapping is not Lipschitz but 1/2-H\u00f6lder, it is dif\ufb01cult to obtain such a result in the NTK analysis without a tradeoff between smoothness and approximation (Bietti & Mairal, 2019).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.05", "instruction": "Give me a more formal version of the following paragraph.", "revised_paragraph": "Revised paragraph: Our methodology addresses the aforementioned challenges effectively. The training of the rotamer density estimator necessitates solely the utilization of protein structures, which positions it as an unsupervised learner regarding the effects of mutations on binding. This feature alleviates the issues associated with the limited availability of annotated mutation data. Furthermore, our approach is independent of the structural data of the mutated protein; instead, it conceptualizes mutated structures as latent variables, with the rotamer density estimator serving as an approximation of the latent distribution. Our method demonstrates superior performance compared to empirical energy functions and machine learning models in the prediction of \u2206\u2206G. Additionally, as a generative model for rotamers, the rotamer density estimator reliably predicts sidechain conformations with high accuracy.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.05", "instruction": "Rewrite this paragraph in a more formal and academic way.", "revised_paragraph": "Revised paragraph: Our proposed methodology addresses the challenges previously outlined. The training of the rotamer density estimator necessitates only protein structural data, thereby enabling it to function as an unsupervised learner regarding the effects of mutations on binding. This characteristic mitigates the challenges associated with the limited availability of annotated mutation data. Furthermore, our approach does not mandate the provision of the mutated protein's structure as input; instead, it conceptualizes mutated structures as latent variables, with the rotamer density estimator serving as an approximation of the latent distribution. Empirical evaluations demonstrate that our method surpasses traditional empirical energy functions and machine learning models in the prediction of \u2206\u2206G. Additionally, as a generative model for rotamers, the rotamer density estimator accurately predicts side chain conformations.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.04", "instruction": "Make this paragraph heavily more concise in the explanations made.", "revised_paragraph": "Revised paragraph: \"Metric learning creates feature representations using data grouping and separation cues. Our method (Fig. 3) segments images by learning a pixel-wise embedding through a contrastive loss, focusing on positive and negative segments relative to pixel i. The objective is to group pixel i with positive segments while distancing it from negative ones, optimizing the latent feature \u03c6(i) to reduce the distance to positive and increase it to negative segments.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.04", "instruction": "Don't give to much details about the method of learning, just keep the main idea.", "revised_paragraph": "Revised paragraph: Metric learning creates a feature representation through cues of data grouping and separation. Our approach focuses on segmenting an image by learning pixel-wise embeddings and employing a contrastive loss. We aim to group each pixel with positive segments while ensuring separation from negative segments, utilizing a distance metric to optimize this process.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.23", "instruction": "Simplify the explanation of the merged w unbnd M R and w unbnd W R.", "revised_paragraph": "Revised paragraph: \"To demonstrate a statistically significant relationship between the entropy estimated by the RDE and the experimental \u2206\u2206 G values, we conduct linear regression analysis using the RDE-Linear model defined in Eq.9. This model includes 7 coefficients and 1 bias: w bound W L, w bound W R, w unbnd W L, w bound M L, w bound M R, w unbnd M L, and (w unbnd M R \u2212 w unbnd W R), along with a bias term b. Since the receptor is not mutated, the entropy estimates for w unbnd M R and w unbnd W R are identical, allowing us to merge these terms into a single coefficient, w unbnd R. We then perform linear regression on the SKEMPI2 dataset, and the results, including regression coefficients, bias, and P-values, are provided in Table 3.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.23", "instruction": "Concise the penultimate sentence. Improve the English in this paragraph.", "revised_paragraph": "Revised paragraph: \"Statistical Significance In order to establish a statistically significant relationship between the entropy estimated by the RDE and the experimental \u2206\u2206G values, we conduct linear regression analysis utilizing the RDE-Linear model defined in Eq. 9. This linear model consists of seven coefficients and one bias: \\( w_{\\text{bound} W_L} \\), \\( w_{\\text{bound} W_R} \\), \\( w_{\\text{unbound} W_L} \\), \\( w_{\\text{bound} M_L} \\), \\( w_{\\text{bound} M_R} \\), \\( w_{\\text{unbound} M_L} \\), and \\( (w_{\\text{unbound} M_R} - w_{\\text{unbound} W_R}) \\), along with \\( b \\). The terms \\( w_{\\text{unbound} M_R} \\) and \\( w_{\\text{unbound} W_R} \\) are merged since the receptor is not mutated, resulting in identical entropy estimations. For brevity, we denote the merged coefficient as \\( w_{\\text{unbound} R} \\). We perform linear regression on the SKEMPI2 dataset and present the regression coefficients, bias, and P-values in Table 3.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "TFoRhVCpnb.yqo5NaW74.00", "instruction": "Use uppercases properly.", "revised_paragraph": "Revised paragraph: \"Image semantic segmentation is the task of pixel-level semantic label allocation for recognizing objects in an image. The development of Deep Neural Networks (DNNs) has promoted the rapid advancement of the semantic segmentation task [6, 58, 19] in recent years. However, training such a fully supervised semantic segmentation model requires large numbers of pixel-wise annotations.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "TFoRhVCpnb.yqo5NaW74.00", "instruction": "Use capital letters at the beginning of every words in the names of segmentation methods.", "revised_paragraph": "Revised paragraph: \"Image Semantic Segmentation Is The Task Of Pixel-Level Semantic Label Allocation For Recognizing Objects In An Image. The Development Of Deep Neural Networks (DNNs) Has Promoted The Rapid Development Of The Semantic Segmentation Task [6, 58, 19] In Recent Years. However, Training Such A Fully-Supervised Semantic Segmentation Model Requires Large Numbers Of Pixel-Wise Annotations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Mu-tqfqX-.6NSudk3nD.03", "instruction": "Rewrite this paragraph to improve its clarity.", "revised_paragraph": "Revised paragraph: \"To illustrate selection collider bias, consider the following scenario: if we were to inquire about the gender of a random person born in 1801 and another born in 1999, you might choose an answer by tossing a coin, as birth date and gender are unconditionally independent in reality. However, if we instead asked for the gender of individuals born in 1801 and 1999 based on two random Wikipedia articles you accessed today, your response would likely depend on a combination of factors, including birth date, gender, and the information available on Wikipedia.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Mu-tqfqX-.6NSudk3nD.03", "instruction": "This paragraph is confusing, rewrite to make it clearer and more readable.", "revised_paragraph": "Revised paragraph: \"To illustrate selection collider bias, consider this example: if we asked you to identify the gender of a random person born in 1801 and another born in 1999, you might use a coin toss to make your decision, as birth date and gender are generally independent in the real world. However, if we were to ask about the gender of two specific individuals from Wikipedia articles published today\u2014one born in 1801 and the other in 1999\u2014you would likely base your guess on a combination of birth date, gender, and the information available in Wikipedia. This introduces a bias because the selection of individuals from Wikipedia may be influenced by factors related to both birth date and gender.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.17", "instruction": "Rewrite the middle part of this paragraph and improve the English in the remainder", "revised_paragraph": "Revised paragraph: \"We validate the choice of using a graph attention network as the relational architecture. In Figure 7, we compare GAT with a graph convolutional network (GCN) (Kipf & Welling, 2016) within the action graph of AGILE. Our analysis indicates that while GCN demonstrates optimal performance in the simple grid world and RecSim tasks, it does so primarily because it functions effectively as a summarizer, even without learning edge weights. However, GCN's performance deteriorates in CREATE and RecSim-pairing (Figure 16) where the action space is extensive and necessitates a broader range of action relations. In contrast, we argue that the attention mechanism in GAT enhances sparsity in the graph, facilitating reinforcement learning training, a challenge in a fully-connected GCN.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.17", "instruction": "Make this paragraph a bit more concise.", "revised_paragraph": "Revised paragraph: \"We validate the choice of graph attention network (GAT) as the relational architecture. In Figure 7, we compare GAT to a graph convolutional network (GCN) (Kipf & Welling, 2016) within AGILE's action graph. GCN achieves optimal performance in the simple grid world and RecSim tasks, as it functions effectively as a summarizer despite not learning edge weights. However, it struggles in CREATE and RecSim-pairing (Figure 16), where the large action space demands diverse action relations. We argue that GAT's attention mechanism creates sparsity, facilitating reinforcement learning training, which is challenging in a fully connected GCN.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "instruction": "Make first sentence more concise. Rewrite phrases, prefer short formulations and avoid we.", "revised_paragraph": "Revised paragraph: \"Listwise RL (CDQN) addresses the combinatorial action space of listwise actions by employing the Cascaded DQN (CDQN) framework proposed by Chen et al. (2019a). The main challenge arises from the infeasibility of constructing the entire list simultaneously due to the enormous number of potential combinations. Therefore, the solution involves incrementally building the list, focusing on one action at a time. Each list index is treated as a distinct non-combinatorial action suitable for reinforcement learning (RL) training. The Q-network of CDQN is replaced with AGILE to accommodate a dynamic action space, and weights are shared across the cascaded Q-networks. Complete details on listwise AGILE are outlined in Algorithm 1.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "instruction": "Make first sentence more concise. Rewrite phrases, prefer short formulations and avoid we.", "revised_paragraph": "Revised paragraph: \"Listwise RL (CDQN): This research adopts the Cascaded DQN (CDQN) framework of Chen et al. (2019a) to address the combinatorial action space problem of listwise actions. The primary challenge arises from the infeasibility of generating the complete list simultaneously due to the enormous number of potential combinations. Therefore, the focus shifts to constructing the list incrementally, one action at a time. Each list index is treated as an individual non-combinatorial action that can be optimized using reinforcement learning. The Q-network of CDQN is replaced with AGILE to accommodate a variable action space while sharing the weights of the cascaded Q-networks. Details on listwise AGILE can be found in Algorithm 1.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "BJ49j43UH.B15bvYjiH.00", "instruction": "Make the last sentence more concise.", "revised_paragraph": "Revised paragraph: \"Datasets: We consider 12 public datasets (3 tabular, 7 image, and 2 language) to evaluate DVRL against multiple benchmark methods. The tabular datasets are (1) Blog, (2) Adult, and (3) Rossmann; the image datasets are (4) HAM 10000, (5) MNIST, (6) USPS, (7) Flower, (8) Fashion-MNIST, (9) CIFAR-10, and (10) CIFAR-100; the language datasets are (11) Email Spam and (12) SMS Spam. Detailed information about the datasets is available via the provided hyperlinks.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "BJ49j43UH.B15bvYjiH.00", "instruction": "Make the last sentence shorter.", "revised_paragraph": "Revised paragraph: \"Datasets: We consider 12 public datasets (3 tabular, 7 image, and 2 language datasets) to evaluate DVRL in comparison to multiple benchmark methods. The public tabular datasets are (1) Blog, (2) Adult, (3) Rossmann; the image datasets are (4) HAM 10000, (5) MNIST, (6) USPS, (7) Flower, (8) Fashion-MNIST, (9) CIFAR-10, and (10) CIFAR-100; the language datasets are (11) Email Spam and (12) SMS Spam. Details of the datasets are provided in the hyperlinks.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.00", "instruction": "Revise this paragraph to be more concise.", "revised_paragraph": "Revised paragraph: \"A prescription is a vital form of medical intervention in modern clinical practice, representing a recommendation from a healthcare provider to a patient. It may involve actions such as taking medications, following a diet, or engaging in physical exercises. When both parties agree on the prescription, patient adherence is expected. Adherence refers to the extent to which patients follow the agreed recommendations. However, non-adherence is a significant healthcare issue, with average rates of 50% and contributing to 33-69% of hospital re-admissions, resulting in billions of dollars in costs annually.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.00", "instruction": "Merge the two first sentences in one shorter one. Improve the sentence defining adherence to make it clearer.", "revised_paragraph": "Revised paragraph: A prescription is a vital medical intervention provided by healthcare providers to patients, recommending actions such as taking medications, following a diet, or executing physical exercises [1,2]. When mutually agreed upon, patients are expected to follow the prescription, and the degree to which patients comply with these recommendations is referred to as adherence [2]. Non-adherence to prescriptions is a significant problem in healthcare [2,4], with adherence rates averaging 50% and accounting for 33-69% of hospital re-admissions, resulting in billions of dollars in costs annually [5,6].", "type_approach": "instruction-annot_2"}
{"id_paragraph": "UlHNcByJV.W1RxpkrWx8.01", "instruction": "Rewrite the bullet points, making them more independent and preferring active over passive formulations", "revised_paragraph": "Revised paragraph: Data points accepted by the biased classifier are included in the dataset with their true labels. If the biased classifier rejects a data point, we utilize our de-biased classifier to determine its inclusion in the Pseudo-label dataset from PLOT. Subsequently, we implement the Pseudo-label mechanism from PLOT, which involves retraining on optimistic labels, to assess the final acceptance of these candidates.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "UlHNcByJV.W1RxpkrWx8.01", "instruction": "Shorten the last sentence. Make this paragraph more direct.", "revised_paragraph": "Revised paragraph: \"If a data point is accepted by the biased classifier, it is added to the dataset with the true label. If rejected, we use our de-biased classifier to determine its inclusion in the Pseudo-label dataset from PLOT. We then apply the Pseudo-label mechanism from PLOT to make the final decision.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.22", "instruction": "Update the last sentence and split it into two sentences to make it easier to understand", "revised_paragraph": "Revised paragraph: \"Bi-LSTM: The raw action representations of candidate actions are passed on to the 2-layer MLP followed by ReLU. Then the output of the MLP is processed by a 2-layer bidirectional LSTM (Huang et al., 2015). This process is followed by another 2-layer MLP, which generates the action-summary for use in the subsequent utility network.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.22", "instruction": "Split this paragraph into smaller and more focused points.", "revised_paragraph": "Revised paragraph: \"Bi-LSTM: The raw action representations of candidate actions are initially processed through a 2-layer multi-layer perceptron (MLP) with ReLU activation. Subsequently, the output from the MLP is fed into a 2-layer bidirectional Long Short-Term Memory (LSTM) network, as described by Huang et al. (2015). Finally, the processed output from the bidirectional LSTM is passed through another 2-layer MLP to generate the action summary that will be utilized in the subsequent utility network.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "5t8NvKONr.tls-ZX2iE.02", "instruction": "Remove unnecessary details. Include citation.", "revised_paragraph": "Revised paragraph: The proof is based on substituting a neural network for the inner product between the branch and trunk networks. A low-complexity neural network can approximate the inner product effectively, as it is infinitely differentiable (cite source). Although DeepONet has limitations, it can be replaced with a neural network using E(u) and y as inputs. Increasing the number of basis functions in DeepONet raises the number of parameters in the trunk network, which serves as the target network in HyperDeepONet. Alternative models like Shift-DeepONet and flexDeepONet achieve desired accuracy with fewer basis functions, but the first hidden layer of the target network still requires a considerable number of units. Furthermore, while NOMAD employs a fully nonlinear reconstruction without restrictions on the last hidden layer's dimensions, the first hidden layer must remain sufficiently wide to accommodate this increase in parameters. Additional details are available in Appendix C.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "5t8NvKONr.tls-ZX2iE.02", "instruction": "Rewrite the beginning of the paragraph to improve the argumentation.", "revised_paragraph": "Revised paragraph: The foundation of this proof relies on the substitution of a neural network for the inner product between the branch net and trunk net, yielding significant improvements in performance. By employing a low-complexity neural network, we can effectively approximate the inner product, capitalizing on the fact that the inner product is an infinitely differentiable function. Although DeepONet has displayed certain shortcomings, it can be effectively replaced with a neural network that utilizes E(u) and y as inputs. Notably, the increase in the number of basis functions p within DeepONet leads to an escalation in the trunk net's parameters, categorizing it as a target network within HyperDeepONet. Alternative models, such as Shift-DeepONet and flexDeepONet, can achieve the requisite accuracy with fewer basis functions; however, they face the challenge of requiring numerous units in the first hidden layer of the target network. Moreover, while NOMAD imposes no limitations on the dimensionality of the last hidden layer for its fully nonlinear reconstruction, it does necessitate a sufficiently wide first hidden layer in the target network, thereby increasing the parameter count. Further details are available in Appendix C.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.15", "instruction": "Restructure the last two sentences in this paragraph", "revised_paragraph": "Revised paragraph: \"Four participants indicated that they intentionally used edges (Figure 5 (iv)) in all conditions. Nevertheless, they were affected by the cursor being obscured by the notch, which led to a loss of visibility. When the cursor was hidden by the notch, all participants reported attempting to locate it by moving the mouse vigorously.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.15", "instruction": "Revise this paragraph to make it more clear and concise.", "revised_paragraph": "Revised paragraph: \"Four participants reported that they intentionally utilized edges (Figure 5 (iv)) in all conditions. However, they experienced difficulties when the cursor was obscured by the notch, leading to a loss of visual tracking. All participants indicated that, when the cursor was hidden, they attempted to locate it by moving the mouse vigorously.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.05", "instruction": "Rewrite this paragraph and focus more on the first sentence", "revised_paragraph": "Revised paragraph: \"The rectangular nature of typical targets in graphical user interfaces (GUIs) means that target height (H) significantly influences movement time [3,8,14,20,21,27]. To account for this, Accot and Zhai [1] developed a model specifically for bivariate (2D) pointing tasks that incorporates H. Additionally, Zhang et al. [28] suggested an approach to balance the effects of both width (W) and height (H) in their analysis (Eq. 2).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.05", "instruction": "Improve this paragraph for clarity, mainly the first sentence.", "revised_paragraph": "Revised paragraph: \"Given that the typical targets on graphical user interfaces (GUIs) are rectangular, the height of the target (H) significantly influences the movement time [3,8,14,20,21,27]. Accot and Zhai [1] developed a model for bivariate (2D) pointing tasks that incorporates H. Furthermore, Zhang et al. [28] suggested a method to balance the effects of width (W) and height (H) as outlined in Equation 2.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.00", "instruction": "Improve the words using in this paragraph", "revised_paragraph": "Revised paragraph: \"Although the mouse cursor can intersect with the notch area at the top of the MacBook Pro (2021) display, it is often partially or completely obscured by the notch itself. Avoiding the notch or maneuvering the cursor meticulously around it can prolong movement time. In this study, we conducted a series of experiments to assess the impact of the notch on the movement of the mouse cursor. Experiment 1 demonstrated that the notch increased pointing movement time under specific conditions. Experiment 2 indicated that steering clear of the notch is preferable to entering it with the current design. Experiment 3 revealed that redesigning the notch to an area inaccessible to the cursor significantly enhances the speed and accuracy of targeting when the target is adjacent to the notch. Consequently, the outer edge of the notch obstructs the cursor, facilitating quicker and more precise target selection. Therefore, it is recommended that the notch be redesigned as an area that the cursor cannot penetrate.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.00", "instruction": "Modify this paragraph to make it more direct and easy to read.", "revised_paragraph": "Revised paragraph: \"The mouse cursor can enter the notch area at the top of the MacBook Pro (2021) display, but it may be partially or completely obscured by the notch. To avoid the notch or carefully maneuver the cursor around it can slow down movement. In our study, we performed several experiments to assess how the notch affects mouse cursor movement. Experiment 1 demonstrated that the notch increases pointing movement time in certain situations. Experiment 2 found that avoiding the notch is more effective than entering it with the current design. Experiment 3 indicated that altering the notch so the cursor cannot enter it allows for quicker and more accurate targeting of objects adjacent to the notch. Thus, the outer edge of the notch can effectively halt the cursor, leading to faster and more precise target selection. Therefore, the notch should be a designated area where the cursor is not allowed to enter.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.16", "instruction": "Clarify the wording in this paragraph.", "revised_paragraph": "Revised paragraph: \"Arrows are an effective tool for communicating suggested actions for conflict resolution in scheduling. End-of-line arrows can indicate when two medication entries are scheduled too closely together, suggesting they should be separated, and vice versa. The calendar should also provide a means to indicate that a specific entry is optional. This feature is particularly relevant for entries such as prescription medications that are taken as needed, as well as over-the-counter medications. These design choices are informed by common activities that users regularly encounter.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.16", "instruction": "Reword my sentence about entries.", "revised_paragraph": "Revised paragraph: \"In managing conflicts, arrows serve as a clear means of conveying recommended actions for conflict resolution. End-of-line arrows can effectively represent that medication entries scheduled too closely together should be separated, and vice versa. Additionally, the calendar should support the designation of certain entries as optional. This feature is particularly relevant for prescription medications that are taken as needed and for over-the-counter medications. These design choices are shaped by the everyday practices with which users are already familiar.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.03", "instruction": "Improve the writing of this paragraph", "revised_paragraph": "Revised paragraph: \"Patrick et al. introduced the Mouse Ether technique based on their observations that users often lose track of the cursor when utilizing multiple displays with differing resolutions, due to unnatural cursor movement across screens [5]. Their findings demonstrated that this technique enhanced performance by as much as 28% by mitigating the distortion that occurs during cursor transitions between displays.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.03", "instruction": "Modify the logical flow of ideas improve the readability of the paragraph.", "revised_paragraph": "Revised paragraph: \"Patrick et al. introduced the Mouse Ether technique after observing that users often lose track of the cursor when transitioning between multiple displays with varying resolutions due to unnatural cursor movement [5]. Their results demonstrated that this technique enhanced performance by as much as 28% by mitigating the unnatural warping that occurs during cursor movement between displays.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "jyac3IgQ44.f4au9jfat5.05", "instruction": "Revise this paragraph for better readability.", "revised_paragraph": "Revised paragraph: \"To take advantage of the natural sparsity of point clouds and enhance efficiency, we have implemented our window center searching, window gathering, and balanced window sampling as sparse CUDA operations. These operations primarily utilize a hash map to map coordinate space to voxel indices [23]. For instance, in the window gathering operation, we query each possible position relative to the specified center within the window and retrieve the corresponding features if the position is a valid key in the pre-built hash map. Further details can be found in the supplementary materials.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "jyac3IgQ44.f4au9jfat5.05", "instruction": "Improve the flow of ideas for better readability.", "revised_paragraph": "Revised paragraph: \"To enhance efficiency by leveraging the natural sparsity of point clouds, we implement our window center searching, window gathering, and balanced window sampling as sparse CUDA operations. These operations primarily rely on a hash map that facilitates the mapping from coordinate space to voxel index [23]. For instance, in the window gathering operation, we query each potential position relative to the specified center within the window and retrieve the corresponding features if the position matches a valid key in the pre-built hash map. Further details can be found in the supplementary materials.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nkOpNqg-ip.OwJsIhe_p.01", "instruction": "Rewrite the last sentence, making it more concise.", "revised_paragraph": "Revised paragraph: \"The surprising result of our experimental evaluation is that, for runtimes of 1 hour, the black box optimizers rarely surpass Naive AutoML. In fact, the experiments demonstrate that the \u201cEx-def\u201d baseline is already quite robust within this timeframe. This finding does not contradict the results of Thornton et al. (2013), which utilized a 30-hour timeout. While further experimentation with extended runtimes is warranted, it is already clear that simple baselines are significantly more effective than previously assumed.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nkOpNqg-ip.OwJsIhe_p.01", "instruction": "Make the last sentence more concise.", "revised_paragraph": "Revised paragraph: \"The surprising result of our experimental evaluation is that, for runtimes of 1 hour, the black box optimizers rarely improve upon Naive AutoML. In fact, the experiments show that the \u201cEx-def\u201d baseline itself is already quite strong in this timeframe. This is not a contradiction to the findings of Thornton et al. (2013), which employed a 30-hour timeout. While these observations call for more extensive experiments with longer runtimes, it is clear that simple baselines are much stronger than previously assumed.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.06", "instruction": "Rewrite this paragraph and choose better words", "revised_paragraph": "Revised paragraph: \"Research indicates that positioning the target adjacent to the screen's edge can significantly decrease movement time [3,9,10,24,25]. When aiming for a target located in the center of the screen, the cursor must halt just inside the target boundaries. In contrast, when targeting an edge position, the cursor can terminate at the edge itself. Consequently, the pointing task can be efficiently completed by moving the cursor horizontally along the edge to which the target is aligned. Furthermore, targets situated at the corners of the screen can be quickly accessed by simply directing the cursor to the corner [24].\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.06", "instruction": "Reorganise the flow of ideas when a sentence is confusing. Try to shorten the paragraph a bit.", "revised_paragraph": "Revised paragraph: \"Placing a target adjacent to the edge of the screen, known as an edge target, is known to reduce movement time [3,9,10,24,25]. When pointing at a central target, the cursor must stop just inside the target, whereas for an edge target, it stops at the edge. This allows for quicker completion of the pointing task by moving the cursor horizontally along the edge. Furthermore, targets near the corner can be quickly selected by simply hitting the corner with the cursor [24].\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.00", "instruction": "Improve the english of this paragraph.", "revised_paragraph": "Revised paragraph: \"Estimating individual treatment effects from observational data poses significant challenges due to the presence of treatment selection bias. Most existing representation-based methods attempt to address this issue by aligning the distributions of different treatment groups within the representation space. However, these methods still encounter two critical problems: (1) Mini-batch Sampling Effects (MSE), which occur when alignment fails due to outcome imbalances or outliers within the batch; and (2) Unobserved Confounder Effects (UCE), where unobserved confounders compromise the accuracy of the alignment.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.00", "instruction": "Edit this paragraph by making more formal choices of wording.", "revised_paragraph": "Revised paragraph: \"Estimating individual treatment effects from observational data presents significant challenges attributable to the presence of treatment selection bias. Most extant representation-based methodologies seek to address this issue by aligning the distributions of diverse treatment groups within the representation space. Nevertheless, these approaches continue to encounter two fundamental issues: (1) Mini-batch Sampling Effects (MSE), characterized by the potential failure of alignment due to outcome imbalances or outliers within the batch; (2) Unobserved Confounder Effects (UCE), wherein unobserved confounders undermine the integrity of the alignment.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "HkW3nTM6X.S1d278zJ4.00", "instruction": "Rephrase the sentence related to Figures 2c-d.", "revised_paragraph": "Revised paragraph: \"The SDE model demonstrated in Figures 2c-d clearly indicates that a time-variant drift function results in a lower prediction error, suggesting an improved understanding of the dynamics involved in a golf swing motion. Additionally, it is evident that the prediction error consistently declines with an increase in the number of inducing points M, achieving its lowest value at M = 80.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "HkW3nTM6X.S1d278zJ4.00", "instruction": "Rephrase the first long sentence to better fit the academic style.", "revised_paragraph": "Revised paragraph: \"The SDE model, as depicted in Figures 2c-d, clearly demonstrates that a time-variant drift function results in a reduction of prediction error, suggesting an improved understanding of the dynamics associated with a golf swing motion. Additionally, it is evident that the prediction error consistently decreases with an increase in the number of inducing points (M), achieving a minimum when M equals 80.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.17", "instruction": "Make the first sentence more concise.", "revised_paragraph": "Revised paragraph: \"To address the significant variance in the IHDP benchmark, we conduct an ablation study on the ACIC benchmark to evaluate the effectiveness of ESCFR\u2019s components and validate our claims in Section 3. In Table 2, ESCFR first augments TARNet with stochastic optimal transport in Section 3.1, which effectively reduces the out-of-sample PEHE from 3.254 to 3.207. It then mitigates the MSE issue with RMPR in Section 3.2 and the UCE issue with PFOR in Section 3.3, reducing the out-of-sample PEHE to 2.768 and 2.633, respectively. Finally, ESCFR combines the RMPR and PFOR in a unified framework in Section 3.4, further reducing the out-of-sample PEHE to 2.316.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.17", "instruction": "Simplify the first sentence. Improve the connections between sentences.", "revised_paragraph": "Revised paragraph: \"To address the significant variance in the IHDP benchmark, we perform an ablation study on the ACIC benchmark to assess the effectiveness of ESCFR's components and support our claims from Section 3. Table 2 shows that ESCFR first enhances TARNet with stochastic optimal transport, leading to a reduction in out-of-sample PEHE from 3.254 to 3.207. Next, it tackles the MSE issue using RMPR, which further decreases the out-of-sample PEHE to 2.768, and subsequently addresses the UCE issue with PFOR, lowering the out-of-sample PEHE to 2.633. Lastly, in Section 3.4, ESCFR integrates RMPR and PFOR into a unified framework, achieving an additional reduction in out-of-sample PEHE to 2.316.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.10", "instruction": "Improve the English of this paragraph", "revised_paragraph": "Revised paragraph: \"We refer to the training steps during which we conduct forward and backward passes as regular steps. Additionally, we introduce re-balancing steps, wherein we compel the model to update only one of the unimodal branches to enhance learning from the corresponding input modality. For a comprehensive explanation of the re-balancing step, please see Appendix A.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.10", "instruction": "Rewrite  ", "revised_paragraph": "Revised paragraph: \"In our study, we denote the training steps where both forward and backward passes are conducted as regular steps. To enhance the learning process for specific input modalities, we implement re-balancing steps, during which the model is required to update exclusively one of the unimodal branches. A comprehensive explanation of the re-balancing step can be found in Appendix A.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.02", "instruction": "Can you make the last sentence simple?", "revised_paragraph": "Revised paragraph: \"Deep CNN is first utilized for image SR in SRCNN (Dong et al., 2014) and continuously shows promising SR performance. There are only three convolutional (Conv) layers in SRCNN, hindering its performance. Kim et al. increased the network depth in VDSR (Kim et al., 2016a) with residual learning and obtained notable improvements over SRCNN. Lim et al. (Lim et al., 2017) built a much deeper network EDSR by using simplified residual blocks. Zhang et al. (Zhang et al., 2018b) proposed RCAN, which is one of the deepest networks in SR. With increased network size, very deep networks, like EDSR (Lim et al., 2017) and RCAN (Zhang et al., 2018b), have achieved remarkable SR performance. However, they also have a large number of parameters and require significant computational resources, making them difficult to use on devices with limited resources (Lee et al., 2020).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.02", "instruction": "Use shorter, more direct formulations to make this paragraph more concise. Rewrite the last two sentences to make them more understandable.", "revised_paragraph": "Revised paragraph: \"Deep CNNs were first applied to image super-resolution (SR) in SRCNN (Dong et al., 2014), showing promising results. However, SRCNN's three convolutional layers limit its performance. Kim et al. enhanced the network depth in VDSR (Kim et al., 2016a) using residual learning, leading to significant improvements over SRCNN. Lim et al. (Lim et al., 2017) developed EDSR, a much deeper network utilizing simplified residual blocks. Zhang et al. (Zhang et al., 2018b) introduced RCAN, one of the deepest SR networks. While deeper networks like EDSR and RCAN have achieved impressive SR performance, they also come with heavy model parameters, high computational demands, and longer inference times. This makes them difficult to deploy on resource-limited platforms without dedicated neural processing units or off-chip memory (Lee et al., 2020).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.09", "instruction": "At the last part, state that the proof will be shown in the appendix. Also, make the sentence more sophisticated.", "revised_paragraph": "Revised paragraph: \"In addition, we derive the upper bound of the Potential Outcome Heterogeneity Estimate (PEHE) in the stochastic batch form, as articulated in Theorem 3.1, drawing upon the framework established by Uri et al. (2017). This analysis reveals that the PEHE can be systematically optimized through an iterative process aimed at minimizing both the factual outcome estimation error and the optimal transport discrepancy at the mini-batch level. The comprehensive proof for this derivation is provided in the appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.09", "instruction": "Make this sentence more concise. Add a reference to Appendix A.3. where the proof is.", "revised_paragraph": "Revised paragraph: \"We derive the upper bound of PEHE in stochastic batch form, as outlined in Theorem 3.1, based on Uri et al. (2017), showing that PEHE optimization can be achieved by iteratively minimizing factual outcome estimation error and optimal transport discrepancy at the mini-batch level (see Appendix A.3 for proof).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "5t8NvKONr.tls-ZX2iE.00", "instruction": "Use correct citation format.", "revised_paragraph": "Revised paragraph: \"In this section, we clarify the complexity of the DeepONet required for approximation \\( A \\) and reconstruction \\( R \\) based on the theory presented by Galanti and Wolf (2020). Furthermore, utilizing the results regarding the upper bound for the complexity of hypernetworks as discussed by Galanti and Wolf (2020), we demonstrate that the HyperDeepONet entails relatively lower complexity than the DeepONet.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "5t8NvKONr.tls-ZX2iE.00", "instruction": "Reorder the last sentence.", "revised_paragraph": "Revised paragraph: \"In this section, we would like to clarify the complexity of the DeepONet required for the approximation A and reconstruction R based on the theory in Galanti & Wolf (2020). Furthermore, we will show that the HyperDeepONet entails a relatively lower complexity than the DeepONet using the results on the upper bound for the complexity of hypernetwork from Galanti & Wolf (2020).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CswFOyPyhT.FUeqrAFby.00", "instruction": "Make the citation in correct order.", "revised_paragraph": "Revised paragraph: \"Previous work has shown that GFlowNets are useful in settings with multi-modal posteriors, which is of particular interest to us because many admissible structures can explain the observed data equally well. Next, we discuss a toy system that has many modes in section 5, and then we present our GFlowNet-based solution in section 4.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CswFOyPyhT.FUeqrAFby.00", "instruction": "Revise this paragraph to present the sections in a coherent order.", "revised_paragraph": "Revised paragraph: \"Previous work has shown that GFlowNets are useful in settings with multi-modal posteriors, which is particularly relevant in scenarios where many admissible structures can equally explain the observed data. In section 5, we discuss a toy system that exhibits multiple modes, followed by our presentation of the GFlowNet-based solution in section 4.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.10", "instruction": "Remove unnnecesary examples.", "revised_paragraph": "Revised paragraph: \"We would like to highlight that sequence-based (evolution-based) methods for single proteins are not suitable for protein-protein interactions due to the lack of evolutionary information in most cases\u2014protein-protein interactions involve two or more chains. These limitations render it infeasible to predict mutational effects by mining sequence databases using existing powerful tools such as multiple sequence alignments and protein language models. Therefore, effective methods for predicting mutational effects on binding currently rely on structural rather than sequence information alone.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.10", "instruction": "Rewrite this paragraph to make it shorter while keeping all the informations.", "revised_paragraph": "Revised paragraph: Sequence-based (evolution-based) methods for single proteins are unsuitable for predicting protein-protein interactions, as they often lack necessary evolutionary information involving multiple chains, which may come from different species (e.g., host and virus). Additionally, inter-chain co-evolution may not occur, such as during immune responses where antibodies rapidly clear pathogens, precluding their evolution. Consequently, using sequence databases and tools like multiple sequence alignments or protein language models to predict mutational effects is often ineffective. Therefore, current effective methods for predicting binding mutational effects rely more on structural data than on sequences alone.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.20", "instruction": "Make this paragraph more clear.", "revised_paragraph": "Revised paragraph: \"RDE-Linear demonstrates performance that is comparable to Rosetta and surpasses several unsupervised representation learning baselines. However, it does not outperform the majority of the baselines when evaluated across the entire SKEMPI2 dataset. Notably, its performance significantly improves when focusing exclusively on single-point mutations (as shown in Table 6 in the appendix). This difference in performance may be attributed to the fact that simple linear models struggle to effectively capture the complex, non-linear relationships that characterize multi-point mutations. Nevertheless, RDE-Linear illustrates that leveraging simple statistics from the estimated rotamer density can successfully predict \u2206\u2206 G, thereby establishing a foundation for the more precise RDE-Network.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.20", "instruction": "Improve the English in this paragraph.", "revised_paragraph": "Revised paragraph: \"RDE-Linear demonstrates performance that is comparable to Rosetta and surpasses certain unsupervised representation learning baselines. Although it does not exceed the performance of most baselines across the entire SKEMPI2 dataset, we observe that its performance significantly improves when we focus exclusively on single-point mutations (see Table 6 in the appendix). This discrepancy may be attributed to the limitations of simple linear models in adequately capturing the non-linear relationships prevalent in multi-point mutations. Nevertheless, RDE-Linear illustrates that utilizing simple statistics of the estimated rotamer density alone can effectively predict \u2206\u2206G, thereby establishing a foundation for the more accurate RDE-Network.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hoDIDL_gcF.NYncw25Z2M.00", "instruction": "Make this paragraph shorter", "revised_paragraph": "Revised paragraph: \"Figure 2 presents the results, indicating that N. + emp + opt effectively leverages the fast spectral decay of the Gaussian kernel, significantly outperforming other methods. Additionally, N. + emp and Thinning demonstrate a respectable convergence rate similar to Herding and iid Bayes, despite not utilizing expectation knowledge.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hoDIDL_gcF.NYncw25Z2M.00", "instruction": "Make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"Figure 2 displays the results, indicating that N. + emp + opt effectively leverages the fast spectral decay of the Gaussian kernel, significantly outperforming other methods. Additionally, N. + emp and Thinning demonstrate a comparable convergence rate to Herding or iid Bayes, despite not utilizing any expectation knowledge.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "IoTyuVEanE.Et-c0vQfeb.01", "instruction": "Rephrase the last sentence.", "revised_paragraph": "Revised paragraph: \"Snuba [14] generates candidate rules using a collection of weak learner primitives (e.g., decision stumps, k-nearest neighbors) and then synthesizes and prunes this set of rules to generate final rules for labeling. ReGAL combines elements of this framework to facilitate iterative communication between downstream classifiers and rule selectors, enabling a mutual enhancement between the two. This distinctive approach allows ReGAL to provide the convenience of model-generated labeling functions while also benefiting from the nuanced insights of human annotator input.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "IoTyuVEanE.Et-c0vQfeb.01", "instruction": "Rewrite the last sentence to better convey the idea.", "revised_paragraph": "Revised paragraph: \"Snuba [14] generates candidate rules using a collection of weak learner primitives (e.g., decision stumps, k-nearest neighbors) and then synthesizes and prunes this set of rules to generate final rules for labeling. ReGAL combines elements of this framework to facilitate iterative communication between downstream classifiers and rule selectors, enabling them to mutually enhance one another. Consequently, ReGAL uniquely provides the advantages of model-generated labeling functions while also incorporating the valuable, nuanced insights derived from human annotator input.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "instruction": "Make expression concrete, correct typos.", "revised_paragraph": "Revised paragraph: \"The experimental results are summarized in Table 5, where CIFAR-10 is used as in-distribution (ID) data and CIFAR-100 as out-of-distribution (OOD) data. The common watermark learning setup (denoted as 'common') demonstrates improved performance in near-OOD detection compared to cases without watermarking (referred to as 'w/o watermark'). Furthermore, implementing watermarking with shifting augmentations, specifically permutation and rotation, significantly enhances the models' detection capabilities, resulting in improvements of up to 8.60 and 4.70 in the false positive rate at 95% (FPR95) for the softmax and free energy scoring methods, respectively.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"The experimental results are summarized in Table 5, which presents CIFAR-10 as the in-distribution (ID) data and CIFAR-100 as the out-of-distribution (OOD) data. In this context, the common watermark learning setup significantly enhances the performance of near-OOD detection compared to the cases without watermarking (denoted as w/o watermark). Furthermore, the incorporation of watermarking with shifting augmentations (specifically permutation and rotation) further enhances the models' detection capabilities, resulting in improvements of up to 8.60 and 4.70 in the false positive rate at 95% recall (FPR95) for softmax and free energy scoring, respectively.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "3686sm4Cs.AJMXMDLVn.00", "instruction": "Edit the first part of this paragraph for conciseness.", "revised_paragraph": "Revised paragraph: \"SuperWeights are linear combinations of templates reused across multiple layers. They capture single operations on input features (e.g., edge or texture detection) and are generated through weighted combinations of one or more templates of trainable parameters from Weight Templates. To generate the weights for a single layer, we first construct SuperWeights from the trainable parameters in Weight Templates (discussed in Section 3.1) and then concatenate all SuperWeights used by the layer to create its final weights (illustrated in the center-right column of Figure 2).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "3686sm4Cs.AJMXMDLVn.00", "instruction": "Make the first half of the paragraphe shorter by merging the two sentences and removing the details about how superweights are generated.", "revised_paragraph": "Revised paragraph: SuperWeights, which capture a single operation on the input features, are constructed by concatenating the SuperWeights used by the layer to create its final weights.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1AJ47hjr.B1E3IvnsH.00", "instruction": "I want to remove the last sentence.", "revised_paragraph": "Revised paragraph: \"Results for K = 20 are depicted in Figure 2b. All approaches update the Q-function with a learning rate of 10\u22123 on the same fixed batch of 10^3 episodes with a percentage of 10% non-optimal transitions. For the multi-step approaches, we set rollout length n = 4. Since there is no generalization among states in the tabular setting, we update the Shifted Q-function with a learning rate of 10\u2212and the Truncated Q-functions with a learning rate of 10\u22123.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1AJ47hjr.B1E3IvnsH.00", "instruction": "Delete the last sentence.", "revised_paragraph": "Revised paragraph: \"Results for K = 20 are depicted in Figure 2b. All approaches update the Q-function with a learning rate of 10 \u2212 3 on the same \ufb01xed batch of 10 3 episodes with a percentage of 10% non-optimal transitions. For the multi-step approaches, we set rollout length n = 4 . Since there is no generalization among states in the tabular setting, we update the Shifted Q-function with a learning rate of 10 \u2212 and the Truncated Q-functions with a learning rate of 10 \u2212 3.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.18", "instruction": "Please, remove unnecessary details of this paragraph", "revised_paragraph": "Revised paragraph: The datasets for training the rotamer density estimator are derived from PDB-REDO (Joosten et al., 2014), a database of refined X-ray structures. Structures with a resolution worse than 3.5 \u02daA are excluded. Protein chains are clustered by 50% sequence identity, resulting in 38,413 clusters, which are then divided into training, validation, and test sets in a 95%/0.5%/4.5% ratio. During training, a cluster is randomly selected, and a chain is chosen from it. The structure is cropped into a patch of 128 amino acids by selecting a seed amino acid and its 127 nearest neighbors based on C-beta distances. To simulate mutations, 10% of the amino acids' rotamers in the patch are masked, and noise is added to the rotamers of amino acids within 8 \u02daA of the nearest masked amino acid.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.18", "instruction": "Delete unnecessary details. Improve the linking between ideas.", "revised_paragraph": "Revised paragraph: The datasets for training the rotamer density estimator are sourced from PDB-REDO (Joosten et al., 2014), which provides refined X-ray structures from the PDB, excluding structures with resolutions worse than 3.5 \u00c5. The protein chains are clustered based on 50% sequence identity, resulting in 38,413 clusters that are randomly allocated into training, validation, and test sets in a ratio of 95%/0.5%/4.5%. During training, a cluster is randomly selected, followed by the selection of a chain from that cluster. Each structure is cropped into a patch of 128 amino acids, starting with a seed amino acid and identifying its 127 nearest neighbors based on C-beta distances. To simulate mutations, 10% of the amino acids in the patch have their rotamers masked, and additional noise is introduced to the rotamers of amino acids whose C-beta distance to the nearest masked amino acid is less than 8 \u00c5.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.13", "instruction": "Check the citation mark format and rewrite", "revised_paragraph": "Revised paragraph: The NVIDIA Dynamic Hand Gesture Dataset, abbreviated as NVGesture (Molchanov et al., 2015), comprises 1,532 video clips, with 1,050 designated for training and 482 for testing, spanning 25 distinct gesture classes. We select 20% of the training samples to form the validation set and utilize both depth and RGB modalities. For data preparation, we adopt the configuration outlined by Joze et al. (2020) and implement I3D as proposed by Carreira and Zisserman (2017) for the uni-modal branches, while employing MMTMs as fusion modules within the six final inception modules.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.13", "instruction": "Make the paragraph slightly more precise.", "revised_paragraph": "Revised paragraph: \"The NVIDIA Dynamic Hand Gesture Dataset, abbreviated as NVGesture (Molchanov et al., 2015), comprises 1,532 video clips categorized into 25 classes, with 1,050 clips designated for training and 482 for testing. We allocate 20% of the training samples for validation and utilize depth and RGB as two modalities. For data preparation, we follow the configuration established by Joze et al. (2020), employing I3D Carreira & Zisserman (2017) as the uni-modal branches and MMTMs as fusion modules within the six final inception modules.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.08", "instruction": "Make expression concrete, add conjunction.", "revised_paragraph": "Revised paragraph: \"As demonstrated in \u00a73.2, the imbalance in conditional utilization rates indicates that the model is exploiting the connection between the target and only one of the input modalities while simultaneously ignoring the valuable crossmodal information. However, the conditional utilization rate can only be measured after training is completed, which makes it challenging to utilize this metric in real-time during the training process. Therefore, we derive a proxy metric, referred to as conditional learning speed, that effectively captures the relative learning speed between modalities throughout the training.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.08", "instruction": "Revise the wording of the middle sentence in this paragraph.", "revised_paragraph": "Revised paragraph: \"As demonstrated in \u00a73.2, the imbalance in conditional utilization rates is a sign of the model exploiting the connection between the target and only one of the input modalities, ignoring crossmodal information. However, measuring the conditional utilization rate is only feasible after the training process is complete, which poses challenges for its application in real-time training scenarios. We instead derive a proxy metric, called conditional learning speed, that captures relative learning speed between modalities during training.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.01", "instruction": "Remove the part of the sentence that talks about application managers. Improve the english of this paragraph.", "revised_paragraph": "Revised paragraph: \"Estimating individual treatment effect (ITE) with randomized controlled trials (RCTs) is a common practice in causal inference, widely utilized in various fields, including e-commerce (Betlei et al., 2021), education (Cordero et al., 2018), and health care (Schwab et al., 2020). For instance, marketing strategies might be imposed on randomly sampled users to evaluate their potential impact on click-through rates, while drug developers conduct clinical A/B tests to assess drug effects. Although RCTs are considered the gold standard for causal inference (Judea & Dana, 2018), their implementation is often prohibitively expensive. Consequently, researchers have increasingly turned to observational data, which can be obtained without intervention, as a compelling alternative. Drug developers, for instance, frequently rely on post-marketing monitoring reports rather than conducting clinical A/B trials to evaluate drug effects. As access to observational data expands, estimating ITE from such data has garnered significant research interest.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.01", "instruction": "Make the sentence 2 more concise. Make the sentence 3 more formal.", "revised_paragraph": "Revised paragraph: \"Estimating individual treatment effects (ITE) using randomized controlled trials (RCTs) is a prevalent practice in causal inference, widely applied in e-commerce (Betlei et al., 2021), education (Cordero et al., 2018), and healthcare (Schwab et al., 2020). For instance, application managers may implement a marketing strategy on randomly selected users to evaluate its impact on the click-through rate, while drug developers may conduct clinical A/B tests to assess drug effects. Although RCTs are considered the gold standard for causal inference (Judea & Dana, 2018), they often entail high costs, making randomized experiments impractical. Consequently, observational data\u2014acquired without intervention\u2014has emerged as an appealing alternative. For example, drug developers frequently rely on post-marketing monitoring reports to evaluate drug effects instead of conducting clinical A/B trials. With the increasing availability of observational data, estimating ITE from such data has garnered significant research interest.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "LWsBC35BgW.r1AX7JdZ0.00", "instruction": "Remove the second last sentence ", "revised_paragraph": "Revised paragraph: The trend we observe from Lemma 3.2 is that activation functions whose Hermite coefficients decay quickly, such as \u03c9 \u03c3, result in a faster decay of the NTK coefficients. We remark that analyzing the rates of decay for l \u2265 3 is challenging due to the calculation of F (p, k, \u00af \u03b1 l \u2212 1) (4). Additionally, in Appendix B.4 we show that the zeroth coefficient strictly increases with depth. Finally, we briefly pause here to highlight the potential for using a truncation of (5) in order to perform efficient numerical approximation of the infinite width NTK.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "LWsBC35BgW.r1AX7JdZ0.00", "instruction": "Remove unnecessary information, use accurate expression and evidence.", "revised_paragraph": "Revised paragraph: \"Lemma 3.2 indicates that activation functions with rapidly decaying Hermite coefficients, such as \u03c9 \u03c3, lead to a quicker decay of the NTK coefficients. Analyzing decay rates for l \u2265 3 proves challenging due to the complexity of calculating F(p, k, \u00af \u03b1 l \u2212 1) (4). In Appendix B, we present preliminary results that demonstrate a specific scenario where increased depth results in slower coefficient decay. Additionally, Appendix B.4 reveals that the zeroth coefficient increases with depth. We also highlight the potential for using a truncation of equation (5) for efficient numerical approximation of the infinite-width NTK.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.05", "instruction": "Rewrite the opening sentences of the paragraph to make them more explicit and clear.", "revised_paragraph": "Revised paragraph: Ethier et al. [58] identify six fundamental components that outline the administrative procedures for prescribing a drug. These components are as follows: 1) drug prescription item, which delineates actions related to one or more drugs; 2) drug administration specification, detailing the specific drug product; 3) drug course specification, which outlines the duration, initiation, and termination of treatment; 4) drug dosage specification, addressing the exact dosage of the drug; 5) drug dose administration specification, which provides administration instructions; and 6) drug dispensing specification, which pertains to the dispensing of the drug product. Kumar et al. [59] further condense these components into four categories: superscription (directive to take), inscription (name and dose), subscription (directions to the pharmacists), and signature (instructions for the patient). Fox [60] offers a different breakdown into drug name, drug dose, drug dose units, drug dose frequency, duration (including start and end dates), and indication. In alignment with previous studies (e.g., [14,31]), we adhere to Fox\u2019s classification [60], favoring its simplicity and clarity in naming conventions.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.05", "instruction": "Replace the listing by normal text to better incorporate de six building blocks into the paragraph.", "revised_paragraph": "Revised paragraph: According to Ethier et al. [58], a prescription comprises six essential building blocks that delineate the modalities of administration for a given drug. These building blocks include the drug prescription item, which specifies actions related to one or several drugs; the drug administration specification that defines the drug product; the drug course specification that outlines the duration, initiation, and termination of treatment; the drug dosage specification detailing the required dosage; the drug dose administration specification, which provides instructions for administration; and finally, the drug dispensing specification that indicates how the drug product should be dispensed. Kumar et al. [59] consolidate these components into four categories: superscription (directive to take), inscription (name and dose), subscription (directions to the pharmacists), and signature (instructions for the patient). Additionally, Fox [60] classifies them as drug name, drug dose, drug dose units, drug dose frequency, duration (encompassing start and end dates), and indication. In line with other studies (e.g., [14,31]), we adopt Fox's classification [60] due to its simplicity and clarity in naming convention.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "g5N2H6sr7.6J3ec8Dl3p.01", "instruction": "Make the text more concise by describing concepts more high-level.", "revised_paragraph": "Revised paragraph: \"Feature transformation can enhance filter strength (Li et al., 2019b). As discussed in Section 3.2.2, the input of GDN consists of smoothed representations H (cid:48) and the recovered structure A (cid:48). Consequently, the inverse of GCN can be expressed as:\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "g5N2H6sr7.6J3ec8Dl3p.01", "instruction": "Make the text more direct and concise.", "revised_paragraph": "Revised paragraph: \"A feature transformation can enhance filter strength (Li et al., 2019b). As noted in Section 3.2.2, the GDN input consists of the smoothed representations H (cid:48) and the recovered structure A (cid:48). Consequently, the inverse version of GCN can be expressed as:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "9T9ueD0PUu.7Pcj7508I2.01", "instruction": "Rewrite the first sentence to also explain the structure of the section.", "revised_paragraph": "Revised paragraph: \"This section begins with an overview of MIMO's experiments, which are based on the Uncertainty Baselines framework, and subsequently discusses the methodology for benchmarking performance against high-quality, well-optimized implementations of baseline methods (for additional baselines, refer to the framework). We investigated three model/dataset combinations: ResNet28-10/CIFAR10, ResNet28-10/CIFAR100, and ResNet50/ImageNet. MIMO\u2019s code will be open-sourced.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "9T9ueD0PUu.7Pcj7508I2.01", "instruction": "Rewrite the first sentence to better introduce the section.", "revised_paragraph": "Revised paragraph: \"This section examines the experiments conducted with MIMO, which are grounded in the Uncertainty Baselines framework. This framework allows us to benchmark the performance and to compare against high-quality, well-optimized implementations of baseline methods (see framework for further baselines than ones highlighted here). We looked at three model/dataset combinations: ResNet28-10/CIFAR10, ResNet28-10/CIFAR100, and ResNet50/ImageNet. MIMO\u2019s code will be open-sourced.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.21", "instruction": "Please, review the following paragraph, rewrite it in a clearer way", "revised_paragraph": "Revised paragraph: \"Visual Comparisons. We present visual comparisons (\u00d7 4) in Fig. 4 for challenging scenarios. The results indicate that most of the methods we compared struggle to recover structural details accurately and exhibit significant blurring artifacts. In contrast, our SRPN-L method effectively reduces these blurring artifacts and retrieves more structural information. These visual assessments align with the quantitative findings, highlighting the superiority of our SRP method.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.21", "instruction": "Rephrase the text and change SRPN-L to SRPN-Lite", "revised_paragraph": "Revised paragraph: \"Visual Comparisons. Additionally, we present visual comparisons (\u00d7 4) in Fig. 4 for more challenging cases. It is evident that the majority of the methods being compared struggle to recover structural details with accurate orientation and are prone to blurring artifacts. In contrast, our SRPN-Lite demonstrates a greater ability to mitigate blurring artifacts and recover more structural information. These visual comparisons align with the quantitative results, highlighting the superiority of our SRP method.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "YkiRt7L93m.jgDbnUD7s.04", "instruction": "Please, make this paragraph mor concise, delete unnecessary details", "revised_paragraph": "Revised paragraph: \"We developed a projection method for probability measures supported on R^d, leveraging the tangential structure of the 2-Wasserstein space. This method approximates a target multivariate distribution using selected control distributions and provides unique, interpretable weights for regular probability measures. For general measures, we create a regular tangent space by applying barycentric projection to optimal transport plans. Our application to evaluate the effects of Medicaid expansion in Montana through an extended synthetic controls estimator illustrates the method's efficiency and its applicability to general probability measures. The approach permits extrapolation beyond the convex hull of control units, facilitating tangential regression, and can be extended to a continuum of measures using established barycenter consistency results.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "YkiRt7L93m.jgDbnUD7s.04", "instruction": "Combine sentences 3 and 4 into a really short one keeping only the main idea. Improve the choice of wording.", "revised_paragraph": "Revised paragraph: \"We have developed a projection method between sets of probability measures supported on R d, leveraging the tangential structure of the 2-Wasserstein space to approximate a potentially multivariate target distribution using selected control distributions. Our implementation yields unique, interpretable weights for regular probability measures, while for general measures, we create a regular tangent space through barycentric projection applied to optimal transport plans. Our application to assessing the first- and second-order effects of Medicaid expansion in Montana, via an extension of the synthetic controls estimator (Abadie & Gardeazabal, 2003; Abadie et al., 2010), highlights the method\u2019s efficiency and the need for applicability to general probability measures. Notably, the approach accommodates optimal weights outside the unit simplex, enabling extrapolation beyond the convex hull of control units, thus providing a framework for tangential regression. Additionally, it can be extended to a continuum of measures, drawing on established consistency results of barycenters (e.g., Le Gouic & Loubes, 2017).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.09", "instruction": "Simplify the first sentence.", "revised_paragraph": "Revised paragraph: \"Using the implicit function theorem on Equation 8, we derive the Jacobians \u2202 Y \u2217 ( z , \u03b8 ) /\u2202 z and \u2202 Y \u2217 ( z , \u03b8 ) /\u2202 \u03b8 , which are necessary for iDSPN to fit into an autodiff framework. Appendix B contains the full details of how this works.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.09", "instruction": "Improve the readability of those sentences.", "revised_paragraph": "Revised paragraph: \"By using the implicit function theorem on Equation 8, we derive the Jacobians \u2202Y\u2217(z, \u03b8)/\u2202z and \u2202Y\u2217(z, \u03b8)/\u2202\u03b8. These derivatives are essential for integrating iDSPN into an autodiff framework. Further details on this process can be found in Appendix B.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.00", "instruction": "Rephrase the text", "revised_paragraph": "Revised paragraph: \"We present a proactive multi-camera collaboration framework utilizing multi-agent reinforcement learning (MARL) to enable real-time distributive adjustments in multi-camera formations for 3D human pose estimation in crowded environments. In our approach, multiple camera agents collaborate seamlessly to achieve successful reconstructions of 3D human poses. Moreover, this decentralized framework provides flexibility regarding formation size and removes the need for a control hierarchy or centralized authority. Concerning the initial challenge, we emphasize the critical importance of the model's ability to predict human movements and future environmental states. To incorporate these capabilities into the state representations, we implement World Dynamics Learning in our model training, which involves five auxiliary tasks aimed at predicting the target's position, the positions of pedestrians, the self-state, the states of teammates, and the team reward.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.00", "instruction": "Remove redundant words.", "revised_paragraph": "Revised paragraph: \"We introduce a proactive multi-camera collaboration framework based on multi-agent reinforcement learning (MARL) for real-time adjustments of multi-camera formations for 3D human pose estimation in crowds. In our approach, multiple camera agents collaborate seamlessly to reconstruct 3D human poses. Additionally, it is a decentralized framework that offers flexibility in formation size and eliminates reliance on a control hierarchy or centralized entity. Regarding the first challenge, we emphasize the model\u2019s ability to anticipate human motions and future environmental states. To incorporate these properties into the state representations, we integrate World Dynamics Learning into our model's training, utilizing five auxiliary tasks to predict the target\u2019s position, pedestrians\u2019 positions, self-state, teammates\u2019 states, and team reward.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MnewiFDvHZ.iAYttXl-uH.01", "instruction": "Remove the second last sentence", "revised_paragraph": "Revised paragraph: COCO-Soft with adversarial constraints: Adversarial constraints are more difficult to satisfy but have been considered in the literature [24, 20, 16, 6, 22]. For COCO-Soft with adversarial constraints, the authors in [24] developed an online mirrored descent type algorithm that achieves O(p? T q) regret and O(p T^{3/4} q) violation. Later, [6, 16] generalized the baseline in [24] and still achieve O(p? T q) regret and O(p T^{3/4} q) violation. With Slater\u2019s condition, [20] presents an online gradient descent algorithm based on the drift-plus-penalty method [19], which achieves O(p? T q) regret and O(p? T q) violation and [22] extended it to an online optimization with sub-modular losses. It remains open that whether O(p T^{3/4} q) violation can be reduced with adversarial constraints (soft or hard) while keeping O(p? T q) regret without Slater\u2019s condition.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MnewiFDvHZ.iAYttXl-uH.01", "instruction": "Remove unnecessary sentence from this paragraph to make it shorter.", "revised_paragraph": "Revised paragraph: \"COCO-Soft with adversarial constraints: Adversarial constraints are more difficult to satisfy but have been considered in the literature [24, 20, 16, 6, 22]. For COCO-Soft with adversarial constraints, the authors in [24] developed an online mirrored descent type algorithm that achieves O(p? T q) regret and O(p T^{3/4} q) violation. Later, [6, 16] generalized the baseline in [24] while still achieving O(p? T q) regret and O(p T^{3/4} q) violation. With Slater\u2019s condition, [20] presents an online gradient descent algorithm based on the drift-plus-penalty method [19], achieving O(p? T q) regret and O(p? T q) violation. Note the key improvement in these works is the Lyapunov drift technique that provides a refined bound on virtual queues with Slater\u2019s condition, achieving a smaller soft constraint violation. It remains open whether O(p T^{3/4} q) violation can be reduced with adversarial constraints while keeping O(p? T q) regret without Slater\u2019s condition.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "XwpokDSFR.0bM_dGxEwf.00", "instruction": "Review this paragraph, remove unnecessary details", "revised_paragraph": "Revised paragraph: Training flow-based encoders involves designing an effective training procedure for our proposed architecture. A key challenge is that the statistical distance is not differentiable, as the classifier \u00b5 \u2217 is binary. To address this, we replace it with a differentiable proxy. Pinsker\u2019s inequality allows us to bound the statistical distance between Z and Z 1 using symmetrized KL divergence between Z 0 and Z 1. We outline our training procedure in Algorithm 1, where we sample batches from the respective distributions and encode them into representations z 0 and z 1. We estimate the symmetrized KL-divergence, denoted as L 0 + L 1, and combine it with a classification loss L clf using a tradeoff parameter \u03b3, subsequently performing a gradient step to minimize the joint loss. Although we utilize a convex scalarization scheme for the joint loss, our approach remains flexible regarding the choice of multi-objective optimization strategies, as discussed in Appendix C.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "XwpokDSFR.0bM_dGxEwf.00", "instruction": "Make the second sentence more concise and fitting to the academic style.", "revised_paragraph": "Revised paragraph: \"Training flow-based encoders presents the challenge of designing an effective training procedure for our newly proposed architecture. The primary issue arises from the non-differentiability of the statistical distance due to the binary nature of classifier \u00b5 \u2217, necessitating the use of a differentiable proxy. According to Pinsker\u2019s inequality, the statistical distance between Z and Z 1 can be bounded using the symmetrized KL divergence between Z 0 and Z 1 (see Appendix A for a more detailed proof). A high-level description of our training procedure is outlined in Algorithm 1. In each step, we sample a batch of x 0 and x from their respective distributions, encoding them into representations z 0 and z 1. We estimate the symmetrized KL-divergence between distributions Z 0 and Z 1, represented as L 0 + L 1, and combine it with a classification loss L clf using a tradeoff parameter \u03b3, followed by a gradient step to minimize the joint loss. Although we employ a convex scalarization scheme to derive the joint loss in Algorithm 1, our method remains compatible with various multi-objective optimization objectives. We will demonstrate compatibility with alternative scalarization schemes in Appendix C.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.16", "instruction": "Remove redundant information and use more scientific words.", "revised_paragraph": "Revised paragraph: \"Representation-based methods alleviate treatment selection bias and enhance overall performance. In particular, CFR-WASS achieves an out-of-sample PEHE of 3.207 on ACIC, substantially surpassing most statistical methods, and an AUUC of 0.715 on IHDP, outperforming all other baselines. Nonetheless, Mean Squared Error (MSE) and Uncertainty of Causal Effect (UCE) issues impede the efficacy of these methods in mitigating treatment selection bias. The proposed ESCFR demonstrates significant improvements across various metrics compared to multiple state-of-the-art baselines. We attribute its superiority to the design of the Robust Multi-Point Regularization (RMPR) and Penalized Function Outlier Reduction (PFOR) regularizers, which enhance its resilience to MSE and UCE.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.16", "instruction": "Make this text simpler and more readable. Remove unnecessary details about AUUC.", "revised_paragraph": "Revised paragraph: \"Representation-based methods help reduce treatment selection bias and improve overall performance. For example, CFR-WASS achieves an out-of-sample PEHE of 3.207 on ACIC, performing better than most statistical methods. It also has an AUUC of 0.715 on IHDP, outperforming all other baselines. However, MSE and UCE issues limit how effectively these methods can reduce treatment selection bias. The proposed ESCFR shows significant improvements in many metrics compared to various advanced baselines. We believe its superior performance is due to the design of the RMPR and PFOR regularizers, which enhance its robustness against MSE and UCE.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "9wfZbn73om.FhHH15YtKt.03", "instruction": "Use formal words.", "revised_paragraph": "Revised paragraph: \"The canine images depicted in Figure 2, while exhibiting considerable dissimilarities at the pixel level, encapsulate analogous semantic significances. Concurrently, they manifest a minimal augmented distance. Consequently, the semantic distance may be partially delineated by the proposed augmented distance. Building upon this augmented distance, we shall now introduce the (\u03c3, \u03b4)-augmentation to assess the concentration of augmented data.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "9wfZbn73om.FhHH15YtKt.03", "instruction": "Improve this paragraph (mostly the first sentence) to make it less confusing.", "revised_paragraph": "Revised paragraph: \"The dog images depicted in Figure 2, while differing significantly at the pixel level, share comparable semantic meanings. Consequently, they exhibit a small augmented distance. This indicates that the semantic distance can be partially represented by the proposed augmented distance. Based on this augmented distance, we now introduce the (\u03c3, \u03b4)-augmentation to assess the concentration of augmented data.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.20", "instruction": "Rewrite the last sentence, splitting it into two to make it easier to understand", "revised_paragraph": "Revised paragraph: \"Figure 13(b) shows the result on both train and test actions. AGILE-Tuned, with pre-summarizer, and No twin-GAT demonstrated similar performance, which is superior to No target-q-change. The difference between AGILE-Tuned and No target-q-change indicates that the cascaded network considers the intermediate list constructed during decision-making. In the computation of the target q-values, the agent also needs to refer to the q-value of the next position in the same list rather than another list from a future time-step.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.20", "instruction": "Simplify heavily the explanations in this paragraph keeping the main points.", "revised_paragraph": "Revised paragraph: \"Figure 13(b) presents the results for both training and testing actions. AGILE-Tuned, With pre-summarizer, and No twin-GAT performed similarly well, outperforming No target-q-change. The comparison between AGILE-Tuned and No target-q-change highlights that the cascaded network benefits from considering the intermediate list during decision-making, requiring the agent to reference the q-value of the next position in the same list rather than a future list.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.21", "instruction": "Simplify the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"Sequence-based models do not accurately predict \u2206\u2206 G for protein-protein binding, as discussed in Section 2.2. Figure 3 shows the distribution of correlation coefficients for each complex. For more results and discussion, see Section B of the appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.21", "instruction": "Improve the English in this paragraph.", "revised_paragraph": "Revised paragraph: \"Sequence-based models do not accurately predict \u2206\u2206G for protein-protein binding, as discussed in Section 2.2. Figure 3 illustrates the distribution of per-complex correlation coefficients. For additional results and discussion, please refer to Section B of the appendix.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.01", "instruction": "Switch the second and the third setence and then rephrase the first three sentences", "revised_paragraph": "Revised paragraph: \"We approach the challenge of active 3D human pose estimation within a crowded environment by framing it as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) and present an innovative multi-camera collaboration framework comprising three or more cameras. To enhance credit assignment in this multi-camera collaboration, we introduce the CTCR method, which shows significant improvements in reconstruction accuracy compared to both passive and active baselines. Furthermore, we introduce five auxiliary tasks designed to aid the model in learning environmental dynamics, thus bolstering its capability to manage highly dynamic scenes. Additionally, we contribute high-fidelity environments created to simulate realistic human crowds exhibiting authentic behaviors, accompanied by visualization software for detailed frame-by-frame video analysis.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.01", "instruction": "Change the position of two points.", "revised_paragraph": "Revised paragraph: \"\u2022 We formulate active 3D human pose estimation in a human crowd problem as a Dec-POMDP and propose a novel multi-camera (n \u2265 3) collaboration framework. \u2022 We introduce five auxiliary tasks to help the model learn environment dynamics, further enhancing the model\u2019s ability to handle highly dynamic scenes. \u2022 We propose CTCR to improve credit assignment in multi-camera collaboration and demonstrate notable improvements in reconstruction accuracy compared to both passive and active baselines. \u2022 We contribute high-fidelity environments built for simulating realistic-looking human crowds with authentic behaviors, along with visualization software for frame-by-frame video analysis.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "zzdwUcxTjWY.rVxmgW1FRK.03", "instruction": "Shorten this paragraph.", "revised_paragraph": "Revised paragraph: \"Jung et al. (2021) proposed a standardized max-logit approach for detecting outliers in semantic segmentation, while Zhao et al. and Grcic et al. (2021) trained a generative model to synthesize outliers in pixel space, unsuitable for object detection with mixed known and unknown objects. Their regularization terms differ from VOS as they rely on entropy maximization.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "zzdwUcxTjWY.rVxmgW1FRK.03", "instruction": "Make the first sentence a lot shorter.", "revised_paragraph": "Revised paragraph: \"Jung et al. (2021) proposed a max-logit approach for outlier detection in semantic segmentation. Zhao et al. and Grcic et al. (2021) trained a generative model to synthesize outliers in pixel space, which is not applicable to object detection involving both known and unknown objects. Their regularization terms rely on entropy maximization, differing from VOS.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.11", "instruction": "Clarify your last sentence.", "revised_paragraph": "Revised paragraph: \"By default, we initiate optimization with a random initial set Y\u2080 sampled from a normal distribution, specifically Y\u2080 \u223c N(0, I/10). Similar to the approach taken in DSPN, we have the option to utilize a learned initial set Y\u2080 to enhance convergence towards a solution. However, in contrast to DSPN, implicit differentiation treats the optimizer in Equation 7 as a black box, meaning there is no gradient feedback for Y\u2080. To address this, we must incorporate a regularizer in Equation 6 that can provide a gradient for Y\u2080. For example, we can include the regularizer proposed by Rajeswaran et al. (2019): \u03bb ||Y - Y\u2080||\u00b2. This regularization term will be utilized when we compare the performance of iDSPN and DSPN in Subsection 4.2.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CVRUl83zah.I75TtW0V7.11", "instruction": "Make the last sentence more precise.", "revised_paragraph": "Revised paragraph: \"By default, we sample a random initial set Y 0 \u223c N (0, I / 10) to start the optimization with. Similar to DSPN, we can also use a learned initial set Y 0 to start closer to a solution. Unlike DSPN, however, implicit differentiation treats the optimizer of Equation 7 as a black box, so there is no gradient signal for Y 0. Consequently, we must incorporate a regularizer in Equation 6 to provide a gradient for Y 0, such as the regularizer proposed by Rajeswaran et al. (2019): \u03bb || Y \u2212 Y 0 ||^2. This regularization will be employed when comparing the performance of iDSPN to DSPN in Subsection 4.2.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "JPHVd17f9N.nH2bI_9hXk.00", "instruction": "Make the last sentence logical.", "revised_paragraph": "Revised paragraph: \"The easiest way to ensure m_n = m_(n+1), so that the acceptance probability remains nonzero, is to set the time step h = 0. This, however, renders the sampler ineffective. If we instead take the limit as h approaches 0, the Euler-Maruyama scheme becomes arbitrarily close to the true SDE trajectory. Crucially, however, per Theorem 1, the acceptance probability remains 0 for any h > 0. Therefore, it is impossible to use the acceptance probability as an effective measure to monitor the discretization error. Furthermore, the Euler-Maruyama scheme fails to satisfy detailed balance. Importantly, it should be noted that we have not yet considered stochastic gradients; this result holds true for any choice of \u2207_\u03b8 U(\u03b8_n).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "JPHVd17f9N.nH2bI_9hXk.00", "instruction": "Make the last sentence more formal and academic.", "revised_paragraph": "Revised paragraph: \"The most straightforward method to ensure that \\( m_n = m_{n+1} \\), thereby maintaining a nonzero acceptance probability, is to establish the time step \\( h = 0 \\). However, this approach renders the sampler ineffective. Conversely, by taking the limit as \\( h \\to 0 \\), the Euler-Maruyama scheme approximates the true stochastic differential equation (SDE) trajectory with increasing precision. Importantly, as indicated by Theorem 1, the acceptance probability remains zero for any \\( h > 0 \\). Consequently, it is infeasible to utilize the acceptance probability as a means to assess the discretization error. The Euler-Maruyama scheme, therefore, cannot adhere to the principles of detailed balance. It is also noteworthy that, to this point, stochastic gradients have not been examined; this result holds for any selection of \\( \\nabla_\\theta U(\\theta_n) \\).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.08", "instruction": "Improve the English of the first sentence of this paragraph.", "revised_paragraph": "Revised paragraph: \"Prescriptions are subject to various constraints related to drug dosage units, frequency, and indications. Constraints can occur within the same medication and between different medications. An example of within-medication constraints is found in the prescription, 'Take 600 mg of Ibuprofen three times a day as needed with food,' which presents three constraints: 1) that 600 mg must be taken at specified intervals; 2) that the maximum number of doses per day is three; and 3) that the drug must be taken with food. However, medications are often more complex. Consider, for example, the following prescription:\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.08", "instruction": "Rephrase the two first sentences for better readability.", "revised_paragraph": "Revised paragraph: \"Prescriptions are subject to limitations regarding drug dosage units, frequency of administration, and specific indications. These constraints can arise both within a single medication and across different medications. An example of within-medication constraints is found in the prescription to take 600 mg of Ibuprofen three times a day as needed with food, which includes three constraints: 1) the dosage of 600 mg must be taken at specified times; 2) the maximum number of doses per day is three; and 3) the medication must be consumed with food. However, medications are often more complex. For instance, consider the following prescription:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "zDDQ6YzcK8.rFbDzCD2Zq.00", "instruction": "Revise this paragraph to be more concise.", "revised_paragraph": "Revised paragraph: \"The attention mechanism in transformers is crucial for modeling relationships between feature representations. In this section, we visualize the self-attention mechanisms of our Entroformer, focusing on specific points in the image. For clarity, the attention map is rescaled. Figure 8 presents self-attention heads from different points, revealing distinct behaviors related to structure, semantics, and color. These visualizations illustrate how the Entroformer identifies relevant contexts to enhance its distribution prediction for the current latent, enabling richer dependency capture and improved compression performance. Figure 9 shows the self-attention heads visualized separately from one point.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "zDDQ6YzcK8.rFbDzCD2Zq.00", "instruction": "This paragraph need to be shorter, do it by removing details but don\u2019t touch the first sentence.", "revised_paragraph": "Revised paragraph: Attention mechanism in transformer is the key component which models relations between feature representations. We visualize the self-attentions mechanism of our Entroformer, focusing on key points in the image. The attention map is rescaled for clarity. In Figure 8, self-attention heads from different points display varying behaviors associated with structure, semantics, and color. These visualizations illustrate how the Entroformer captures relevant context to enhance its distribution prediction. In Figure 9, we visualize self-attention heads from one point separately.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.09", "instruction": "Review the following paragraph", "revised_paragraph": "Revised paragraph: \"Methods for predicting mutational effects in single proteins can be broadly categorized into structure-based and sequence-based (evolution-based) approaches. Structure-based methods are further subdivided into biophysical methods, statistical methods, and deep learning-based techniques. These approaches primarily focus on predicting the thermal stability or fitness of the protein, rather than the binding free energy between proteins (Schymkowitz et al., 2005; Park et al., 2016; Alford et al., 2017). In contrast, the mutational effects of single proteins can also be inferred solely from their sequences by examining their evolutionary history. This traditional approach typically involves statistical analysis of multiple sequence alignments (MSAs), which are generated by querying large-scale sequence databases (Hopf et al., 2017; Riesselman et al., 2018; Rao et al., 2021; Luo et al., 2021; Frazer et al., 2021; Notin et al., 2022). Recent studies indicate that protein language models (PLMs) trained on extensive sequence databases can effectively assess mutations without the need for MSAs (Meier et al., 2021).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.09", "instruction": "Make this paragraph shorter and more fitted to academic style.", "revised_paragraph": "Revised paragraph: Methods for predicting the mutational effects of single proteins can be categorized as structure-based or sequence-based. Structure-based methods further subdivide into biophysical, statistical, and deep learning approaches, focusing primarily on predicting thermal stability or protein fitness rather than binding free energy (Schymkowitz et al., 2005; Park et al., 2016; Alford et al., 2017). Sequence-based predictions leverage evolutionary history, often utilizing statistics from multiple sequence alignments (MSAs) derived from large-scale sequence databases (Hopf et al., 2017; Riesselman et al., 2018; Rao et al., 2021; Luo et al., 2021; Frazer et al., 2021; Notin et al., 2022). Recent advancements indicate that protein language models (PLMs) trained on extensive sequence datasets can assess mutations independently of MSAs (Meier et al., 2021).", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.19", "instruction": "Rewrite this paragraph to be considerably more concise.", "revised_paragraph": "Revised paragraph: \"Conflict overlays were clearly identifiable across all designs. Users prefer indicators for the positions of conflicting medication entries rather than thick solid lines, which create clutter; thin or dotted lines are more effective. While lines can connect conflicting entries, using line styles to convey the nature of the conflict may be misleading, as similar styles can look alike from a distance. Therefore, line style should be used judiciously to indicate actions.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.19", "instruction": "I want to trim my paragraph so that the readers can read more easily.", "revised_paragraph": "Revised paragraph: \"Conflict overlays were clearly identifiable on all designs. Users preferred indicators to represent the positions of medication entries involved in conflicts. Connectors for conflicting pairs should avoid thick solid lines that create clutter; instead, thin or dotted lines are recommended. While lines effectively connect conflicting entries, using line style to indicate the nature of the conflict can be counterproductive, as different styles may appear similar from a distance, hindering clear communication. Therefore, line style should be used judiciously to imply action.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "l1D720s69O.vCKjjOP1ze.02", "instruction": "Remove the ideas which are not particularly essential for the overall paragraph.", "revised_paragraph": "Revised paragraph: \"We introduce the diffusion distance, which considers two nodes similar when they diffuse in a similar way through the graph and influence other nodes similarly. The diffusion distance at time K between nodes i and j is defined as follows:\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "l1D720s69O.vCKjjOP1ze.02", "instruction": "Delete the first part of the first sentence and adapt it in consequence.", "revised_paragraph": "Revised paragraph: \"Two nodes are considered similar when they diffuse in a similar way through the graph, and therefore when they influence the other nodes in a similar manner (Fouss et al., 2012). In other words, two nodes are close if they are in the same cluster which has a consistent local structure. More precisely, the diffusion distance at time K between nodes i and j is defined as follows:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "zzdwUcxTjWY.rVxmgW1FRK.01", "instruction": "Rewrite this paragraph to make it more concise and convincing.", "revised_paragraph": "Revised paragraph: \"Why is Out-of-Distribution (OOD) detection particularly challenging? Modern neural networks are typically trained solely on in-distribution (ID) data, lacking awareness of unknowns during this process. Consequently, the decision boundary, while effective for ID tasks like classification, inadvertently encompasses OOD data. This is illustrated in Figure 1, where the ID data (gray) consists of three class-conditional Gaussians used to train a three-way softmax classifier. The classifier demonstrates overconfidence in areas distant from the ID data, as seen in the red shading of Figure 1(b). When deployed directly, such models create decision boundaries that hinder OOD detection. An ideal model would establish a tighter decision boundary, exhibiting low uncertainty for ID data while demonstrating high uncertainty elsewhere (e.g., Figure 1(c)). However, achieving this is challenging due to the absence of supervision signals for unknowns, making it difficult to anticipate unknown data amid a vast space of OOD uncertainty. This raises the question: How can we develop unknown-aware deep neural networks without prior knowledge of these unknowns?\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "zzdwUcxTjWY.rVxmgW1FRK.01", "instruction": "Rewrite this paragraph to make it more precise, clear and concise while fitting the academic style.", "revised_paragraph": "Revised paragraph: OOD detection presents significant challenges primarily because modern neural networks are typically optimized solely on in-distribution (ID) data and do not acquire explicit knowledge of unknowns during training. As a result, the decision boundary, while effective for ID tasks like classification, may inadvertently encompass out-of-distribution (OOD) data. This issue is exemplified in Figure 1, where the ID data (gray) is represented by three class-conditional Gaussians, leading to a three-way softmax classifier. The classifier exhibits overconfidence in areas distant from the ID data, as indicated by the red shading in Figure 1(b). Consequently, when applied directly, such models can produce misleading decision boundaries for OOD detection. Ideally, a model should establish a more compact decision boundary, maintaining low uncertainty for ID data while exhibiting high uncertainty for OOD data, as shown in Figure 1(c). However, achieving this is complex due to the absence of supervisory signals regarding unknowns and the difficulty of anticipating unknown data across the vast OOD uncertainty space. This raises the critical question: How can we develop unknown-aware deep neural networks without prior explicit knowledge of these unknowns?", "type_approach": "instruction-annot_2"}
{"id_paragraph": "PcoXwm4jl.UxdreZBFz.00", "instruction": "Split the long sentences into more concise sentences.", "revised_paragraph": "Revised paragraph: For all experiments, we use an image size of 128 \u00d7 128. The batch size ranges from 12 to 16, depending on memory usage. We employ the RMSProp optimizer (Tieleman & Hinton, 2012) with a learning rate of 1 \u00d7 10 \u2212 5 for the foreground module. For the background module, we utilize the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1 \u00d7 10 \u2212 3, except for Figure 5. In Figure 5, we set the learning rate to 1 \u00d7 10 \u2212 4 for SPAIR to ensure a fair comparison. We also implement gradient clipping with a maximum norm of 1.0. In our Atari games experiments, we find it beneficial to set \u03b1 to a fixed value for the initial several thousand steps. We then vary the actual value and the number of steps for different games. This approach enables both the foreground and background modules to learn effectively during the early stage of training.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "PcoXwm4jl.UxdreZBFz.00", "instruction": "The second sentence is too long, split it and make it more readable.", "revised_paragraph": "Revised paragraph: \"For all experiments, we use an image size of 128 \u00d7 128 and a batch size of 12 to 16, depending on memory usage. The foreground module employs the RMSProp optimizer (Tieleman & Hinton, 2012) with a learning rate of 1 \u00d7 10 \u2212 5, while the background module utilizes the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1 \u00d7 10 \u2212 3. However, for Figure 5, we adjust the learning rate to 1 \u00d7 10 \u2212 4 in order to ensure a fair comparison with SPAIR. Additionally, we implement gradient clipping with a maximum norm of 1.0. In the case of Atari games, we find it beneficial to fix \u03b1 for the first several thousand steps, after which we vary both the actual value and the number of steps for different games. This strategy facilitates learning in both the foreground and background modules during the early stages of training.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.04", "instruction": "Remove the second setence and make the paragraph more concise", "revised_paragraph": "Revised paragraph: \"Figure 3 illustrates how to plug numbers into Eq. 6 to compute CTCR for each of the three cameras. The CTCR is incentivized by the Shapley Value, meaning that overall optimality must account for the optimality of every potential sub-formation. In an active HPE task, for a camera agent to achieve the highest CTCR, its position and view must be optimal relative to both its current formation and any possible sub-formation.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.04", "instruction": "Remove unnecessary information, use concicse expression.", "revised_paragraph": "Revised paragraph: \"Figure 3 illustrates the application of Eq. 6 to compute the CTCR for each of the three cameras, with a breakdown of the equation provided below. The CTCR, driven by the Shapley Value, emphasizes that overall optimality must consider the optimality of all potential sub-formations. In active HPE tasks, a camera agent achieves the highest CTCR when its position and view are optimal relative to both its current formation and any possible sub-formation.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "8_oadXCaRE.Kt4-LpYuM.03", "instruction": "Remove training details. Rewrite last paragraph to shorten.", "revised_paragraph": "Revised paragraph: \"The Euclidean norm and the derived plasticity rule allowed us to update weights and biases in an unsupervised manner using a network of K = 2000 neurons. We trained the network for 100 epochs on the 60,000 training digits, where validation testing indicated that softmax with a base of 1000 performed optimally. The learning rate decreased linearly from 0.03 to 0 throughout training, with each experiment repeated five times to ensure statistical reliability. Input labels were inferred through two approaches: a single-layer method that assigned labels based on which training label won the WTA competition most frequently, without updating weights using labels, and a two-layer method involving a supervised perceptron classifier trained on the WTA output. The SoftHebb model achieved accuracies of (96.18 \u00b1 0.06)% and (96.94 \u00b1 0.02)% in its single-layer and two-layer forms, respectively. Additionally, we explored a hard-WTA approach, where only the strongest neuron activated, yielding optimal performance at a learning rate of 0.05. Ultimately, the SoftHebb model outperformed the hard WTA configuration (Fig. 1A).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "8_oadXCaRE.Kt4-LpYuM.03", "instruction": "Remove unnecessary details to make the paragraph shorter.", "revised_paragraph": "Revised paragraph: \"The Euclidean norm and our derived plasticity rule updated the network's weights and biases in an unsupervised manner using K = 2000 neurons. We trained the network for 100 epochs with randomly ordered presentations of 60,000 training digits. Validation testing showed that softmax with a base of 1000 performed best. The learning rate decreased linearly from 0.03 to 0 throughout training. Each experiment was repeated five times with different random initializations and input orders, and we report mean and standard deviation of accuracies. Input label inference by the WTA network occurred through a single-layer approach assigning labels based on training set wins, without further weight updates, and a two-layer approach using a supervised perceptron classifier on the WTA layer. The classifier was trained with the Adam optimizer and cross-entropy loss for 60 epochs while freezing the WTA parameters. SoftHebb achieved accuracies of (96.18 \u00b1 0.06)% and (96.94 \u00b1 0.02)% for its 1- and 2-layer forms, respectively. The SoftHebb model outperformed a hard WTA setup, which used a learning rate of 0.05 (Fig. 1A).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.08", "instruction": "Please, rewrite this paragraph.", "revised_paragraph": "Revised paragraph: \"Despite their utility, these pre-training strategies do not adequately capture the fundamental aspects of protein-protein interactions. Recently, the promising performance of protein language models has led to increased interest in implementing the masked prediction (BERT) approach in the context of protein 3D structures (Wang et al., 2018; Shroff et al., 2020; Jing et al., 2020; Yang et al., 2022; Zhang et al., 2022; Hsu et al., 2022). These methodologies involve the partial masking of specific amino acid types along a protein backbone, with neural networks employed to reconstruct the obscured information. Notably, research indicates that the variance in the probabilities of amino acid types pre- and post-mutation correlates with alterations in binding free energy (Yang et al., 2022). Thus, these approaches can function as unsupervised predictors of mutational impacts on binding interactions.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.08", "instruction": "Rewrite this paragraph to make it shorter while keeping all the informations.", "revised_paragraph": "Revised paragraph: \"Pre-training strategies often fail to accurately capture the foundation of protein-protein interactions. Recently, protein language models have sparked interest in applying the mask-predict (BERT) paradigm to protein 3D structures (Wang et al., 2018; Shroff et al., 2020; Jing et al., 2020; Yang et al., 2022; Zhang et al., 2022; Hsu et al., 2022). These methods mask certain amino acid types on a protein backbone and use neural networks to recover the information. Research indicates that the probability differences of amino acid types before and after mutation correlate with changes in binding free energy (Yang et al., 2022), making them effective unsupervised predictors of mutational effects on binding.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "instruction": "Rewrite this paragraph in a more formal style and remove any unnecessary details.", "revised_paragraph": "Revised paragraph: The visualization of the pruning process is depicted in Figure 3, which illustrates the mean L1-norm of filters in two layers of the EDSR baseline during the SRP training. The filters are categorized into two groups: pruned and retained. It is observed that the mean L1-norm of the pruned filters decreases progressively due to the increasing penalty, which drives the values towards zero. Notably, the L1-norms of the retained filters exhibit an increase despite the absence of any regularization term encouraging their growth. This observation suggests that the network demonstrates a self-recovery mechanism, analogous to the compensation effect observed in human cognition (Duffau et al., 2003). Additional visualization results are provided in the appendix.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "instruction": "Rewrite this paragraph in a more formal style and remove the last sentence", "revised_paragraph": "Revised paragraph: The visualization of the pruning process is essential for understanding the operation of the Semantic Retention Pruning (SRP) methodology. In Figure 3, we present the mean L1-norm of filters from two layers of the Enhanced Deep Super-Resolution (EDSR) baseline during the training phase of SRP. The filters are categorized into two distinct groups: pruned and retained. It is evident that the mean L1-norm of the pruned filters decreases progressively, attributed to the intensifying penalty that compels these norms toward zero. Notably, the L1-norms of the retained filters increase autonomously, despite the absence of any regularization term to promote their growth. This observation suggests that the network is capable of self-recovery, resembling the compensatory mechanisms observed in the human brain (Duffau et al., 2003).", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.10", "instruction": "Rewrite this paragraph to be more concise.", "revised_paragraph": "Revised paragraph: \"We define O + as the set of image segments with overlapping object categories. For instance, if pixel i is labeled as a sofa and another image also contains a sofa, all segments from that image are included in O + ; otherwise, they are classified as negative segments O \u2212 . This semantic relationship improves global and higher-level regularization on the pixel embedding without needing localization annotations.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.10", "instruction": "Make it more concise.", "revised_paragraph": "Revised paragraph: \"We define O + as the set of segments from images with overlapping object categories. For instance, if pixel i is labeled as a sofa and another image also contains a sofa, all segments from that image are included in O +; otherwise, they are classified as negative segments O \u2212. This semantic context relationship enhances global and higher-level regularization on pixel embeddings without requiring localization annotations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aFzc_2nNz.WIdHkazOg.01", "instruction": "Make the language of this paragraph more formal.", "revised_paragraph": "Revised paragraph: \"A method to assess potential correspondences involves the systematic categorization of the validation samples into M equal-mass bins (hereinafter referred to as validation bins) and subsequently comparing the confidence levels of the training samples that are situated within the same validation bins. Prior to this analysis, it is essential to elucidate several notations of significance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aFzc_2nNz.WIdHkazOg.01", "instruction": "Improve the writinng in the linking of the two last sentences to make it more formal.", "revised_paragraph": "Revised paragraph: \"One way to check for any correspondences is to group the validation samples into M equal-mass bins (henceforth referred to as validation-bins) and subsequently compare the confidence levels with those of the training samples that are categorized within the same validation-bins. However, prior to conducting this analysis, it is important to clarify a few notations that are pertinent to our discussion.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nIRqrHmpIE.pL1E71anH.00", "instruction": "Use \"half\" instead of \"semi.\"", "revised_paragraph": "Revised paragraph: \"To visualize the advantage of our approach, consider the half-circle domain in Figure 1, adapted from [3]: a 2-dimensional agent must navigate to a goal located somewhere on the half-circle. A task therefore corresponds to the goal location, and the task distribution is uniform on the 1-dimensional half-circle.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nIRqrHmpIE.pL1E71anH.00", "instruction": "Replace all semi-circle by half-circle.", "revised_paragraph": "Revised paragraph: \"To visualize the advantage of our approach, consider the half-circle domain in Figure 1, adapted from [3]: a 2-dimensional agent must navigate to a goal, located somewhere on the half-circle. A task therefore corresponds to the goal location, and the task distribution is uniform on the 1-dimensional half-circle.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ryESgXktV.BJ4dKdWmr.01", "instruction": "Revise the opening of this paragraph to make it more compelling.", "revised_paragraph": "Revised paragraph: \"Our previous research (Chakraborti et al. 2017) sheds light on the critical issue of model inconsistencies, particularly the discrepancies that arise between human perceptions and the models employed by agents when generating explanations. In this context, an explanation serves not merely as a description but as a compelling invitation for the human to recalibrate their understanding, aligning it with the robot's behavior to create a coherent framework that meets their expectations. This intricate interplay between human cognition and robotic behavior is encapsulated in the concept of model reconciliation, as outlined in our prior work (Chakraborti et al. 2017; Zhang et al. 2017).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ryESgXktV.BJ4dKdWmr.01", "instruction": "Rewrite the first sentence to make it more convincing.", "revised_paragraph": "Revised paragraph: \"In our previous research (Chakraborti et al. 2017), we compellingly elucidate how inconsistencies manifest as model differences, particularly when examining the divergences between human perceptions and an agent's internal model during the explanation generation process. An explanation thus serves as a prompt for the human to realign their mental models, enabling the robot's behavior to align with revised expectations. This decision-making process in the face of model differences is referred to as model reconciliation (Chakraborti et al. 2017; Zhang et al. 2017).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.02", "instruction": "Explain the concept more clearly.", "revised_paragraph": "Revised paragraph: \"The concept of fusion modules refers to mechanisms that facilitate the transfer of information between different branches within a network. In this particular architecture, intermediate fusion is utilized, which is classified as one of the multi-modal fusion strategies in the deep learning literature, as discussed by Ngiam et al. (2011), Atrey et al. (2010), and Baltru\u0161aitis et al. (2018). This approach has shown to be competitively effective when compared to multi-modal deep neural networks (DNNs) that use late fusion techniques, demonstrating superior performance across various tasks, as evidenced by the findings of Perez et al. (2018), Joze et al. (2020), Anderson et al. (2018), and Wang et al. (2020b).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.02", "instruction": "Rewrite this paragraph to improve its clarity.", "revised_paragraph": "Revised paragraph: \"These fusion modules facilitate the transfer of information between different branches of the network. This architecture utilizes intermediate fusion, as classified within the deep learning literature on multi-modal fusion strategies (Ngiam et al., 2011; Atrey et al., 2010; Baltru\u0161aitis et al., 2018). It has shown competitive performance compared to multi-modal deep neural networks that employ late fusion across various tasks (Perez et al., 2018; Joze et al., 2020; Anderson et al., 2018; Wang et al., 2020b).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "3686sm4Cs.AJMXMDLVn.03", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"In this study, we introduce SuperWeight Ensembles, a novel method for parameter sharing in heterogeneous ensembles. SuperWeight Ensembles surpass existing approaches in the anytime prediction task by effectively utilizing gradient information for parameter sharing. Our findings indicate that this automatic sharing enhances the performance of individual members by 2% compared to baseline models. Additionally, SuperWeight Ensembles achieve performance levels comparable to those of efficient ensembles in the low-parameter regime, while providing the added flexibility to adjust parameters\u2014a feature lacking in prior work. When we increase the number of parameters, we outperform standard ensembles on ImageNet while using only 50% of the parameters. We propose that SuperWeight Ensembles signify a significant advancement in parameter efficiency. Future research will focus on further exploring architecture diversity, as Gontijo-Lopes et al. (2021) have demonstrated that model architecture heterogeneity can play a crucial role in enhancing ensemble diversity on challenging tasks.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "3686sm4Cs.AJMXMDLVn.03", "instruction": "Make this paragraph a bit more concise.", "revised_paragraph": "Revised paragraph: \"In this work, we introduce SuperWeight Ensembles, a method for parameter sharing in heterogeneous ensembles. Our method outperforms existing approaches on the anytime prediction task by utilizing gradient information for more effective parameter sharing, resulting in a 2% improvement in single member performance compared to baselines. SuperWeight Ensembles also match the performance of efficient ensembles in the low-parameter regime, while offering additional flexibility in parameter adjustment absent in prior work. When increasing parameters, we exceed the performance of standard ensembles on ImageNet using only 50% of the parameters. We believe SuperWeight Ensembles represent a significant advancement in parameter efficiency. Future research will further explore architectural diversity, as Gontijo-Lopes et al. (2021) suggested that model architecture heterogeneity can greatly enhance ensemble diversity on challenging tasks.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.12", "instruction": "Add a citation in the last sentence and modify the rest so that the total length of the paragraph remains the same. ", "revised_paragraph": "Revised paragraph: \"Datasets. The absence of counterfactuals complicates the evaluation of the true PEHE in observational benchmarks. In line with the methods proposed by Liuyi et al.; Uri et al. (2017), our experiments utilize two semi-synthetic benchmarks. The IHDP benchmark focuses on estimating the effects of specialist home visits on infants' future cognitive scores, containing 747 observations and 25 covariates; the ACIC dataset, derived from the collaborative perinatal project (Niswander & Gordon, 1972), comprises 4802 observations and 58 covariates (Smith, 2020).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.12", "instruction": "Improve the language in this text.", "revised_paragraph": "Revised paragraph: \"Datasets. The absence of counterfactual data renders the evaluation of the ground-truth Population Expectation under Heterogeneous Treatment Effects (PEHE) over observational benchmarks infeasible. In accordance with the methodologies proposed by Liuyi et al. and Uri et al. (2017), experiments are conducted on two semi-synthetic benchmarks. Specifically, the Infant Health and Development Project (IHDP) benchmark seeks to assess the impact of specialist home visits on infants' future cognitive scores, comprising 747 observations and 25 covariates. Conversely, the ACIC dataset, derived from the Collaborative Perinatal Project (Niswander & Gordon, 1972), encompasses 4,802 observations and 58 covariates.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.15", "instruction": "Please, remove unnecessary details of this paragraph", "revised_paragraph": "Revised paragraph: \"Rotamers with d torsional angles can be viewed as points on the D-dimensional torus, which is the product of D circles (T D = S 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 S 1). We model the distribution on T D by the joint distribution of D variables on S 1, using the coupling layer technique (Dinh et al., 2016). Each coupling layer updates one dimension using the bijective for S 1, with the other D \u2212 1 dimensions fixed and incorporated with the amino acid's hidden representation to parameterize the bijective.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.15", "instruction": "Make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"Rotamers with d torsional angles can be represented as points on the D-dimensional torus, T D = S 1 \u00d7\u00b7 \u00b7 \u00b7 \u00d7 S 1. We model the distribution on T D by examining the joint distribution of D variables on S 1, utilizing the coupling layer technique (Dinh et al., 2016). In each layer, we update one dimension with a bijective function for S 1, while keeping the other D \u2212 1 dimensions fixed and conditioning on the hidden representation of the amino acid (Figure 2B):\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ZcvguGK9Q.dBwWtd12-n.00", "instruction": "Review the following paragraph, only when necesarry make modificatons to make it easier to read", "revised_paragraph": "Revised paragraph: \"Human Evaluation of KG Explanations: We conduct a user study to assess the ability of KG-augmented models to provide plausible explanations, utilizing both the original KG and a perturbed KG (RLRR). For each KG, we sample 30 questions from the CSQA and OBQA test sets that were correctly answered by MHGRN. For every question, we retrieve the top-scoring path for each answer choice using MHGRN\u2019s path decoder attention. We then ask three human subjects to rate each path based on readability and usability, with the ratings aggregated through majority voting. Readability (Read) indicates whether the path makes sense, while usability (Use) assesses the relevance of the path to the specific question-answer pair; both are measured on a scale from 0 to 1. We achieve a Fleiss\u2019 \u03ba of 0.1891, indicating slight agreement among raters. To illustrate our findings, we provide examples of explanation paths along with their consensus ratings.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ZcvguGK9Q.dBwWtd12-n.00", "instruction": "Better balance the length of the first and second sentences.", "revised_paragraph": "Revised paragraph: \"Human Evaluation of KG Explanations We conduct a user study to measure the ability of KG-augmented models to provide plausible explanations, utilizing the original KG or the RLRR perturbed KG. For both KGs, we sample 30 questions from the CSQA and OBQA test sets that were correctly answered by MHGRN. We retrieve the top-scoring path for each answer choice using MHGRN\u2019s path decoder attention. Subsequently, three human subjects rate each path for readability and usability, with ratings aggregated through majority voting. Readability (Read) assesses whether the path makes sense, while usability (Use) evaluates the relevance of the path to the given question-answer pair; both are measured on a [0, 1] scale. We obtain a Fleiss\u2019 \u03ba of 0.1891, indicating slight agreement among raters. To illustrate our findings, we provide examples of explanation paths along with their consensus ratings.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.11", "instruction": "Rephrase this paragraph to make its goal and explanations much more clear.", "revised_paragraph": "Revised paragraph: \"Feature affinity. This study examines the process of transferring labels from a labeled dataset to an unlabeled dataset using nearest neighbor search in the feature space. We hypothesize that semantic clusters form during training under sparse supervision, which is further supported by the previously discussed relationships between pixels and segments. By propagating labels within the feature space, we aim to enhance the learning of these semantic clusters.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.11", "instruction": "Rewrite this paragraph to bring the argument through the idea that the goal is to learn a pixel-wise feature for semantic segmentation.", "revised_paragraph": "Revised paragraph: \"In our study, we focus on learning pixel-wise features for semantic segmentation by leveraging feature affinity. This process involves propagating labels from an annotated dataset to an unlabeled dataset through nearest neighbor search within the feature space. We posit that during training, semantic clusters emerge as a result of sparse supervision, which is further enhanced by the established pixel-to-segment relationships. By effectively propagating labels in the feature space, we aim to strengthen the learning of these pixel-wise features, ultimately improving the accuracy of semantic segmentation.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ByZyHzZC-.HktKf7-AW.04", "instruction": "I want to make my paragraph shorter and clearer.", "revised_paragraph": "Revised paragraph: \"We derive the relationship between learning rate, batch size, gradient covariance, and minima properties (loss and width), and experimentally demonstrate that controllable noise \\( n_c = \\eta/S \\) influences the width and height of minima in Stochastic Gradient Descent (SGD). We also explore the effect of this noise on memorization. Additionally, we address the limitations of our theory, noting its breakdown when learning rates are excessively large. Our findings confirm that \\( \\eta \\) and \\( S \\) are interchangeable as long as \\( n_c \\) remains constant, applicable to both cyclic and constant schedules. In the cyclic case, we provide evidence that learning rates oscillate between sharp/deep and wide/shallow minima, given a sufficiently long stage of increased noise.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ByZyHzZC-.HktKf7-AW.04", "instruction": "Delete the context in the first sentence. Delete the second sentence from the end. Smooth out the writing.", "revised_paragraph": "Revised paragraph: \"We experimentally verify that the controllable noise n c = \u03b7/S regulates the width and height of the minima toward which SGD converges. The impact of the controllable noise on the memorization phenomenon is also examined. The limitations of the theory are discussed, particularly in scenarios where the learning rate becomes excessively large. Additionally, we demonstrate that \u03b7 and S are interchangeable, provided that the controllable noise \u03b7/S remains consistent. This holds true for both cyclic and constant schedules. In the cyclic case, we provide experimental evidence suggesting that cyclical learning rates oscillate between sharp/deep and wide/shallow minima, as long as the duration of increased noise is sufficient for effective mixing.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Skrry_KpQ.rJKpSniHN.00", "instruction": "Remove unnecessary details.", "revised_paragraph": "Revised paragraph: \"This paper presents generalization error bounds of deep ReLU networks for a Besov space and its variant with mixed smoothness, which encompasses special cases such as the H\u00f6lder space and Sobolev space. We demonstrate that deep learning achieves the optimal minimax rate on the Besov space, significantly outperforming linear estimators like kernel ridge regression. Furthermore, we establish that deep learning can circumvent the curse of dimensionality on the mixed smooth Besov space while achieving the same optimal rate. Previous work on approximation error analysis for Besov spaces has largely excluded ReLU activation and lacked comprehensive analyses, leaving unclear whether ReLU neural networks can surpass methods like kernel methods. The contributions of this paper are summarized as follows:\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Skrry_KpQ.rJKpSniHN.00", "instruction": "The end of the paragraph is too long, remove the part that fit the less in it.", "revised_paragraph": "Revised paragraph: \"In this paper, we provide generalization error bounds for deep ReLU networks within a Besov space and its variant with mixed smoothness, which includes the H\u00f6lder space, the Sobolev space, and the function class with total variation as special cases. By doing so, we demonstrate that (i) deep learning achieves the minimax optimal rate on the Besov space and notably outperforms linear estimators such as kernel ridge regression, and (ii) deep learning can avoid the curse of dimensionality on the mixed smooth Besov space while attaining the minimax optimal rate. As related work, Mhaskar & Micchelli (1992); Mhaskar (1993); Chui et al. (1994); Mhaskar (1996); and Pinkus (1999) developed an approximation error analysis that pertains to Besov spaces, but the ReLU activation is largely excluded and comprehensive analyses have not been provided. Consequently, it remains unclear whether ReLU neural networks can outperform other methods such as kernel methods. The contributions of this paper are summarized as follows:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "F3z0hchpGy.xeuzrNJiNW.02", "instruction": "Add brackets to the citations", "revised_paragraph": "Revised paragraph: \"The irregular structure of meshes leads to a variety of approaches to define convolutions. Closely related to our method are graph based methods, which are often based on variations of graph convolutional networks (Kipf & Welling, 2017; Defferrard et al.). GCNs have been applied on spherical meshes (Perraudin et al.) and cortical surfaces (Cucurull et al.; Zhao et al.). Verma et al. (2018) augment GCNs with anisotropic kernels, which are dynamically computed via an attention mechanism over graph neighbours.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "F3z0hchpGy.xeuzrNJiNW.02", "instruction": "Put the references between parenthesis.", "revised_paragraph": "Revised paragraph: \"The irregular structure of meshes leads to a variety of approaches to define convolutions. Closely related to our method are graph-based methods which are often based on variations of graph convolutional networks (Kipf & Welling, 2017; Defferrard et al.). GCNs have been applied on spherical meshes (Perraudin et al.) and cortical surfaces (Cucurull et al.; Zhao et al.). Verma et al. (2018) augment GCNs with anisotropic kernels which are dynamically computed via an attention mechanism over graph neighbours.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "CzTbgFKuy.hfDu8DsDq6.01", "instruction": "Remove unecessary details, use abbrieviation. ", "revised_paragraph": "Revised paragraph: We study a setting where all instances have instance-specific features, a practical assumption [27, 30] that includes numerical representations of the instance (e.g., bits for queries or graphs) and other information such as weather or day of the week. These features are input to functions (e.g., linear predictors, neural nets, trees) with parameters learned from data. We focus on linear predictors, benefiting from similar analyses as described earlier since the composition of a convex and affine function is convex. For instance, we can extend matching results to bound the regret and sample complexity in learning a linear predictor of duals. Page migration presents additional challenges due to outputs needing to lie in the simplex; we address this by using matrices with columns in the simplex (i.e., rectangular stochastic matrices). Both results are detailed in Appendix C. Importantly, our guarantees for page migration include the auto-regressive setting, where server probabilities are determined by a fixed linear transform of past states.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "CzTbgFKuy.hfDu8DsDq6.01", "instruction": "Make this paragraph a bit more concise.", "revised_paragraph": "Revised paragraph: \"We study a setting where all instances have instance-specific features, a practical assumption that includes numerical representations such as bits for a query or graph, as well as external information like weather or day of the week. These features are utilized by functions, such as linear predictors, neural nets, or trees, with parameters learned from data. Our focus is on linear predictors, which allow for similar analyses due to the convex nature of their composition. For instance, we can extend the matching results to establish bounds on the regret and sample complexity of learning a linear predictor for duals. Page migration presents additional challenges since outputs must reside in the simplex; we address this by restricting our analysis to rectangular stochastic matrices. These results are detailed in Appendix C. Notably, for page migration, our guarantees also apply to the auto-regressive setting, where server probabilities are determined by a fixed linear transformation of past states.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.20", "instruction": "Make the second sentence more concise", "revised_paragraph": "Revised paragraph: \"Figure 6 shows a schematic of the task. In experiment 1, the starting position of the trial was fixed, whereas in experiment 2, it was not a specified condition. The starting area was a rectangle, and trials commenced with a single click within this area. Aside from this modification, the task remained identical to that in experiment 1.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.20", "instruction": "Make sentence 2 shorter.", "revised_paragraph": "Revised paragraph: \"Figure 6 shows a schematic of the task. In experiment 1, we strictly set the starting position of the trial. In experiment 2, we did not impose a starting position. The starting area was a rectangle, and the trials started by simply clicking an area once. Except for this change, the task was the same as in experiment 1.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "fJhx73ErBg.NeKLbmOxG8.01", "instruction": "Please, paraphrase this paragraph.", "revised_paragraph": "Revised paragraph: \"Problem 1 outlines a feature matching issue akin to various formulations in inverse reinforcement learning (IRL) [10] and imitation learning (IOC) [11]. We establish a collection of features f, alongside a risk feature f \u03c6 \u03be, that reflect the driving and risk management inclinations of the individual demonstrating the behavior. Utilizing the demonstration data, we then identify the parameters \u03be for the risk metric and the weights necessary to integrate the feature values, resulting in a cost model. Optimizing a trajectory based on this cost model yields driving behaviors that resemble those of the demonstrator, given our specified features and risk models. Current IOC methodologies can replicate typical driving patterns, with the probability P \u03b8 typically modeled as an exponential distribution. Consequently, the trajectories produced from this cost model are significantly more favored by the agent. In our approach, incorporating the risk measure enhances our ability to align with drivers in high-risk situations. Although none of these methods perfectly replicate the demonstrator, they successfully encapsulate the demonstrator's driving style and facilitate adaptation to new high-risk contexts.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "fJhx73ErBg.NeKLbmOxG8.01", "instruction": "This paragraph is confusing, rewrite the second and the two last sentence for clarity. Smooth out the liking between sentences.", "revised_paragraph": "Revised paragraph: \"Problem 1 defines a feature matching problem similar to many inverse reinforcement learning (IRL) [10] and inverse optimal control (IOC) [11] formulations. We design a set of features, denoted as f, along with a risk feature, f \u03c6 \u03be, that collectively represent the driving and risk management preferences of the demonstrator. Using the demonstration data, we then learn the parameters of the risk metric, \u03be, as well as the weights that combine the feature values, which together create a cost model. By solving for a trajectory that minimizes this cost model, we obtain driving behaviors that closely resemble those of the demonstrator, given our defined features and risk models. Traditional IOC approaches typically imitate standard driving behaviors by assuming that the probability P \u03b8 follows an exponential distribution. Consequently, the trajectories generated from the cost model are significantly more aligned with the agent's preferences. In our approach, incorporating the risk measure enhances our ability to better match drivers, especially in risky situations. Although none of these methods can perfectly replicate the demonstrator's actions, they effectively capture the demonstrator's driving style and facilitate adaptation to new, high-risk scenarios.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.12", "instruction": "Rewrite this paragraph to make it considerably clearer.", "revised_paragraph": "Revised paragraph: \"Specifically, we assign a semantic label to each unlabeled segment by identifying the closest labeled segment within the feature space. We represent this updated labeled set, which includes the original labeled segments, as \u02c6 C. Within this set, \u02c6 C + refers to all segments of the same category as segment s, excluding segment s itself, where pixel i belongs to segment s. This approach is grounded in three assumptions: first, that the original labeled set is sufficiently large to encompass the feature space; second, that the labeled segments are evenly distributed within that space; and third, that the embedding contains relevant semantic information. Consequently, we implement this method to propagate keypoint annotations in the DensePose dataset, where each body part is represented by a specific point.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.12", "instruction": "Revise this paragraph to improve readability and cohesiveness.", "revised_paragraph": "Revised paragraph: \"In this study, we assign a semantic label to each unlabeled segment by identifying its nearest labeled counterpart in the feature space. We define this augmented labeled set, in conjunction with the original labeled set, as \\( \\hat{C} \\), where \\( \\hat{C}^+ \\) represents all segments of the same category, excluding segment \\( s \\) to which pixel \\( i \\) belongs. This formulation is grounded in three key assumptions: 1) the original labeled set is sufficiently large to encompass the feature space, 2) the labeled segments are uniformly distributed within that space, and 3) the embedding effectively encodes relevant semantic information. Consequently, we apply this relationship specifically to propagate keypoint annotations in the DensePose dataset, where each body part is marked by a distinct point.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "wNQ4_8Ym_c.1vd_qn2D93.00", "instruction": "Make as concise as possible the paragraph, removing any ideas that are not essential. Use a clearer word choice.", "revised_paragraph": "Revised paragraph: A key issue in post hoc explanations is the absence of a unified foundational goal among methods. Some methods focus on function approximation, others utilize game theoretic concepts like Shapley-Aumann values, while some aim for clean visualizations. This fragmentation hinders a coherent understanding of post hoc explainability and complicates the practical decision of which method to use.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "wNQ4_8Ym_c.1vd_qn2D93.00", "instruction": "Concise this paragraph while improving the academic english.", "revised_paragraph": "Revised paragraph: \"A significant issue in the literature on post hoc explanations is the absence of a unified foundational goal among various methods. For instance, while some methods are based on function approximation, others employ game-theoretic concepts like Shapley-Aumann values, and some are devised ad hoc to achieve clear visualizations. This fragmentation hinders a coherent conceptual understanding of post hoc explainability and complicates the practical challenge of determining the appropriate method to use in specific situations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MZYBK_Wp2X.HVFitLjAId.00", "instruction": "Improve the writing of this text", "revised_paragraph": "Revised paragraph: \"Numerous benchmarking studies have assessed node classification and clustering on both generated graphs and datasets (Fouss et al., 2012; Sommer et al., 2016; 2017; Avrachenkov et al., 2017; Ivashkin & Chebotarev, 2016; Guex et al., 2018; 2019; Aynulin, 2019a; b; Courtain et al., 2020; Leleux et al., 2020). Despite the abundance of experimental findings, significant theoretical results remain elusive. Among the most critical theoretical contributions to graph measures is the work by Luxburg et al. (2010), which addresses the challenges associated with Commute Time on large graphs and proposes a substantiated amendment to resolve these issues. This study illustrates the complexities involved in proving such results. Furthermore, there exists a lack of comprehensive empirical understanding regarding the effects that require validation. Our empirical research offers two key advantages over previous studies. First, it encompasses a wide array of graph measures, providing an unprecedented holistic view. Second, contrary to studies that culminate in a global leaderboard, our investigation focuses on identifying the leading measures for each set of LFR parameters.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MZYBK_Wp2X.HVFitLjAId.00", "instruction": "Use accurate and scientific words.", "revised_paragraph": "Revised paragraph: \"Numerous benchmarking studies have explored node classification and clustering across both generated graphs and real-world datasets (Fouss et al., 2012; Sommer et al., 2016; 2017; Avrachenkov et al., 2017; Ivashkin & Chebotarev, 2016; Guex et al., 2018; 2019; Aynulin, 2019a; b; Courtain et al., 2020; Leleux et al., 2020). Despite the abundance of experimental findings, significant theoretical advancements remain elusive. A pivotal theoretical contribution in the realm of graph measures is the work by Luxburg et al. (2010), which analytically addressed the Commute Time problem on large graphs and proposed a substantiated amendment to mitigate the identified issue. This paper elucidates the challenges associated with such proofs. In addition to the complexity of these proofs, there remains a lack of comprehensive empirical understanding regarding the effects that necessitate validation. Our empirical investigation presents two primary advantages over existing studies. First, we evaluate a broad spectrum of graph measures, offering a comprehensive overview for the first time. Second, in contrast to studies that conclude with a global leaderboard, our aim is to identify the most effective measures tailored to each specific set of LFR parameters.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ssjKKm0b5y.3wi5X8wrM_.03", "instruction": "Make the sentence correct, put conjuctions in front of sentences.", "revised_paragraph": "Revised paragraph: \"Hyperparameter tuning: For our PHN method, we select hyperparameters based on the HV computed on a validation set. However, selecting hyperparameters for the baselines is non-trivial, as there is no clear criterion that is reasonable in terms of runtime. Moreover, in order to select hyperparameters based on HV, each approach needs to be trained multiple times on all rays. Therefore, we select hyperparameters based on a single ray and apply those for all rays. Our selection criterion is as follows: we collect all models trained using all hyperparameter configurations and filter out the dominated solutions. Finally, we select the combination of hyperparameters with the highest uniformity.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ssjKKm0b5y.3wi5X8wrM_.03", "instruction": "Improve the English and the flow of this paragraph.", "revised_paragraph": "Revised paragraph: \"Hyperparameter tuning: In our PHN method, we select hyperparameters based on the hypervolume (HV) calculated from a validation set. The process of selecting hyperparameters for the baseline models is complex, as there are no clear criteria that balance efficiency and runtime. To evaluate hyperparameters using HV, each approach must be trained multiple times across all rays. Consequently, we opt to choose hyperparameters based on a single ray and apply these selections universally across all rays. Our selection criterion is as follows: we gather all models trained with various hyperparameter configurations and filter out dominated solutions. Ultimately, we choose the combination of hyperparameters that exhibits the highest uniformity.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.07", "instruction": "Rewrite some sentences, making them more connected and using more formal language.", "revised_paragraph": "Revised paragraph: \"In this study, we enhance the grid world environment established by Chevalier-Boisvert et al. (2018), wherein an agent navigates a two-dimensional maze characterized by two lava rivers in pursuit of a designated goal. The agent is afforded the ability to execute movements in four cardinal directions, as well as a turning maneuver to the left (refer to Figure 5). Notably, two out of four specialized skills\u2014namely, turn-right, step-forward, dig-orange-lava, and dig-pink-lava\u2014are randomly assigned to the agent for each task instance. Furthermore, the agent is capable of traversing lava for a brief interval of one timestep without incurring fatal consequences; however, it can eliminate the lava using the corresponding dig-lava skill. Consequently, when these dig-lava skills are accessible, they can be strategically employed to create shortcut pathways leading to the goal, thereby enhancing the potential reward. For the training of all methodologies employed in this study, we utilize Proximal Policy Optimization (PPO).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.07", "instruction": "Use a slightly more academic logical language.", "revised_paragraph": "Revised paragraph: \"We have modified the grid world environment (Chevalier-Boisvert et al., 2018), wherein an agent navigates a two-dimensional maze characterized by two lava rivers in order to reach a designated goal. The agent is endowed with the capability to execute four directional movements alongside a turn-left skill (see Figure 5). Notably, two of the four specialized skills\u2014turn-right, step-forward, dig-orange-lava, and dig-pink-lava\u2014are randomly sampled for the agent in each task instance. The agent can traverse over lava for a single timestep without incurring harm; however, it may eliminate the lava through the application of the corresponding dig-lava skill. Consequently, when accessible, these dig-lava skills can be employed to create shortcut pathways leading to the goal, thereby facilitating the acquisition of higher rewards. We utilize Proximal Policy Optimization (PPO) to train all methodologies.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "vokZIVWUXN.zMdXRtaisu.00", "instruction": "Remove the second-to-last sentence.", "revised_paragraph": "Revised paragraph: \"To mitigate the burden of data labeling, we aim at improving data efficiency for both classification and regression setups in deep learning. However, the current focus is on classification problems while rare attention has been paid to deep regression, which usually requires more human effort to labeling. Further, due to the intrinsic difference between categorical and continuous label space, the common intuitions for classification, e.g., cluster assumptions or pseudo labeling strategies, cannot be naturally adapted into deep regression. To this end, we first delved into the existing data-efficient methods in deep learning and found that they either encourage invariance to data stochasticity (e.g., consistency regularization under different augmentations) or model stochasticity (e.g., difference penalty for predictions of models with different dropout). To take the power of both worlds, we propose a novel \u03c7-model by simultaneously encouraging the invariance to data stochasticity and model stochasticity. Extensive experiments verify the superiority of the \u03c7-model among various tasks, from a single-value prediction task of age estimation to a dense-value prediction task of keypoint localization, a 2D synthetic and a 3D realistic dataset, as well as a multi-category object recognition task.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "vokZIVWUXN.zMdXRtaisu.00", "instruction": "Delete the sentence about the minmax game.", "revised_paragraph": "Revised paragraph: To mitigate the burden of data labeling, we aim at improving data efficiency for both classification and regression setups in deep learning. However, the current focus is on classification problems while rare attention has been paid to deep regression, which usually requires more human effort for labeling. Further, due to the intrinsic difference between categorical and continuous label spaces, the common intuitions for classification, such as cluster assumptions or pseudo labeling strategies, cannot be naturally adapted into deep regression. To this end, we first delved into the existing data-efficient methods in deep learning and found that they either encourage invariance to data stochasticity (e.g., consistency regularization under different augmentations) or model stochasticity (e.g., difference penalty for predictions of models with different dropout). To harness the benefits of both approaches, we propose a novel \u03c7-model by simultaneously encouraging invariance to data stochasticity and model stochasticity. Extensive experiments verify the superiority of the \u03c7-model across various tasks, from a single-value prediction task of age estimation to a dense-value prediction task of keypoint localization, involving both a 2D synthetic dataset and a 3D realistic dataset, as well as a multi-category object recognition task.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1BhqsOsB.1mgtDFRDc.02", "instruction": "Rephrase the paragraph", "revised_paragraph": "Revised paragraph: \"These models employ an inference network that processes entire sequences of video frames to predict the positions of 2D object bounding boxes and the displacements between frames, aiming to reduce view prediction error in two dimensions. However, we were unable to obtain significant results from their inference networks. The achievements of Hsieh et al. (2018) could be partly attributed to the meticulous selection of priors for the locations and sizes of 2D object bounding boxes, which align with the statistical properties of the moving MNIST dataset utilized in their study, as indicated by the publicly accessible code. We do not assume the existence or knowledge of such priors for the object locations or sizes in our CARLA dataset.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1BhqsOsB.1mgtDFRDc.02", "instruction": "Make this paragraph more clear and precise.", "revised_paragraph": "Revised paragraph: \"These models employ an inference network that processes complete sequences of video frames to predict the locations of 2D object bounding boxes and frame-to-frame displacements, aiming to minimize view prediction error in 2D. However, we were unable to generate meaningful results from their inference networks. The success demonstrated by Hsieh et al. (2018) may be partly attributable to their careful selection of priors for 2D object bounding box locations and object size parameters, which align with the statistics of the moving MNIST dataset used in their study, as indicated by the publicly available code. In contrast, we do not assume the knowledge or existence of such object location or size priors in our CARLA dataset.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.13", "instruction": "Reduce the explanations in this paragraph and just give a high level explanation, to keep it concise.", "revised_paragraph": "Revised paragraph: \"SegSort (Hwang et al., 2019) is an image segmentation model that produces pixel-wise embeddings and relies on spherical K-Means clustering for segmenting images based on the assumption of independent normal distributions within segments. It utilizes a maximum likelihood loss to enhance segment discrimination and incorporates soft neighborhood assignment to group similar segments. Segment labels during inference are predicted using K-Nearest Neighbor retrievals.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.13", "instruction": "Make the explanation of SegSort shorter.", "revised_paragraph": "Revised paragraph: SegSort (Hwang et al., 2019) is an end-to-end image segmentation model that produces pixel-wise embeddings and over-segmentation based on the assumption that segments follow independent normal distributions on a hypersphere. It employs Spherical K-Means clustering (Banerjee et al., 2005) for joint segmentation and feature learning, promoting homogeneous representations within segments. To optimize the embedding, SegSort uses a maximum likelihood loss to enhance segment discrimination, along with soft neighborhood assignment (Goldberger et al., 2005) to group semantically similar segments. During inference, segment labels are predicted via K-Nearest Neighbor retrievals.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.07", "instruction": "Brush up the sentence for readability", "revised_paragraph": "Revised paragraph: \"P T = \u03c8 ( r ) represents the distribution of representations in the treated and untreated groups, respectively, as induced by the mapping r = \u03c8 ( x ). We can minimize the discrepancy by updating the representation map \u03c8 using gradient-based optimizers, since it is differentiable with respect to \u03c8 (Flamary et al., 2021).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.07", "instruction": "Reorganize the paragraph to make it more logical. Improve the language.", "revised_paragraph": "Revised paragraph: \"The representation distributions under the treated and untreated groups are denoted as P T = \u03c8 (r), which are induced by the mapping r = \u03c8 (x). To minimize the discrepancy between these distributions, the representation map \u03c8 can be updated using gradient-based optimizers, leveraging its differentiability with respect to \u03c8 (Flamary et al., 2021).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1qImCcFQ.Ske132uA7.00", "instruction": "Rephrase the paragraph", "revised_paragraph": "Revised paragraph: \"Analogous to standard rSLDS, the dynamics are conditionally linear when conditioned on a leaf node z_t. It is reasonable to anticipate that adjacent areas in latent space exhibit comparable dynamics. With respect to the tree-structured stick-breaking partitions, those that share a common parent are expected to display similar dynamics. We explicitly incorporate this concept by imposing a hierarchical tree-structured prior on the dynamics.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1qImCcFQ.Ske132uA7.00", "instruction": "Make the paragraph more formal.", "revised_paragraph": "Revised paragraph: \"Analogous to standard recurrent State-Dependent Linear Systems (rSLDS), the dynamics are conditionally linear given a leaf node denoted as z_t. It is reasonable to anticipate that proximate regions within the latent space exhibit analogous dynamics. In the framework of tree-structured stick-breaking partitions, segments that share a common parent should display comparable dynamics. We explicitly formalize this notion by imposing a hierarchical tree-structured prior on the dynamics.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nElUizYNh.CZNfAQwVJ.00", "instruction": "Summarize heavily the results and explanations obtained. Fix any typos.", "revised_paragraph": "Revised paragraph: The experiments conducted on the MNIST dataset, which included 1000 training images, involved disabling all external regularization methods except for learning rate decay and batch normalization. We tested full-batch gradient descent (GD) at various learning rates, finding the highest test accuracy to be 95 (see Figure 2). In comparison, full-batch gradient descent with momentum (GD+M) was assessed with a fixed step size of 0.1 across different momentum values (\u03b2), yielding a maximum test accuracy of 96 (see Figure 3). Notably, GD's best performance across all learning rates was inferior to that of GD+M across all momentum values. Cohen et al. (2021) suggested that gradient descent tends to increase sharpness until reaching a threshold termed \u201cthe edge of stability.\u201d For GD+M, sharpness can escalate up to 2(1 + \u03b2) h, which potentially allows it to operate in sharper regions prior to instability. Although higher permissible sharpness for GD+M might indicate an advantage in test accuracy over GD, our experiments did not support this hypothesis. We propose that the implicit regularization inherent in GD+M significantly contributes to its superior test accuracy, suggesting that the impact of implicit gradient regularization (IGR) for momentum outweighs the effects of sharpness.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nElUizYNh.CZNfAQwVJ.00", "instruction": "Summarize the second half of the paragraph to make the paragraph shorter.", "revised_paragraph": "Revised paragraph: \"We conducted experiments on the MNIST dataset with 1000 training images while disabling all external regularization schemes except for learning rate decay and batch normalization. Full-batch gradient descent (GD) was performed with various learning rates, achieving a maximum test accuracy of 95 (Figure 2). In contrast, full-batch gradient descent with momentum (GD+M) was tested with various momentum values and a fixed learning rate of 0.1, yielding a maximum test accuracy of 96 (Figure 3). Our findings indicate that GD consistently underperformed compared to GD+M. Cohen et al. (2021) highlighted that GD tends to increase sharpness up to 2h, termed \"the edge of stability,\" whereas GD+M can reach a sharpness of up to 2(1+\u03b2)h, allowing it to explore a sharper region before instability. Despite the greater sharpness potential of GD+M, which could suggest superior accuracy, our results showed otherwise, likely due to the implicit regularization effect of GD+M, which surpasses the sharpness advantage of GD.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "instruction": "Make expression concise, add conjuction, include all citations.", "revised_paragraph": "Revised paragraph: \"The definition of \u00b5(\u03b8; \u00b7) is inspired by discussions on effective parameter updates in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021), as previous studies have shown that normalization techniques, such as batch normalization (Ioffe & Szegedy, 2015), influence the update direction of \u03b8, specifically \u03b8 / ||\u03b8||\u00b2, reflecting the extent to which updates to \u03b8 adjust the model f to fit the batch of samples.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "instruction": "Rewrite this paragraph to make it clearer.", "revised_paragraph": "Revised paragraph: \"The definition of \u00b5(\u03b8; \u00b7) is informed by discussions in the literature regarding effective parameter updates (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Prior research indicates that the application of normalization techniques, such as batch normalization (Ioffe & Szegedy, 2015), affects the update direction of \u03b8, specifically \u03b8 / ||\u03b8||\u2082\u00b2. This update direction illustrates the extent to which changes in \u03b8 influence the model f to better accommodate the given batch of samples.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.02", "instruction": "Rewrite this paragraph for improved readability.", "revised_paragraph": "Revised paragraph: Our contribution consists of two main aspects. First, we propose a design for calendars that accommodate medication prescriptions. These calendars facilitate scheduling medication alongside other daily activities and provide mechanisms for addressing scheduling conflicts that may arise due to unsafe drug interactions caused by violating the administration constraints outlined in the prescriptions. Second, we present findings from a qualitative study involving twelve participants who interacted with our calendar designs. The results indicate that it is possible to create calendars that effectively integrate medication management, and participants generally expressed a preference for using such calendars, provided the design remains consistent with familiar calendar formats. Additionally, our findings demonstrate that the calendars can effectively highlight conflicts resulting from unsafe rescheduling. Based on these insights, we outline five key design goals for integrated calendars: 1) utilizing familiar design (DG2), 2) minimizing clutter (DG3), 3) enabling personalization (DG3), 4) supporting personal reflection (DG5), and 5) emphasizing critical information for user attention (DG6).", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.02", "instruction": "Rewrite this paragraph to make it more readable and fitting to the academic style.", "revised_paragraph": "Revised paragraph: Our contribution is twofold. Firstly, we propose a design for calendars specifically tailored to enhance medication prescription management. These calendars facilitate the scheduling of medication alongside other daily activities, while also providing mechanisms for identifying and resolving scheduling conflicts that may arise from unsafe drug interactions due to the violation of administration constraints outlined in the prescriptions. Secondly, we present findings from a qualitative study involving twelve participants who interacted with the proposed calendar designs. The results indicate that calendars can be effectively designed to integrate medication management, and participants expressed a general willingness to use such calendars, provided the designs remain consistent with familiar calendar formats. Moreover, the findings reveal that calendars are capable of rendering conflicts stemming from unsafe rescheduling. Based on these results, we delineate five additional design goals for an integrated calendar: the employment of familiar design elements (DG2), the avoidance of visual clutter (DG3), the allowance for personalization (DG4), the support of personal reflection (DG5), and the emphasis on critical information to capture user attention (DG6).", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.00", "instruction": "Improve the English in this paragraph for better readability.", "revised_paragraph": "Revised paragraph: \"Proteins seldom function in isolation; rather, they typically engage with other proteins to coordinate biological processes (Alberts & Miake-Lye, 1992; Kastritis & Bonvin, 2013). For instance, antibodies, which are components of the immune system, identify pathogens by binding to proteins on their surfaces and trigger immune responses through interactions with receptor proteins on immune cells (Lu et al., 2018). Given that protein-protein interactions are crucial to numerous biological functions, it is essential to develop methods for modulating these interactions. One common approach is to introduce mutations in the amino acids at the interaction interface; certain mutations can enhance binding strength, while others may weaken or even disrupt the interaction (Gram et al., 1992; Barderas et al., 2008). Biologists may choose to either strengthen or weaken binding depending on specific objectives. For example, to boost the efficacy of a neutralizing antibody against a virus, it is generally necessary to enhance the binding strength between the antibody and the viral protein. However, due to the vast combinatorial space of possible amino acid mutations, it is often impractical or cost-prohibitive to conduct experimental assays on all potential variants. Thus, computational approaches are essential for guiding the identification of beneficial mutations by predicting their effects on binding strength, typically represented by changes in binding free energy (\u2206\u2206G).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.00", "instruction": "Lightly revise this paragraph to make it more clear and precise while keeping the academic style.", "revised_paragraph": "Revised paragraph: \"Proteins seldom function in isolation; rather, they typically interact with other proteins to facilitate biological processes (Alberts & Miake-Lye, 1992; Kastritis & Bonvin, 2013). For instance, antibodies\u2014a type of protein involved in immune responses\u2014recognize pathogens by binding to surface proteins and elicit immune reactions through interactions with receptor proteins on immune cells (Lu et al., 2018). Given that protein-protein interactions are crucial for numerous biological functions, developing strategies to modulate these interactions is essential. A common method for altering protein-protein interactions involves mutating amino acids at the interaction interface; some mutations enhance binding affinity while others diminish or entirely disrupt it (Gram et al., 1992; Barderas et al., 2008). Biologists must strategically choose whether to increase or decrease binding strength based on their research objectives. For example, to enhance the efficacy of a neutralizing antibody against a virus, it is typically necessary to strengthen the interaction between the antibody and the viral protein. However, due to the vast combinatorial space of possible amino acid mutations, conducting exhaustive wet-lab assays to evaluate every viable mutation is often impractical and costly. Consequently, computational methods are required to assist in identifying beneficial mutations by predicting their effects on binding affinity as indicated by changes in binding free energy (\u2206\u2206G).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.03", "instruction": "Exclude unnecessary reasoning, correct the typos.", "revised_paragraph": "Revised paragraph: \"This paper studies the generalization performance of gradient-based meta-learning with an overparameterized model. For a precise analysis, we focus on linear models where the total number of data points across all tasks is smaller than the dimension of the model parameters. We show that when the data heterogeneity across tasks is relatively small, the per-task data covariance matrices with certain properties lead to benign overfitting for gradient-based meta-learning with the minimum-norm solution. This explains why overparameterized meta-learning models can generalize well to new data and tasks. Furthermore, our theory demonstrates that overfitting is more likely to occur in meta-learning than in empirical risk minimization (ERM), especially when data heterogeneity across tasks is relatively high. One limitation of this work is that the analysis focuses on the meta-linear regression case. While this analysis can capture practical scenarios in which we reuse the feature extractor from pre-trained models and only meta-train the parameters in the last linear layer, it is also promising to extend our analysis to nonlinear cases using random features and neural tangent kernels in future work.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.03", "instruction": "Improve the english of this paragraph, particularly the last part. Replace all mentions of \"gradient-based meta learning\" with \"nested meta learning\".", "revised_paragraph": "Revised paragraph: \"This paper studies the generalization performance of nested meta learning with an overparameterized model. For a precise analysis, we focus on linear models where the total number of data from all tasks is smaller than the dimension of the model parameter. We show that when the data heterogeneity across tasks is relatively small, the per-task data covariance matrices with certain properties lead to benign overfitting for nested meta learning with the minimum-norm solution. This explains why overparameterized meta learning models can generalize well to new data and new tasks. Furthermore, our theory indicates that overfitting is more likely to occur in meta learning than in empirical risk minimization (ERM), especially when the data heterogeneity across tasks is relatively high. One limitation of this work is that the analysis focuses on the meta linear regression case. While this analysis can capture practical scenarios where we reuse the feature extractor from pre-trained models and only meta-train the parameters in the last linear layer, it is also promising to extend our analysis to nonlinear cases through the use of random features and neural tangent kernels in future work.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.15", "instruction": "Correct some issues in the paragraph and replace certain words to improve it", "revised_paragraph": "Revised paragraph: \"Ablation Studies: Figure 5 presents the results of our ablation experiments on test actions. In the Grid World environment, all ablations utilize a summarized action set as input, which is aggregated through various mechanisms. This enables the identification of available dig-lava skills and informs each action decision accordingly. In such limited action spaces with straightforward action relationships, the summary-based ablations perform comparably to AGILE. This observation is consistent across RecSim and Real RecSys, where the information pertaining to the most common category is adequate for selecting items from the same category to maximize click-through rates (CPR), as illustrated in Figure 6(c). Consequently, we observe only a 5\u201320% improvement of AGILE over the ablations. To assess the consistency of these results, we further evaluate two additional RecSim tasks: (i) Direct CPR: where the objective remains to maximize CPR, yet the agent receives an additional reward based on the CPR metric on top of the click/no-click reward (refer to Section B.3), and (ii) Pairing Environment: in which the goal is to recommend pairs of associated items based on predefined pairings. We replicate the trend indicating that AGILE outperforms the ablations.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.15", "instruction": "Improve the overall clarity of the paragraph.", "revised_paragraph": "Revised paragraph: \"Ablation Results: Figure 5 presents the ablation results for test actions. In the Grid World context, all ablations utilize a summarized action set as input, aggregated through various mechanisms. This allows the system to identify available dig-lava skills and effectively guide action decisions. In small action spaces characterized by simple action relationships, the summary-ablations perform comparably to AGILE. This trend is also observed in RecSim and Real RecSys, where knowledge of the most frequently occurring category suffices to select items from the same category, thereby maximizing click-through rate (CPR), as illustrated in Figure 6(c). Consequently, AGILE demonstrates only a 5% to 20% improvement over the ablation methods. To further assess the consistency of these results, we evaluate two additional RecSim tasks: (i) Direct CPR, where the objective remains maximizing CPR, but the agent receives an extra CPR metric reward in conjunction with the click/no-click reward (Sec B.3), and (ii) Pairing environment, where the task is to recommend pairs of related items based on predefined pairings. We replicate the trend showing that AGILE is greater than or equal to the ablations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "fDUdAYCQqZy.0cNiGAHFml.00", "instruction": "Remove a redundant sentence. Correct citation format.", "revised_paragraph": "Revised paragraph: \"Another line of methods, on the contrary, uses the returns of the behavior policy as the signal for policy learning, as adopted in Wang et al. (2018), Peng et al. (2019), and Chen et al. (2020). However, the behavior policy of the dataset can be imperfect and insufficient to guide policy learning. Learning optimal values within the dataset, on the other extreme, can lead to erroneously optimistic value estimates since data is limited and off-policy. To achieve a trade-off between imitation learning and optimal value learning, we propose Expectile V-learning (EVL), which is based on a new expectile operator that smoothly interpolates between the Bellman expectation operator and the optimality operator.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "fDUdAYCQqZy.0cNiGAHFml.00", "instruction": "Make it shorter by removing what is not essential.", "revised_paragraph": "Revised paragraph: \"Methods such as those in Wang et al. (2018), Peng et al. (2019), and Chen et al. (2020) use the behavior policy's returns for policy learning, maintaining the procedure within the dataset\u2019s support. However, the behavior policy may be imperfect, and learning optimal values can lead to overly optimistic value estimates due to limited, off-policy data. To balance imitation learning and optimal value learning, we propose Expectile V-learning (EVL), which employs a new expectile operator that interpolates between the Bellman expectation and optimality operators.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.13", "instruction": "Replace every apparition of \"\"amino acids\" or \"amino acids in the protein complex\" by \"residues\"", "revised_paragraph": "Revised paragraph: \"Encoder Network The encoder network starts with two multi-layer perceptrons (MLPs) that generate embeddings for each single residue and each pair of residues respectively. The MLP for single residue encodes the residue type, backbone dihedral angles, and local atom coordinates of each residue into a vector e i ( i = 1 . . . n ). The other MLP for residue pairs mainly encodes the distance and relative position between two residues and we denote a pair embedding vector as z ij ( i, j = 1 . . . n ). Next, we use a self-attention-based network invariant to rotation and translation (Jumper et al., 2021) to transform the single embeddings { e i} and pair embeddings { z ij} into hidden representations { h i }. The hidden representation h i is aimed at capturing the information of both the i-th residue itself and its structural environments. It serves as an encoding of the condition { a j , p j , O j , \u02dc \u03c7 j } nj =1 for the probability density w.r.t. \u03c7 i .\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.13", "instruction": "Replace all mentions of amino acid by 'residue'. Reorder sentences in a more logical order when needed.", "revised_paragraph": "Revised paragraph: \"Encoder Network The encoder network starts with two multi-layer perceptrons (MLPs) that generate embeddings for each single residue and each pair of residues, respectively. The MLP for single residues encodes the residue type, backbone dihedral angles, and local atom coordinates of each residue into a vector e i (i = 1 . . . n). The other MLP for residue pairs mainly encodes the distance and relative position between two residues, denoting a pair embedding vector as z ij (i, j = 1 . . . n). Next, we use a self-attention-based network invariant to rotation and translation (Jumper et al., 2021) to transform the single embeddings {e i} and pair embeddings {z ij} into hidden representations {h i}. The hidden representation h i aims to capture the information of both the i-th residue itself and its structural environments. It serves as an encoding of the condition {a j, p j, O j, \u02dc\u03c7 j} nj =1 for the probability density with respect to \u03c7 i.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.03", "instruction": "Improve the english of this paragraph.", "revised_paragraph": "Revised paragraph: \"However, as illustrated in Figure 2(a), the treatment selection bias alters the distribution of covariates across groups. Consequently, \u03d5 1 and \u03d5 0 are likely to overfit the properties of their respective groups, which hinders their ability to generalize to the entire population. As a result, the estimated value of \u02c6 \u03c4 is likely to be biased.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.03", "instruction": "Edit the paragraph to make it more formal and precise.", "revised_paragraph": "Revised paragraph: \"However, as illustrated in Figure 2(a), the selection bias in treatment affects the distribution of covariates across different groups. Consequently, the parameters \u03d51 and \u03d50 may lead to an overfitting of the characteristics specific to each group, resulting in a lack of generalizability to the broader population. As a result, the estimated treatment effect, denoted as \u02c6\u03c4, is likely to be biased.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.10", "instruction": "Rewrite so that it looks more organized", "revised_paragraph": "Revised paragraph: \"The issue primarily arises from the mass preservation constraint. In particular, units that act as outliers at the mini-batch level are compelled to be transported by this marginal constraint, as illustrated in Figure 1. This situation impedes the transport of normal units and makes the group discrepancy susceptible to the sampling effect. Furthermore, the problem is exacerbated by the use of small batch sizes.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.10", "instruction": "Rewrite this paragraph to make it more easily readable.", "revised_paragraph": "Revised paragraph: \"The main issue arises from the constraint of mass preservation. In particular, units that behave as outliers within a mini-batch are compelled to adhere to this constraint, as illustrated in Figure 1. This restriction impedes the transport of typical units and makes the group discrepancies susceptible to sampling effects. Moreover, this problem worsens when the batch size is small.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "instruction": "Improved the writing and reformulate the third sentence", "revised_paragraph": "Revised paragraph: \"Training Settings. In accordance with the methodology established by Zhang et al. (2018b), we apply data augmentation to the training images through random rotations of 90\u00b0, 180\u00b0, and 270\u00b0, as well as horizontal flipping. Each training batch comprises 16 low-resolution (LR) color patches, each sized 48 \u00d7 48 pixels. The training of our super-resolution model employs the ADAM optimizer (Kingma & Ba, 2014) with parameters \u03b21 = 0.9, \u03b22 = 0.999, and \u03b5 = 10\u22128. We initialize the learning rate at 10\u22124, subsequently halving it every 2 \u00d7 10^5 iterations during back-propagation. Our models are implemented using PyTorch (Paszke et al., 2017) and executed on a Tesla V100 GPU.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "instruction": "Improved the writing and reformulate the third sentence", "revised_paragraph": "Revised paragraph: Training Settings. In accordance with the approach outlined by Zhang et al. (2018b), we apply data augmentation to the training images, which includes random rotations of 90\u00b0, 180\u00b0, and 270\u00b0, as well as horizontal flipping. Each training batch is composed of 16 low-resolution color patches, each measuring 48 \u00d7 48 pixels. Our super-resolution model is optimized using the ADAM optimizer (Kingma & Ba, 2014) with parameters \u03b21 = 0.9, \u03b22 = 0.999, and \u03b5 = 10\u22128. The initial learning rate is set at 10\u22124, which is halved after every 2 \u00d7 10^5 iterations of back-propagation. We implement our models using PyTorch (Paszke et al., 2017) on a Tesla V100 GPU.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.22", "instruction": "Switch the two parts of the first sentence", "revised_paragraph": "Revised paragraph: \"Changing the notch to an area where the cursor cannot enter can be considered because avoid-strategy was shown to be desirable in experiment 2. Experiment 3 was almost the same experiment as experiment 2; however, the notch was changed to an area where the cursor cannot enter. The apparatuses, participants, task, and measurements were the same as in experiment 2.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.22", "instruction": "Reorder the first sentence.", "revised_paragraph": "Revised paragraph: \"Changing the notch to an area where the cursor cannot enter can be considered because avoid-strategy was shown to be desirable in experiment 2. Experiment 3 was almost the same experiment as experiment 2; however, the notch was changed to an area where the cursor cannot enter. The apparatuses, participants, task, and measurements were the same as in experiment 2.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.12", "instruction": "Please rephrase my paragraph.", "revised_paragraph": "Revised paragraph: \"Upon completion of the pruning process, we eliminate the insignificant filters\u2014removing them entirely from the model rather than simply zeroing them out\u2014resulting in a more compact model. Subsequently, we fine-tune this smaller model to recover performance, following established practices in the field of pruning (Reed, 1993).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.12", "instruction": "Revise this academic paragraph for readability.", "revised_paragraph": "Revised paragraph: \"Upon completing the pruning process, we eliminate the unimportant filters entirely from the model, rather than merely zeroing them out. This removal leads to a more compact model. Subsequently, we fine-tune this smaller model to restore its performance, as is common practice in pruning (Reed, 1993).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.07", "instruction": "Make this paragraph considerably more concise. Remove any information that is not essential to this paragraph itself.", "revised_paragraph": "Revised paragraph: \"We propagate semantic labels from labeled data C to unlabeled data U by utilizing the identified relationships. C and U represent sets of segment indices, and our label propagation relies on grouping and separating data in a learned feature space. We describe the pixel-to-segment semantic relationships to augment the sets of positive and negative segments.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.07", "instruction": "Delete the third sentence. Concise the first and last one.", "revised_paragraph": "Revised paragraph: \"We aim to propagate semantic labels from labeled data C to unlabeled data U by leveraging the relationships mentioned earlier. We denote C and U as the sets of segment indices. We describe each pixel-to-segment semantic relationship for enhancing the sets of positive and negative segments using both labeled and unlabeled pixels.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "tOMAf1V5dI.SNeLZ71pb5.01", "instruction": "Revise this paragraph to be more concise.", "revised_paragraph": "Revised paragraph: \"MLP-based Architectures. The MLP-Mixer (Tolstikhin et al., 2021) presents a concise framework that leverages matrix transposition and MLP to transfer information between spatial features. Utilizing MLP, layer skip connections, and normalization, MLP-Mixer achieves promising results. A concurrent approach, FF (Melas-Kyriazi, 2021), employs a similar architecture and reaches comparable conclusions. These results indicate that MLP-based architectures can perform on par with CNNs and transformers. Following this, Res-MLP (Touvron et al., 2021a) demonstrates impressive performance using residual MLP solely trained on ImageNet1K. gMLP (Liu et al., 2021a) and EA (Guo et al., 2021) enhance pure MLP performance through the introduction of Spatial Gating Unit (SGU) and external attention, respectively.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "tOMAf1V5dI.SNeLZ71pb5.01", "instruction": "Make this paragraph shorter by deleting details.", "revised_paragraph": "Revised paragraph: MLP-based Architectures. MLP-Mixer (Tolstikhin et al., 2021) presents a concise framework utilizing matrix transposition and MLPs to transmit information between spatial features, achieving promising results through MLP, skip connections, and normalization. Similarly, FF (Melas-Kyriazi, 2021) employs a comparable architecture with similar outcomes. These results indicate that MLP-based architectures can match the performance of CNNs and transformers. Res-MLP (Touvron et al., 2021a) further demonstrates impressive results using only residual MLPs on ImageNet1K. gMLP (Liu et al., 2021a) and EA (Guo et al., 2021) enhance MLP performance by introducing Spatial Gating Unit (SGU) and external attention, respectively.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.03", "instruction": "Improve the paragraph.", "revised_paragraph": "Revised paragraph: \"Neural network pruning is widely recognized as an effective strategy for reducing model complexity (Reed, 1993; Sze et al., 2017). In the pursuit of acceleration, researchers predominantly concentrate on filter pruning, also known as structured pruning (Li et al., 2017), rather than weight-element pruning, referred to as unstructured pruning (Han et al., 2015; 2016b). Integrating filter pruning with image super-resolution (SR) appears to offer a promising avenue for achieving an optimal performance-complexity balance. Nonetheless, the application of filter pruning methods developed for classification tasks is often challenging when directly applied to SR networks. This difficulty primarily arises from the inherent complexity of pruning residual connections in structured pruning scenarios (Li et al., 2017), despite their prevalent use in advanced SR architectures, such as EDSR, which incorporates 32 residual blocks (Lim et al., 2017), and RCAN, which features nested residual blocks (Zhang et al., 2018b).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.03", "instruction": "Rewrite the third sentence using more appropriate language.", "revised_paragraph": "Revised paragraph: \"On the other hand, neural network pruning is well-known as an effective technique to reduce model complexity (Reed, 1993; Sze et al., 2017). For acceleration, researchers primarily concentrate on filter pruning (also known as structured pruning) (Li et al., 2017) rather than weight-element pruning (also referred to as unstructured pruning) (Han et al., 2015; 2016b). Integrating filter pruning with image super-resolution appears to be a viable approach to achieving an improved balance between performance and complexity. However, the application of filter pruning methods developed for classification does not readily extend to super-resolution networks. The primary challenge arises from the difficulty of pruning residual connections in structured pruning (Li et al., 2017), which are extensively utilized in state-of-the-art super-resolution networks (e.g., EDSR (Lim et al., 2017) incorporates 32 residual blocks; RCAN (Zhang et al., 2018b) features even nested residual blocks).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.14", "instruction": "Move the less important details of the training into an appendix. ", "revised_paragraph": "Revised paragraph: \"Architecture, training, and testing. For all experiments on the VOC dataset, we utilize the DeepLab architecture (Chen et al., 2017) with ResNet101 (He et al., 2016) as the backbone network. In our experiments with the DensePose dataset, we employ PSPNet (Zhao et al., 2017) as the backbone. All models are pre-trained on the ImageNet dataset (Deng et al., 2009). Inference is conducted following the SegSort method (Hwang et al., 2019) to perform k-nearest neighbor retrievals. Detailed hyper-parameters and settings for training and testing can be found in the Appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.14", "instruction": "VOC is now named PASCAL VOC. Replace the last two sentence by a reference to an appendix. Correct the english in this paragraph.", "revised_paragraph": "Revised paragraph: \"Architecture, training, and testing. For all the experiments on PASCAL VOC, we base our architecture on DeepLab (Chen et al., 2017) with ResNet101 (He et al., 2016) as the backbone network. For the experiments on the DensePose dataset, we adopt PSPNet (Zhao et al., 2017) as the backbone network. We only use models pre-trained on the ImageNet (Deng et al., 2009) dataset. For training our models, we set \u03bb I, \u03bb C, \u03bb O, and \u03bb A according to different types of annotations and datasets, as shown in Table 1. For inference, we follow SegSort (Hwang et al., 2019) to perform k-nearest neighbor retrievals. Details regarding hyper-parameters and settings for training and testing can be found in the appendix.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.17", "instruction": "Rewrite the second sentence. Replace some words for the better", "revised_paragraph": "Revised paragraph: \"In experiment 1, we demonstrated that the notch prolongs the pointing movement time under certain conditions. Furthermore, when the notch is situated between the targets, participants employed two primary strategies to maneuver the cursor: (i) moving the cursor along the edge (along-strategy) and (ii) circumventing the notch (avoid-strategy). In experiment 2, we explored which of the aforementioned strategies is more advantageous in the current configuration that permits the cursor to penetrate the notch.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.17", "instruction": "Make this paragraph more logical and precise.", "revised_paragraph": "Revised paragraph: \"In Experiment 1, we demonstrated that the presence of the notch increases pointing movement time under specific conditions. Participants employed two primary strategies when moving the cursor between targets: (i) the along-strategy, in which they moved the cursor along the edge of the notch, and (ii) the avoid-strategy, where they aimed to bypass the notch entirely. In Experiment 2, we further examined which of these strategies is more effective in a modified context that permits the cursor to enter the notch.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.10", "instruction": "Rewrite the sentences, making them shorter and more connected.", "revised_paragraph": "Revised paragraph: We utilize the open-sourced RecSim simulator (Ie et al., 2019a) to enable an agent to recommend items to simulated users. The agent works with a base action set of 250 training and 250 test items, receiving 20 items per episode and required to recommend a list of 6. The user preference vector serves as the state in the MDP, while ground truth action representations come from the environment. We enhance click probabilities by incorporating CPR, simulating how the user preference model is influenced by the entire recommendation list. An optimal agent identifies the most prevalent category in the action set and recommends items predominantly from that category, necessitating an understanding of relationships among candidate items. We train CDQN-based models to maximize clicks within user sessions.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.10", "instruction": "Rewrite the sentences of this paragraph for better readability and fitting to the academic style.", "revised_paragraph": "Revised paragraph: \"In this study, we utilize the open-source simulator RecSim (Ie et al., 2019a), which facilitates agent interactions with simulated users through item recommendations. Our base action set comprises 250 training items and 250 test items, with the agent receiving 20 items in each episode, from which it must recommend a list of 6 items. We define the user preference vector as the state within a Markov Decision Process (MDP) and derive ground truth action representations from the environment. To incorporate a Click-Probability Ranking (CPR) mechanism, we enhance the likelihood of a user clicking on any item in proportion to the CPR, thereby simulating scenarios in which the user preference model is influenced by the entire recommendation list. An optimal agent should be capable of identifying the most prevalent category within the available action set and recommending items predominantly from that category. This necessitates an understanding of the relationships among candidate items within the same category. To achieve this, we train models based on Convergent Deep Q-Networks (CDQN) to maximize the total number of clicks during a user session.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "4cxEpKddZp.QPmWFhqQU6.00", "instruction": "Remove the argument that Action Graph is a more natural representation.", "revised_paragraph": "Revised paragraph: \"Recently, Ji et al. (2019) presented Action Genome, a new video dataset annotated by SGs. This dataset includes spatio-temporal SG annotations, where for each video, few individual frames were chosen and spatially annotated by SGs. Here, we use the Something-Something V2 (Goyal et al., 2017) dataset that is larger (200K vs. 10K videos) and more diverse since it includes basic human activities created by a large number of crowd workers. Finally, we propose the Action Graph representation, which we view as a temporal extension of SGs.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "4cxEpKddZp.QPmWFhqQU6.00", "instruction": "Remove the less important details in the last sentence.", "revised_paragraph": "Revised paragraph: \"Recently, Ji et al. (2019) presented Action Genome, a new video dataset annotated by SGs, which includes spatio-temporal SG annotations for selected frames. Here, we use the Something-Something V2 (Goyal et al., 2017) dataset, which is larger and more diverse, as it includes basic human activities from a wide range of crowd workers. Finally, we propose the Action Graph representation, a temporal extension of SGs, and argue it is more natural for representing videos of actions.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.18", "instruction": "Remove unnecessary details and explanations.", "revised_paragraph": "Revised paragraph: Color should be less dominant and not serve as the primary identifier for medication entries. While solid dull color indicated busy slots, using color fill for medication entries cluttered the designs. Additionally, medication identification information in the calendar entry should be minimized, as labels for name and dosage often overflowed the containing entry and caused confusion when multiple entries occupied the same cell. Labels should be abstracted from the overview and made available as details on demand.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.18", "instruction": "I want to restate my third sentence.", "revised_paragraph": "Revised paragraph: \"Color should be made less dominant and should not serve as the primary identifier for medication entries. While solid, dull colors effectively indicated busy slots, the use of color fills for medication entries led to a cluttered design. Additionally, the visual amount of medication identification information within the calendar entry should be minimized. Medication labels, which include the name and dosage, often overflow their containing entry and create confusion regarding their corresponding entries when multiple items occupy the same cell. To enhance clarity, these labels should be abstracted from the overview and made accessible as details on demand.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.10", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"Calendar entries are displayed akin to standard calendars: the height of the rectangular entries represents their duration, while their color hue signifies their type or category, as determined by the user. Additionally, medication entries are depicted using rectangles (or bars), where the vertical position and height indicate the start and end of the allowed administration period for the medication. Each medication entry features an embossed horizontal marker placed along the bar to signify the preferred administration period, at which point a reminder would activate if programmed. The preferred administration time is represented with higher opacity, whereas the allowed administration time is shown with lower opacity. The color hue further encodes the type of medication.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.10", "instruction": "Make the first sentence more concise and direct.", "revised_paragraph": "Revised paragraph: Calendar entries resemble standard calendars, where the height of rectangular entries reflects their duration, color hues denote their user-defined type or category, and labels convey their names. In this design, medication entries are represented by rectangles (or bars), with their vertical position and height indicating the start and end of the allowed administration period. An embossed horizontal marker along the bar indicates the preferred administration period, triggering a reminder if programmed. The preferred administration time is displayed with higher opacity, while the allowed administration time is shown with lower opacity. Color hues encode the type of medication.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "82kcycVoyM.V4CEs3gijY.00", "instruction": "Rewrite some formulations to describe HDNO more as a state than a progress.", "revised_paragraph": "Revised paragraph: HDNO represents a state where dialogue acts are regarded as options within a structured framework. In this state, each word generated by Natural Language Generation (NLG) is recognized as a primitive action. Consequently, dialogue policy and NLG establish a high-level policy governing option selection and a low-level policy addressing intra-option actions, respectively. Unlike traditional modular systems, HDNO incorporates contextual elements into NLG to align with the option framework. Furthermore, the primitive action space, which includes a termination symbol, allows NLG to assume the responsibility for managing termination, leading to the incorporation of the termination policy within the intra-option policy. HDNO is formally defined in Definition 1.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "82kcycVoyM.V4CEs3gijY.00", "instruction": "Make the last sentence more direct. Give slightly more context for better readability.", "revised_paragraph": "Revised paragraph: HDNO. Specifically, in HDNO, a dialogue act is viewed as an option, while each generated word from natural language generation (NLG) is treated as a primitive action. As a result, dialogue policy represents a high-level policy over options, whereas NLG functions as a low-level policy within the option framework. Unlike conventional modular systems, we enhance NLG by providing it with context to better fit this option framework. Furthermore, since the primitive action space, which consists of the vocabulary, includes a termination symbol, NLG can effectively manage the termination process. Consequently, the termination policy is integrated into the intra-option policy. HDNO is formally defined in De\ufb01nition 1.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.19", "instruction": "Make the benefit clearer.", "revised_paragraph": "Revised paragraph: \"Representation-based methods, such as Bayesian Neural Networks (BNN) (Johansson et al., 2016) and Counterfactual Regression (CFR) (Uri et al., 2017), effectively balance the distributions in the latent space, thereby enhancing the model's ability to accurately capture relationships within the data. Notably, Liuyi et al. and Hassanpour & Greiner (2020) improve upon CFR by incorporating local similarity and non-confounding factors, which further refine the model's accuracy. Kallus (2020) and Yoon et al. (2018) introduce adversarial training as a means to achieve balanced distributions in representations, which aids in more robust decision-making. Overall, representation learning stands out as the primary approach for mitigating treatment selection bias due to its capacity to avoid high variance issues, making it particularly suitable for large-scale scenarios where data complexity might otherwise hinder accurate analysis.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.19", "instruction": "Reorder the elements in sentences to improve the readability.", "revised_paragraph": "Revised paragraph: \"Representation learning has emerged as the primary approach to mitigate treatment selection bias, primarily due to its ability to avoid the high variance issue and its suitability for large-scale scenarios. Among the representation-based methods, BNN (Johansson et al., 2016) and CFR (Uri et al., 2017) focus on balancing distributions in the latent space. Additionally, Liuyi et al. and Hassanpour & Greiner (2020) enhance CFR by incorporating local similarity and non-confounding factors, respectively. Furthermore, Kallus (2020) and Yoon et al. (2018) propose the use of adversarial training to balance the distributions of representations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ssjKKm0b5y.3wi5X8wrM_.00", "instruction": "Exclude unnecessary ideas.", "revised_paragraph": "Revised paragraph: \"Mahapatra & Rajan (2020) introduced the Exact Pareto Optimal (EPO) method, a multi-objective optimization (MOO) technique that converges to a specified ray in loss space. Given a preference ray r, EPO seeks an exact Pareto optimal solution that is both Pareto optimal and lies at the intersection of the Pareto front and the preference vector r. The EPO method balances the objectives of identifying a descent direction and reaching the desired ray. It searches for a point within the convex hull of the gradients, which includes descent directions, that forms a maximal angle with a vector d bal that directs the point toward the desired ray. By combining gradient descent and controlled ascent, EPO can reach an exact Pareto optimal solution if one exists, or the nearest Pareto optimal solution.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ssjKKm0b5y.3wi5X8wrM_.00", "instruction": "Rewrite the first half of this paragraph to make it clearer and easier to read.", "revised_paragraph": "Revised paragraph: \"Mahapatra and Rajan (2020) introduced the Exact Pareto Optimal (EPO) method, a multi-objective optimization (MOO) technique capable of converging to a specified ray in the loss space. Given a preference ray, denoted as \\( r \\), EPO aims to identify an exact Pareto optimal solution. This solution meets two criteria: it is (i) Pareto optimal and (ii) lies at the intersection of the Pareto front and the preference vector \\( r \\). The EPO approach seeks to balance two objectives: determining a suitable descent direction and moving towards the desired ray.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hAi0PMz9T7.Ut8ESfYp1.02", "instruction": "Rewrite some formulations, preferring shorter ones and fix typos.", "revised_paragraph": "Revised paragraph: \"Branch Decider. The network context is unknown during deployment, necessitating a branch decider module. This module reuses cluster labels from the training stage for K Nearest Neighbors classification. A lightweight distance-based metric classifies the inference-time observation into one of the training groupings, allowing the execution of the corresponding branch's symbolic policy. Figure 3 illustrates our complete training and deployment techniques.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hAi0PMz9T7.Ut8ESfYp1.02", "instruction": "Improve the english of this text.", "revised_paragraph": "Revised paragraph: \"Branch Decider. Since the network context is unknown during deployment, there is a necessity for a branch decider module. The branch decider utilizes cluster labels from the training phase for K Nearest Neighbors classification. A lightweight distance-based metric is employed to classify the observation at inference time into one of the training groupings, thereby executing the symbolic policy corresponding to that branch. Figure 3 illustrates our complete training and deployment techniques.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.16", "instruction": "Remove the second part of the first sentence", "revised_paragraph": "Revised paragraph: First, many models have high | d util |. This confirms that the standard multi-modal learning process encourages the model to rely on one modality and ignore the other one, which is consistent with our hypothesis. We make this observation across all tasks, confirming that the conventional multi-modal learning process is greedy regardless of network architectures and tasks.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.16", "instruction": "Exclude unnecessary details.", "revised_paragraph": "Revised paragraph: \"Many models exhibit high | d util | when trained with two distinct modalities, supporting our hypothesis that conventional multi-modal learning encourages reliance on one modality while neglecting the other. This observation is consistent across all tasks, indicating that the typical multi-modal learning process is inherently greedy.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "uTur5gpEC.TOWMu718N.00", "instruction": "Be more specific when talking about the result.", "revised_paragraph": "Revised paragraph: \"To examine the impact of employing a discriminator trained on empirical distributions, we define the reward as being represented by an optimal discriminator, such that r(s, a) = log E(s, a) D(s, a). Consequently, this specification modifies objective (9) to reflect the enhanced performance characteristics associated with the optimal discriminator.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "uTur5gpEC.TOWMu718N.00", "instruction": "Improve this text to fit a more academic style.", "revised_paragraph": "Revised paragraph: \"In order to examine the effects of employing a discriminator trained on empirical distributions, we assume that the reward is denoted by an optimal discriminator, such that r(s, a) = log E(s, a) D(s, a). Consequently, objective (9) is reformulated as follows:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.15", "instruction": "Make this paragraph more concise.", "revised_paragraph": "Revised paragraph: The calendar design should align with familiar user interfaces (DG2). This was evident in aspects like layout, medication entries, and icons for annotations, such as those indicating meals. Over 80% of participants preferred Design B due to its resemblance to existing calendars, despite Design A being intended for that purpose. Participants found Design A cluttered because its medication entries spanned the entire administration period. Preferred layouts featured a vertical orientation with days of the week at the top and times on the left. Medication dosages should use familiar units consistent with prescriptions, displaying actual quantities (e.g., milligrams) rather than relative measures like the number of pills. Additionally, realistic food-related icons, like bananas, should denote medications that require food, rather than custom icons resembling utensils such as spoons, and should be positioned with the entry, not in medication summaries.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.15", "instruction": "Shorten my sentence related to realistic food-related icons.", "revised_paragraph": "Revised paragraph: \"The design of a calendar must align with familiar calendar interfaces (DG2), evident in aspects like layout, medication entries, and icons for food-related annotations. Over 80% of participants preferred Design B due to its similarity to existing calendars, which was surprising since Design A was intended to mimic them. The rejection of Design A is mainly attributed to the height of its medication entries, which cluttered the interface. Results suggest a vertically oriented layout with days of the week at the top and times of day on the left. The dosage in medication entries should be familiar to users, using consistent units like milligrams rather than relative classifications like number of pills. Furthermore, realistic food-related icons are preferred over custom designs for indicating medications that must be taken with food; for example, bananas are more effective than a spoon icon. Such icons should be placed alongside the entry rather than in medication summaries.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.20", "instruction": "Revise this paragraph for better clarity.", "revised_paragraph": "Revised paragraph: \"It is important to highlight our distinctions from emerging causal inference approaches that utilize optimal transport. Dunipace (2021) enhances the Inverse Probability Weighting (IPS) method with a propensity score estimator grounded in optimal transport; however, this approach suffers from the previously mentioned high variance problem. Similarly, Torous et al. (2021) utilize the push-forward operator to improve change-in-change models, but their methods are tailored for multi-phase data, which is not available in our context. Li et al. (2022) share some similarities with our research; however, their focus is on variable decomposition in latent space and aligns closely with the work of Uri et al. (2017) regarding alignment technology. Our contribution lies in exploring the role and flexibility of optimal transport to enhance Causal Forest Regression (CFR), addressing the Mean Squared Error (MSE) and Uncertainty of Causal Estimates (UCE) challenges that have long been overlooked in the existing literature, even as recently as this year.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.20", "instruction": "Make this paragraph more simple to read and concise phrases that are to long when possible.", "revised_paragraph": "Revised paragraph: \"We highlight our differences from new causal inference methods based on optimal transport. Dunipace (2021) enhances the IPS method using a propensity score estimator linked to optimal transport, but it faces high variance issues. Torous et al. (2021) improve change-in-change models with a push forward operator, but these models are tailored for multi-phase data, which is not applicable in our context. Li et al. (2022) address similar conditions but focus on variable decomposition in latent space, and their approach aligns with Uri et al. (2017). Our contribution is to explore how optimal transport can enhance CFR, reducing the mean square error (MSE) and unconditional coverage error (UCE) challenges that have persisted in the literature, including recent studies.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.13", "instruction": "Rewrite for fluency (while keeping the original structure of the sentences)", "revised_paragraph": "Revised paragraph: \"Training protocol. A fully connected neural network with two hidden layers, each containing 60 neurons, has been selected to implement the representation map \u03c8 and the factual outcome map \u03d5 for ESCFR and other neural network-based baselines. To ensure a fair comparison, all neural models are trained for 400 epochs using the Adam optimizer, as described by Kingma and Ba (2015), with both the learning rate and weight decay set to 1 \u00d7 e \u2212 3. Other optimizer settings follow the guidelines established by Kingma and Ba (2015). We conduct a hyperparameter search within the range depicted in Figure 5, checkpointing the validation performance every 2 epochs, and subsequently export the best model to assess its performance on the test dataset.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.13", "instruction": "Improve the language in this text and make it slightly more formal.", "revised_paragraph": "Revised paragraph: The training protocol involves the use of a fully connected neural network featuring two hidden layers, each containing 60 nodes, to establish the representation map \u03c8 and the factual outcome map \u03d5 for ESCFR and other neural network-based baselines. To ensure a fair comparison, all neural models are trained over 400 epochs utilizing the Adam optimizer, as described by Kingma and Ba (2015), with both the learning rate and weight decay set to 1 \u00d7 10^\u22123. The remaining optimization settings adhere to the guidelines established by Kingma and Ba (2015). Hyperparameters are explored within the range illustrated in Figure 5, validation performance is recorded every two epochs, and the best-performing model is subsequently exported for evaluation on the test dataset.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OzYyHKPyj7.O9Mk1uqXra.02", "instruction": "Make this paragraph shorter", "revised_paragraph": "Revised paragraph: \"The Nondeterministic Stack RNN (NS-RNN) model by DuSell & Chiang (2020) maintains a probability distribution over entire stacks using a weighted PDA, differing from the superposition stack which calculates expectations over individual elements. Although it has cubic time complexity and quadratic space complexity relative to input length, resulting in longer run times compared to other stack RNNs, it often achieves superior task performance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OzYyHKPyj7.O9Mk1uqXra.02", "instruction": "Remove the unnessential details from the paragraph.", "revised_paragraph": "Revised paragraph: \"The Nondeterministic Stack RNN (NS-RNN) model by DuSell & Chiang (2020) maintains a probability distribution over whole stacks, utilizing a weighted PDA. Although it has cubic time complexity and quadratic space complexity with respect to input length, it often achieves better task performance compared to other stack RNNs.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.14", "instruction": "Improve the understandability of the entire paragraph", "revised_paragraph": "Revised paragraph: \"Study Design: For each task outlined in \u00a75.1, we create a dataset by duplicating one of the modalities to serve as the two input modalities, alongside the original dataset. For instance, in the case of MNIST, we predict the digit class using two identical grayscale images. We then train a multi-modal deep neural network (DNN) on each task's dataset, as detailed in the following sections.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.14", "instruction": "The wording in this paragraph is confusing, please improve the readability.", "revised_paragraph": "Revised paragraph: \"Study Design: For each task outlined in \u00a75.1, we create a dataset by duplicating one of the input modalities alongside the original dataset. For instance, in the case of MNIST, we predict the digit class using two identical gray-scale images. We then train a multi-modal deep neural network (DNN) on each dataset corresponding to each task, as detailed below:\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "HkfoEihoH.QdgynxJz5.00", "instruction": "Simplify the wording of this paragraph.", "revised_paragraph": "Revised paragraph: \"This simple sampler generally produces diverse batches like a k-DPP. As illustrated in Figure 1, switching between the two samplers does not impact the active learner\u2019s statistical performance, but significantly enhances computational efficiency. A detailed comparison of the running times and test accuracies of k-MEANS++ and k-DPP based gradient embedding sampling is available in Appendix G.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "HkfoEihoH.QdgynxJz5.00", "instruction": "Make the last sentence more concise.", "revised_paragraph": "Revised paragraph: \"This simple sampler tends to produce diverse batches similar to a k-DPP. As shown in Figure 1, switching between the two samplers does not affect the active learner\u2019s statistical performance while greatly improving computational performance. A detailed comparison of the running times and test accuracies of k-MEANS++ and k-DPP based gradient embedding sampling is in Appendix G.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "jyac3IgQ44.f4au9jfat5.03", "instruction": "Rewrite this paragraph for improved readability.", "revised_paragraph": "Revised paragraph: \"Let { s_k | s_k \u2208 Z^3 } for k = 0 denote a series of window sizes, with s_0 representing the size of the query window and s_1, ..., s_M representing the sizes of M progressively larger key windows. Define V = { v_i | v_i = ( x_i , f_i ) } for i = 1 to |V| as the input voxel set, where the coordinates x_i \u2208 Z^3 correspond to voxel i and its feature vector f_i \u2208 R^C. We begin by partitioning the voxel set into non-overlapping 3D windows, each of size s_0, identifying the non-empty ones as query windows, with their centers represented as { c_i | c_i \u2208 Z^3 } for i = 0 to L, where L is the total number of query windows. To obtain the query voxels V_{c_i} corresponding to the query window centered on c_i, we collect all non-empty voxels within this window as the queries. In order to enhance efficiency, we introduce a novel chessboard sampling strategy, which will be explained in Section 3.1.2.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "jyac3IgQ44.f4au9jfat5.03", "instruction": "Improve the readability of this paragraph.", "revised_paragraph": "Revised paragraph: \"Let { s_k | s_k \u2208 Z^3 } with k = 0 represent a series of window sizes, where s_0 is the size of the query window and s_1, ..., s_M are the sizes of M progressively larger key windows. We define V = { v_i | v_i = (x_i, f_i) } for i = 1 to |V| as the input voxel set, where x_i \u2208 Z^3 represents the xyz coordinates and f_i \u2208 R^C is the feature vector for voxel i. Initially, we partition the voxel set into non-overlapping 3D windows, each of size s_0, and identify the non-empty ones as query windows. The centers of these query windows are denoted by { c_i | c_i \u2208 Z^3 } for i = 0 to L, where L indicates the total number of query windows. To obtain the query voxels V_{c_i} for the query window centered on c_i, we simply gather all the non-empty voxels within the window. To maintain efficiency, we introduce a novel chessboard sampling strategy, which will be discussed in Section 3.1.2.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.13", "instruction": "Rewrite the middle sentences, preferring active formulations over passive ones.", "revised_paragraph": "Revised paragraph: \"\u2022 Mask-Output (No representations, No input action set): Assuming a fixed action space in the output, the system masks out q-values or policy probabilities for unavailable actions. It represents prior SAS-MDP works - Boutilier et al.; Chandak et al. (2020a), Huang & Onta\u00f1\u00f3n (2020). \u2022 Mask-Input-Output (No representations): This method augments the binary availability mask of given actions, using 1s for available indices and 0s for unavailable ones, and adds it to the state input of Mask-Output. \u2022 Utility-Policy (No input action set): Proposed by Jain et al. (2020), this approach uses action representations independently to compute each action\u2019s utility, without considering any interdependence. \u2022 Simple DQN (No cascade, No input action set): For listwise RL specifically, we include the DQN baseline that selects the top-K items directly, rather than reasoning about the overall list. As a result, it overlooks interdependences among available actions and other items in the list.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.13", "instruction": "\"No representations\" become \"No action representations\". Rewrite the \"Mask-Input-Output\" and \"Utility-Policy\" points in the list for better readability. Uniformise the language in the paragraph.", "revised_paragraph": "Revised paragraph: \"\u2022 Mask-Output (No action representations, No input action set): This approach assumes a fixed action space in the output, whereby q-values or policy probabilities are masked for unavailable actions. It reflects prior works in SAS-MDP, such as Boutilier et al., Chandak et al. (2020a), and Huang & Onta\u00f1\u00f3n (2020). \u2022 Mask-Input-Output (No action representations): This method enhances the binary availability mask of actions, assigning 1s to available indices and 0s to unavailable ones, and incorporates this mask into the state input of Mask-Output. \u2022 Utility-Policy (No input action set): Proposed by Jain et al. (2020), this framework computes each action\u2019s utility using action representations independently, disregarding any interdependence among them. \u2022 Simple DQN (No cascade, No input action set): Specifically for listwise reinforcement learning, this baseline includes a DQN model that selects the top-K items without considering the overall list. Consequently, it overlooks interdependences: (i) among other available actions, and (ii) with other items in the list.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.02", "instruction": "Improve the english", "revised_paragraph": "Revised paragraph: \"Proactive Motion Capture has been examined in a limited number of studies that focus on the use of a single mobile camera (Zhou et al., 2018; Cheng et al., 2018; Kiciroglu et al., 2019). In contrast, a larger body of research has investigated the control of multi-camera teams. Many of these studies employ optimization techniques across various system designs, including marker-based systems (N\u00e4geli et al., 2018), RGBD-based systems (Xu et al., 2017), two-stage systems (Saini et al., 2019; Tallamraju et al., 2019), and hierarchical systems (Ho et al., 2021). It is essential to note that the aforementioned methods primarily address static occlusion sources or operate within clean environments. A shared limitation of these studies is the reliance on hand-crafted optimization objectives and fixed camera formations, resulting in inadequate adaptability to dynamic scenes characterized by uncertainties. Recently, reinforcement learning (RL) methods have garnered attention for their potential to facilitate dynamic formation adjustments. These approaches have focused on active 3D human pose estimation (HPE) in the Gazebo simulation (Tallamraju et al., 2020) or within a Panoptic dome (Joo et al., 2015; Pirinen et al., 2019; G\u00e4rtner et al., 2020) for active view selection. Among them, AirCapRL (Tallamraju et al., 2020) bears similarities to our research; however, it is limited to coordinating between two cameras in clean landscapes devoid of occlusion. Our study expands on this by examining collaboration among multiple cameras and addressing the credit assignment problem with innovative CTCR incentives. Furthermore, we tackle a more challenging scenario that includes multiple distracting humans as dynamic occlusions, necessitating the development of specialized algorithms to effectively manage them.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NAxP0iFmBr.5QBuYp8GH.02", "instruction": "Use accurate words and expression.", "revised_paragraph": "Revised paragraph: \"Proactive Motion Capture has been explored in a limited number of studies utilizing a single mobile camera (Zhou et al., 2018; Cheng et al., 2018; Kiciroglu et al., 2019). In contrast, a more extensive body of research has investigated the control of multi-camera systems. Many of these studies employ optimization techniques across various system designs, including marker-based approaches (N\u00e4geli et al., 2018), RGBD-based methods (Xu et al., 2017), two-stage systems (Saini et al., 2019; Tallamraju et al., 2019), and hierarchical configurations (Ho et al., 2021). It is important to highlight that the aforementioned methods primarily address static occlusions and operate within clean environments. A common characteristic among these works is their reliance on manually crafted optimization objectives and fixed camera formations, which limit adaptability to dynamic scenes fraught with uncertainties. Recently, reinforcement learning (RL)-based methods have garnered increasing attention due to their capability for dynamic formation adjustments. These studies have focused on active three-dimensional human pose estimation in environments such as the Gazebo simulation (Tallamraju et al., 2020) or the Panoptic dome (Joo et al., 2015; Pirinen et al., 2019; G\u00e4rtner et al., 2020) for active view selection. Notably, AirCapRL (Tallamraju et al., 2020) exhibits similarities to our approach; however, it is limited to coordinating two cameras in clean environments devoid of occlusions. Our research examines the collaboration between multiple cameras and addresses the credit assignment challenge using novel CTCR incentives. Furthermore, we investigate a more complex scenario involving multiple distracting individuals as dynamic occlusions, necessitating specialized algorithms for effective management.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.17", "instruction": "Edit this paragraph to be more concise.", "revised_paragraph": "Revised paragraph: \"Visual quality comparison and ablation study indicate that our results align closely with image boundaries and are visually comparable to the fully-supervised model. Notably, Fig. 7 demonstrates significant improvement when different relationships are additively introduced for enhanced regularization. Additional details and ablation studies can be found in the Appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.17", "instruction": "Improve the english of this paragraph and make it slightly shorter.", "revised_paragraph": "Revised paragraph: \"Visual quality comparison and ablation study. The results in Fig. X demonstrate strong alignment with image boundaries and visual similarity to fully-supervised counterparts. Additionally, Fig. 7 illustrates a significant improvement in our results through the additive introduction of various relationships for enhanced regularization. For further details and ablation studies, please refer to the Appendix.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "ydudDS_QrK.dSEEUtAQ1H.00", "instruction": "Remove the last sentence of this paragraph.", "revised_paragraph": "Revised paragraph: \"We have developed a novel influence analysis to understand the effects of graph elements on the parameter changes of GCNs without needing to retrain the GCNs. We chose Simple Graph Convolution due to its convexity and its competitive performance to non-linear GNNs on a variety of tasks. Our influence functions can be used to approximate the changes in model parameters caused by edge or node removals from an attributed graph. Moreover, we provided theoretical bounds on the estimation error of the edge and node influence on model parameters. We experimentally validated the accuracy and effectiveness of our influence functions by comparing its estimation with the actual influence obtained by model retraining. We showed in our experiments that our influence functions could be used to reliably identify edge and node with negative and positive influences on model performance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "ydudDS_QrK.dSEEUtAQ1H.00", "instruction": "Delete the last sentence of this paragraph.", "revised_paragraph": "Revised paragraph: \"We have developed a novel influence analysis to understand the effects of graph elements on the parameter changes of GCNs without needing to retrain the GCNs. We chose Simple Graph Convolution due to its convexity and its competitive performance to non-linear GNNs on a variety of tasks. Our influence functions can be used to approximate the changes in model parameters caused by edge or node removals from an attributed graph. Moreover, we provided theoretical bounds on the estimation error of the edge and node influence on model parameters. We experimentally validated the accuracy and effectiveness of our influence functions by comparing its estimation with the actual influence obtained by model retraining. We showed in our experiments that our influence functions could be used to reliably identify edge and node with negative and positive influences on model performance.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hAi0PMz9T7.Ut8ESfYp1.03", "instruction": "Rewrite the last sentence and make it easier to understand.", "revised_paragraph": "Revised paragraph: \"Interpretability \u2013 a universal boon for ML? In the PCC domain, model interpretability is closely tied to the availability of domain knowledge. By converting a complex black-box neural network into a more straightforward symbolic rule, practitioners in network congestion can more easily identify issues and make manual modifications or improvements.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hAi0PMz9T7.Ut8ESfYp1.03", "instruction": "Rephrase the second part of the last sentence.", "revised_paragraph": "Revised paragraph: \"Interpretability \u2013 a universal boon for ML? In the PCC domain, model interpretability is associated with the depth of domain knowledge. By transforming a black-box neural network into a transparent symbolic rule, the congestion rule enables network congestion practitioners to more easily identify issues and make manual modifications or improvements.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "NvI7ejSHFe.ppieLd2M4a.01", "instruction": "Remove the second sentence", "revised_paragraph": "Revised paragraph: \"In this section, we investigate the influence of activation functions in PINNs for solving PDE/ODE systems. The results show that the choice of activation functions is crucial for PINNs and depends on the problem. Motivated by this observation, we propose to learn specialized activation functions for different PDE systems.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "NvI7ejSHFe.ppieLd2M4a.01", "instruction": "Remove unnecessary details to make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"We investigate the influence of activation functions in PINNs for solving PDE/ODE systems. Our evaluation of several common activation functions reveals that their effectiveness is crucial and problem-dependent. Based on this, we propose learning specialized activation functions for different PDE systems.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "TFoRhVCpnb.yqo5NaW74.01", "instruction": "Remove unnecessary details and make my numbers clear.", "revised_paragraph": "Revised paragraph: \"Table 2 presents the segmentation performance on the MS COCO 2014 dataset, highlighting our method's mIoU value of 42.6% on the validation set. This result surpasses the IRN baseline by 1.2% and significantly exceeds other recent competitive methods. Specifically, we reproduce results for IRN using CONTA, achieving an mIoU of 41.4%. In terms of relative improvements, CONTA shows a 0.8% increase over IRN (from 32.6% to 33.4%), while our method demonstrates a 1.2% increase (from 41.4% to 42.6%).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "TFoRhVCpnb.yqo5NaW74.01", "instruction": "Remove the sentence about reproduction", "revised_paragraph": "Revised paragraph: MS COCO 2014 dataset: Table 2 illustrates the segmentation performance on MS COCO 2014 compared with other methods. Our method achieves 42.6% in terms of the mIoU values on the validation set, surpassing 1.2% over the IRN [1], regarded as our baseline, and outperforming other recent competitive methods [8, 56, 49, 1] by a large margin. Hence, we compare the relative improvements for comparison: CONTA reaches a 0.8% mIoU improvement compared with IRN (32.6 to 33.4), while our method achieves a 1.2% mIoU improvement (41.4 to 42.6).", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.18", "instruction": "Rewrite this paragraph to improve readability and make contributions more evident.", "revised_paragraph": "Revised paragraph: In this paper, we introduce a groundbreaking approach to universal weakly-supervised semantic segmentation, which utilizes Semisupervised Pixel-wise Metric Learning. Our method identifies four distinct types of attraction and repulsion relationships between pixels and segments. These relationships can be leveraged regardless of the nature of the partial annotations, which may include coarse image tags, bounding boxes, sparse keypoints, or scribbles. Our experiments on the PASCAL VOC and DensePose datasets demonstrate that our approach consistently delivers significant improvements compared to state-of-the-art methods, particularly when dealing with the least informative keypoint supervision.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.18", "instruction": "Improve the logical flow of ideas in this text.", "revised_paragraph": "Revised paragraph: \"SUMMARY We present a novel universal weakly-supervised semantic segmentation method that utilizes Semisupervised Pixel-wise Metric Learning. Our approach identifies four common types of pixel-to-segment attraction and repulsion relationships, applicable regardless of whether the partial annotations consist of coarse image tags and bounding boxes or sparse keypoints and scribbles. The evaluation of our method on the PASCAL VOC and DensePose datasets demonstrates consistent and substantial improvements over the state-of-the-art, particularly with the sparsest keypoint supervision.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Z3g7qGrao.v3MXAzVXjk.00", "instruction": "Remove details about the theoretical framework and make this paragraph more concise.", "revised_paragraph": "Revised paragraph: We select a small constant for M, as L D, [X D, X M] serves as a lower bound for the log marginal likelihood log p(D) when M > 1. We set r to a number of particle functions that can effectively represent the posterior SP. Our goal is to estimate (cid:101)\u2207F L D, X(Q[T])(cid:12)(cid:12)F=0 using mini-batches. An unbiased estimate of \u2113(D, f iX D) is n/s \u00b7 \u2113(D s, f i (cid:101)X), leading to \u03bb = s/n. Although L D s, X is not generally a lower bound of log p(D), we observed that \u03bb = s/n yields reasonable performance in practice. We present two algorithms, Stein functional variational NNs and Stein functional variational gradient boosting (A.3.1), which rely on the estimated Stein functional variational gradient. If an analytical score gradient does not exist, a score gradient estimator can be employed, although this is often computationally intensive. Our framework also incorporates a gradient-free SVGD method that only requires evaluating the marginal densities of the prior process.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Z3g7qGrao.v3MXAzVXjk.00", "instruction": "Remove unnecessary content to make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"We select a small constant for M, as L D , [ X D , X M ] serves as a lower bound for the log marginal likelihood log p ( D ) when M > 1. We set r to represent the posterior SP accurately. Our focus is on estimating (cid:101) \u2207 F L D , X ( Q [ T ] ) (cid:12)(cid:12) F =0 using mini-batches. An unbiased estimate of \u2113 ( D , f iX D ) is n/s \u00b7 \u2113 ( D s , f i (cid:101) X ), leading to \u03bb = s/n. Although L D s , X is not generally a lower bound of log p ( D ), our practical findings suggest \u03bb = s/n yields reasonable performance. Our theoretical framework ensures that full-batch training maximizes a lower bound of log p ( D ). We present two algorithms: Stein functional variational NNs and Stein functional variational gradient boosting (A.3.1), which rely on the estimated score gradient of the functional prior at X. In the absence of an analytical score gradient, we can utilize a score gradient estimator per Sun et al., though this can be computationally intensive (Zhou et al., 2020). Additionally, our framework allows for a gradient-free SVGD approach (Han & Liu, 2018) that only utilizes the evaluation of prior process marginal densities.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.15", "instruction": "Make first half concise and second half precise.", "revised_paragraph": "Revised paragraph: \"Matching methods, such as PSM, show strong performance on ranking metrics, making them popular in counterfactual ranking applications (Betlei et al., 2021). However, their limited effectiveness on PEHE restricts their use in counterfactual estimation applications like advertising systems, where accuracy is imperative.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.15", "instruction": "Improve the writing in this paragraph.", "revised_paragraph": "Revised paragraph: \"Matching methods, such as Propensity Score Matching (PSM), demonstrate impressive performance on ranking metrics, which accounts for their popularity in counterfactual ranking applications (Betlei et al., 2021). However, their inadequate performance in terms of Population Efficacy of the Heterogeneous Effect (PEHE) limits their applicability in counterfactual estimation scenarios, particularly in advertising systems where accuracy metrics are of greater importance.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "SRquLaHRM4.vI2x5N-YHC.02", "instruction": "Remove any information that is not essential to the main points of the paragraph.", "revised_paragraph": "Revised paragraph: \"In this section, we replace the cosine distance between the global visual feature and prompt ensemble with the OT distance between the feature map and all 7 prompts. However, the OT distance achieves only 58.78% accuracy without any learning, highlighting a limitation of the PLOT method, which requires few-shot data for optimization and cannot be directly applied in the zero-shot setting. We identify two reasons for the ineffectiveness of the OT distance without learning: 1) prompt engineering relies on the global feature and cosine distance rather than OT distance with the feature map; 2) the selected prompts are too similar to the global feature and lack complementarity.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "SRquLaHRM4.vI2x5N-YHC.02", "instruction": "I do not want to mention a limitation.", "revised_paragraph": "Revised paragraph: \"Using the ImageNet dataset, we further ensemble our models to achieve a top-1 accuracy of 60.38%. In this section, we replace the cosine distance between the global visual feature and prompt ensemble with the optimal transport (OT) distance between the feature map and all seven prompts. Without any learning, the OT distance yields an accuracy of 58.78%. We propose that the performance of the OT distance can be attributed to two factors: 1) prompt engineering typically selects prompts based on the global feature and cosine distance rather than the OT distance with the feature map; and 2) the selected prompts are closely aligned with the global feature, lacking the necessary complementarity.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "kBsx5htyKn.qV5njV8W5.03", "instruction": "Shorten this paragraph by removing details about the figure.", "revised_paragraph": "Revised paragraph: \"The proportion of outliers selected by uncertainty sampling varies significantly across different datasets, with newsgroup exhibiting the highest average at 80%.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "kBsx5htyKn.qV5njV8W5.03", "instruction": "Remove unnecessary details to make this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"Fig 2 shows the proportion of outliers selected through uncertainty sampling across different datasets and models. Each bar represents the average of three runs, where 10% random data points from one of the three other datasets are injected. The dataset significantly influences the selection of outliers, with the newsgroup dataset having the highest average proportion at 80%.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "jd7eZJSVj.YFwsqqBl_J.00", "instruction": "Move the less important results to an appendix.", "revised_paragraph": "Revised paragraph: \"We present the training curves of Algorithm 1 on the AKI dataset in Figures 3 and 4. The performance of the RL policies, evaluated on train, validation, and test sets, is illustrated in Figure 3. The similarity among the three curves confirms the generalizability of the learned dynamic classification policy. In Figure 4, we demonstrate that the diagnostic classifier, which is trained from scratch in an end-to-end approach, effectively maps partially-observed patient states to accurate diagnoses and predictions. To focus on classification quality while eliminating the influence of dynamic test selection, we evaluate this classifier on static data distributions. The results indicate that the classifier improves its prediction accuracy during reinforcement learning, primarily based on the data selected by the RL algorithm.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "jd7eZJSVj.YFwsqqBl_J.00", "instruction": "Replace less important results by a reference to Appendix B. Revise this paragraph.", "revised_paragraph": "Revised paragraph: \"We present the training curves of Algorithm 1 on the AKI dataset in Figures 3 and 4. The performance of the RL policies evaluated on the train, validation, and test sets is illustrated in Figure 3, where we observe that the three curves closely match one another, confirming the generalizability of the learned dynamic classification policy. Additionally, our results indicate that SM-DDPO successfully learns the disease model. The diagnostic classifier, trained from scratch during end-to-end training, maps any partially-observed patient state to a diagnosis or prediction. To focus on classification quality without the influence of dynamic test selection, we evaluated this classifier on static data distributions. Figure 4 demonstrates that the classifier improves its prediction quality during RL training, relying solely on data selected by the RL algorithm. Further details on less critical results can be found in Appendix B.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.05", "instruction": "Make this paragraph heavily more concise, keeping the main ideas.", "revised_paragraph": "Revised paragraph: \"In a supervised setting, positive and negative sets, denoted as C+ and C\u2212, consist of same- and different-category pixels relative to pixel i. This concept, however, fails in weakly- or un-supervised settings where labels are unknown. For weakly-supervised segmentation, only sparsely labeled pixels exist, leading to smaller C+ and C\u2212 sets and reduced learning efficiency.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "atxti8SVk.3K9AmPwALM.05", "instruction": "Concise and improve this explanation to make it more easy to understand.", "revised_paragraph": "Revised paragraph: \"In a supervised setting, we can define positive and negative sets as the pixels that belong to the same or different categories as pixel i, represented as C + and C \u2212. However, this approach is not suitable for weakly or unsupervised settings, where the label for pixel x i is unknown. In weakly-supervised segmentation, we only have a few labeled pixels in the image, leading to smaller sets of C + and C \u2212 and reduced learning efficiency.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "usz0l2mwO.5ie3V0GP-.01", "instruction": "Improve the readablity of the text. Use more concise and straight-forward ideas.", "revised_paragraph": "Revised paragraph: \"NLP (Cherry et al., 2019). Sentence embeddings often contain irrelevant information for the target task. This makes it challenging to separate relevant from irrelevant details when fine-tuning large language models with limited task-specific data, leading to over-fitting. In many real-world applications, collecting enough annotated data to effectively train these large neural models can be difficult and costly.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "usz0l2mwO.5ie3V0GP-.01", "instruction": "Rewrite this paragraph to better fit the academic writing style.", "revised_paragraph": "Revised paragraph: \"Natural Language Processing (NLP) techniques, as discussed by Cherry et al. (2019), often generate sentence embeddings that contain a significant amount of information that is irrelevant to the target task. This proliferation of non-essential information poses challenges in discerning relevant content during the fine-tuning process of language models characterized by a considerable number of parameters, particularly when applied to limited target task datasets, which can lead to overfitting. Furthermore, in many real-world applications, the collection of sufficient annotated data to enable these large neural models to perform optimally can prove to be both challenging and costly.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "S1-LZxvKX.rJ009I8RX.01", "instruction": "Rewrite this paragraph, removing any redundant information for a more concise version.", "revised_paragraph": "Revised paragraph: \"We present a novel systematic method for training sparse models directly without increasing the parameter footprint, achieving performance comparable to post-training compression of dense models. Our dynamic reparameterization method for training convolutional networks significantly surpassed static approaches and existing dynamic sparse reparameterization techniques, all while incurring lower computational costs.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "S1-LZxvKX.rJ009I8RX.01", "instruction": "Fully rewritte this paragraph in a more concise and direct way fitting the academic style.", "revised_paragraph": "Revised paragraph: This study presents the first systematic method for training sparse models without increasing parameter counts throughout the training process, achieving performance comparable to that of post-training compression of dense models at a given sparsity level. We introduced a novel dynamic reparameterization method for training convolutional networks, demonstrating that our approach significantly surpasses static counterparts. Furthermore, our method outperformed existing dynamic sparse reparameterization techniques while incurring substantially lower computational costs.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "I-N2JgVIgy.BNpWofyXgi.00", "instruction": "Remove details about the baseline results and improve the readability.", "revised_paragraph": "Revised paragraph: \"In this subsection, we perform an ablation study to demonstrate the advantages of our low-dimensional contrastive embedding method compared to traditional contrastive embedding approaches. Utilizing the STL-10 and CIFAR-10 datasets, we train various models, including two implementations of CLLR with different regularization techniques: \u21132,1-norm and nuclear norm. All models are trained for 100 and 400 epochs with a consistent batch size and learning rate, and we assess their performance by fine-tuning a linear softmax classifier. Our method focuses on learning embeddings in a low-dimensional space, employing a regularizer to preserve the relevant non-zero columns in the projection matrix L.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "I-N2JgVIgy.BNpWofyXgi.00", "instruction": "Remove sentences that are unnecessary here. Simplify this text a bit.", "revised_paragraph": "Revised paragraph: In this subsection, we conduct an ablation study to compare the low-dimensional contrastive embedding (i.e., our method) with traditional contrastive embedding (i.e., the baseline method). We utilize the STL-10 and CIFAR-10 datasets to train the baseline SimCLR and two versions of CLLR, employing \u21132,1-norm and nuclear-norm regularization. All models are trained for 100 and 400 epochs with consistent batch sizes and learning rates, and we record the test accuracy after fine-tuning a linear softmax. The baseline method learns contrastive embeddings in high-dimensional spaces (dimensions 2048, 3072, and 4096) and a fixed low-dimensional space (dimensions 256 and 512). Additionally, we include baseline results without \u21132,1-norm and nuclear norm constraints (i.e., \u03b1 = 0). Our method focuses on low-dimensional embeddings, applying a regularizer to maintain the relevant non-zero columns in the projection matrix L.", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.18", "instruction": "Rewrite the second sentence of the paragraph and improve the English in the remainder", "revised_paragraph": "Revised paragraph: \"We compare a version of AGILE in which the GAT receives only action features as input, without any information about the state. Consequently, while the decision-making process remains aware of other actions, the learned relationships are fixed and do not vary with the state. As illustrated in Figure 7, there is a decline in performance for both Grid World and CREATE, where the pertinent action relationships fluctuate based on the state. Conversely, RecSim shows no decline in performance because the CPR task solely necessitates knowledge of the most common category, which is independent of the user's state.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.18", "instruction": "Make this paragraph more concise, keeping the main points of each sentence.", "revised_paragraph": "Revised paragraph: \"We compare AGILE's version where the GAT inputs only action features, omitting state information. In this setup, decision-making is informed by fixed action relations, independent of state variations. Figure 7 illustrates performance declines in Grid World and CREATE, where action relations vary by state. In contrast, RecSim shows no performance drop, as the CPR task relies solely on identifying the most common category, independent of user state.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.19", "instruction": "Paraphrase this paragraph using formal language", "revised_paragraph": "Revised paragraph: The results presented in Table 1 indicate that the RDE-Network surpasses all baseline models. Significantly, the RDE-Network enhances per-structure correlations by a considerable margin, suggesting its greater reliability for practical applications. The superiority of the RDE-Network over the MIF-Network demonstrates that representations derived from fitting rotamer densities prove to be more effective than those obtained through masked inverse folding. This is attributed to the fact that protein binding is primarily influenced by atomic interactions, which the RDE-Network effectively captures by modeling the conformations of sidechain atoms.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nCTSF9BQJ.DGhBYSP_sR.19", "instruction": "Revise this paragraph in a more academic style.", "revised_paragraph": "Revised paragraph: \"The results presented in Table 1 indicate that the RDE-Network surpasses all baseline models in performance. Notably, the RDE-Network demonstrates a substantial enhancement in per-structure correlations, suggesting its superior reliability for practical applications. The comparative advantage of the RDE-Network over the MIF-Network underscores the efficacy of representations derived from fitting rotamer densities as opposed to those obtained through masked inverse folding. This distinction is attributable to the fact that protein binding is predominantly influenced by atomic interactions, which the RDE-Network effectively encapsulates through its modeling of sidechain atom conformations.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.06", "instruction": "Make expression concise.", "revised_paragraph": "Revised paragraph: \"This section introduces the greedy learner hypothesis to address challenges in training multi-modal DNNs. We begin by outlining assumptions about multi-modal data and reviewing preliminary observations from the literature on multi-modal learning.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.06", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "Revised paragraph: \"In this section, we present the greedy learner hypothesis to elucidate the challenges encountered in training multi-modal deep neural networks (DNNs). Before detailing our hypothesis, we will first discuss certain assumptions regarding multi-modal data and preliminary observations documented in the literature on multi-modal learning.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "oH-CV7Qprn.l7-CEr3ki.00", "instruction": "Improve the English of this paragraph", "revised_paragraph": "Revised paragraph: \"In comparison to ESTR in [17], \u03b5-FALB in [15], and LowESTR in [ ], our algorithms are specifically designed for a nonlinear reward framework. Relative to LowGLOC in [26], our algorithms achieve an improved regret bound, can accommodate varying action sets, and are computationally efficient. For G-ESTT, we enhance the GLM-UCB algorithms [11] through the application of a regularization technique alongside several novel strategies. Our proposed G-ESTS is straightforward and can be readily implemented based on any state-of-the-art generalized linear bandit algorithms. Notably, when G-ESTS is combined with certain efficient algorithms (e.g., SGD-TS [9]), the total time complexity after a warm-up period scales as O(Tr(d\u2081 + d\u2082)). We demonstrate that G-ESTT and G-ESTS are the first two algorithms capable of achieving the O((d\u2081 + d\u2082)r\u221aT) optimal regret bound for low-rank matrix bandit problems, accounting for logarithmic factors.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "oH-CV7Qprn.l7-CEr3ki.00", "instruction": "Rewrite the first part of the paragraph to make it more convincing.", "revised_paragraph": "Revised paragraph: \"In contrast to the methodologies employed in ESTR [17], (cid:15)-FALB [15], and LowESTR, our algorithms are specifically crafted to operate within a nonlinear reward framework, showcasing a significant advancement in handling complex reward structures. Furthermore, when compared to LowGLOC [26], our algorithms not only demonstrate superior regret bounds but also maintain the flexibility to accommodate varying action sets while ensuring computational efficiency. For the G-ESTT algorithm, we enhance the GLM-UCB approach [11] by integrating a regularization technique along with several innovative methodologies. Our proposed G-ESTS stands out due to its straightforward implementation process, making it adaptable to any state-of-the-art generalized linear bandit algorithms. Notably, when G-ESTS is combined with efficient algorithms such as SGD-TS [9], the overall time complexity following a warm-up period scales optimally as O(Tr(d\u2081 + d\u2082)). Importantly, we establish that G-ESTT and G-ESTS represent the first two algorithms capable of achieving the \u02dcO((d\u2081 + d\u2082)r\u221aT) optimal regret bound for low-rank matrix bandit problems, subject only to logarithmic factors.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.12", "instruction": "Remove unnecessary details for the paragraph.", "revised_paragraph": "Revised paragraph: \"Participants completed the task in varying times for each design: 5.0 seconds for Design A, 6.5 seconds for Design B, and 7.0 seconds for Design C. With Design A, participants had to scroll vertically to see all entries, which led to complaints from several participants about the excessive scrolling. In contrast, Design B featured daily medication summaries at the top, which three participants found helpful. However, some participants noted issues with unclear demarcations between days.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "u9NaukzyJ-.hh0KECXQLv.12", "instruction": "The first sentence is a bit unclear.", "revised_paragraph": "Revised paragraph: \"Participants took an average of 7.0 seconds to complete the task with Design A, 7.0 seconds with Design B, and 7.0 seconds with Design C. To complete this task with Design A, participants were required to vertically scroll to the end of the day to view all entries, which led to complaints from three participants (P3, P5, and P7). For instance, P3 remarked, \u201cI find it\u2019s a lot of scrolling down. It would be helpful if there was a way to condense it or to make it possible to see the entire calendar available in terms of morning, afternoon, and evening.\u201d P8 echoed this sentiment, stating, \u201cThe time frames are a bit big. So it makes, like I said, it really makes it scroll off that you can\u2019t see it all in one consolidated view.\u201d In contrast, participants using Design B were expected to refer to the daily medication summaries provided at the top of the interface. Three participants (P1, P6, and P9) found these daily summaries beneficial for task completion, with P9 commenting, \u201cYeah, I like the idea of having the first row on the calendar dedicated only to the medications that need to be taken. I think it brings an overall idea [of] what should be taken during that day.\u201d However, three participants (P3, P4, and P5) reported difficulties with the clarity of the lines distinguishing the days. For example, P4 stated, \u201cI have a harder time differentiating the calendar component from the days, because there\u2019s not a strong border between the days of the week.\u201d\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "yxeD_Ju-SM.p9Au1Sb-uj.00", "instruction": "Make this paragraph more concise.", "revised_paragraph": "Revised paragraph: \"KG, D RAGON features a cross-modal encoder (GreaseLM) that bidirectionally fuses text-KG pairs (\u00a72.2) and employs a pretraining objective that integrates bidirectional self-supervision (\u00a72.3). This objective combines masked language modeling (MLM) and KG link prediction (LinkPred) to enhance mutual information and joint reasoning between text and KG. We also outline the finetuning process for the pretrained D RAGON model for downstream tasks (\u00a72.4). Although the components of our approach (GreaseLM, MLM, LinkPred) are not novel, our contribution lies in successfully unifying them to create a significantly effective pretrained model (\u00a73, \u00a74).\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "yxeD_Ju-SM.p9Au1Sb-uj.00", "instruction": "Make the last sentence more concise.", "revised_paragraph": "Revised paragraph: \"KG, D RAGON consists of a cross-modal encoder (GreaseLM) that fuses the input text-KG pair bidirectionally (\u00a72.2), and a pretraining objective that performs bidirectional self-supervision on the text-KG input (\u00a72.3). Our pretraining objective unifies masked language modeling (MLM) and KG link prediction (LinkPred) to enable mutual information and joint reasoning between text and KG. Finally, we describe how we finetune the pretrained D RAGON model for downstream tasks (\u00a72.4). While the individual components of our approach (GreaseLM, MLM, LinkPred) are not new, our contribution lies in the effective unification of these elements, demonstrating that this integration leads to a significantly performant pretrained model (\u00a73, \u00a74).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "vrvf56Ug_C.PgzrILJ_er.00", "instruction": "Make this paragraph shorter by eliminating details about further work.", "revised_paragraph": "Revised paragraph: \"Another interesting direction is to improve upper bounds on the pseudo-dimension by restricting heuristic functions to specific classes, as mentioned in Section 2. In Appendix D, we present an example where we achieve polylog(n) upper bounds on the pseudo-dimension by designing heuristic functions with fewer tunable parameters than n.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "vrvf56Ug_C.PgzrILJ_er.00", "instruction": "Delete the sentence about further study and the reference to section 2.", "revised_paragraph": "Revised paragraph: \"Another interesting direction is to improve upper bounds on the pseudo-dimension by restricting heuristic functions to some classes. In Appendix D, we present an illustrative example where we can achieve polylog( n ) upper bounds on the pseudo-dimension by assuming that heuristic functions with significantly fewer tunable parameters than n can be designed in an instance-specific manner.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.14", "instruction": "The second sentence is too complicated. Make it more understandable. Also brush up the rest.", "revised_paragraph": "Revised paragraph: \"Statistical estimators demonstrate strong performance on the PEHE metric. Notably, neural network estimators surpass linear and random forest methods due to their ability to capture nonlinearity in the data. TARNet achieves superior overall performance compared to other statistical estimators by incorporating the strengths of both T-learners and S-learners (R et al., 2019). However, treatment selection bias affects these estimators, resulting in biased outcomes and sub-optimal performance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "OV5v_wBMHk.bw4cqlpLh.14", "instruction": "Reorganize the ideas in the sentences to improve the logical flow of the text.", "revised_paragraph": "Revised paragraph: \"Statistical estimators exhibit competitive performance on the PEHE metric, but treatment selection bias can lead to sub-optimal performance and introduce bias in these estimators. In particular, neural network estimators outperform linear and random forest methods due to their ability to capture nonlinearity in data. Among these, TARNet achieves better overall performance than other statistical estimators by leveraging the advantages of both the T-learner and S-learner (R et al., 2019).\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.19", "instruction": "Improve the English of this paragraph and make it shorter.", "revised_paragraph": "Revised paragraph: \"AGILE-Tuned without sync-freq-change: Mnih et al. (2015) addressed the challenge of a rapidly changing target in Q-network updates by using periodic syncing between the target and main networks. In this study, we examined two extreme syncing frequencies: 10 (Sync-freq=10 in Fig. 13 (a)) and 500 (AGILE-Tuned). AGILE-Tuned without graph-dim-change: To explore the challenges of expressing action relations in a compact representation, we compared larger and smaller action graphs, with node features encoded in 32 (Graph-dim=32) and 64 (AGILE-Tuned) dimensions.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.19", "instruction": "Make the wording of this paragraph much more straight forward, to be more consice.", "revised_paragraph": "Revised paragraph: \"\u2022 AGILE-Tuned without sync-freq-change: Mnih et al. (2015) introduced periodic syncing between the target and main networks to address the challenge of a moving target in Q-network updates. In this study, we examined two extreme syncing periods: 10 (Sync-freq=10 in Fig. 13 (a)) and 500 (AGILE-Tuned). \u2022 AGILE-Tuned without graph-dim-change: To investigate the difficulty of representing action relations compactly, we compared large and small representations in the action graph, specifically using node-features encoded in 32 (Graph-dim=32) or 64 (AGILE-Tuned) dimensions.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Sx6SnclSL.nQLOUHvx8n.04", "instruction": "Rewrite the last sentence to make it more concise.", "revised_paragraph": "Revised paragraph: \"Hierarchical Modules. As reported in Table 7, in addition to our final solution of Point-M2AE in the first row, we experiment with removing the hierarchical encoder, hierarchical decoder, skip connections, and local spatial self-attention layers from our framework. Specifically, we replace our encoder and decoder with 1-stage plain architectures similar to MAE, which contain 15 and 2 blocks of vanilla self-attention layers, respectively. We observe that omitting the multi-stage structures in either the encoder or decoder negatively impacts performance, with the hierarchical encoder proving more beneficial than the decoder. Furthermore, skip connections and local spatial attention enhance the network by supplying complementary information and local inductive bias.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Sx6SnclSL.nQLOUHvx8n.04", "instruction": "Make the paragraph shorter.", "revised_paragraph": "Revised paragraph: \"Hierarchical Modules. As shown in Table 7, we tested our final Point-M2AE solution against variants with the hierarchical encoder, hierarchical decoder, skip connections, and local spatial self-attention layers removed. We replaced the encoder and decoder with 1-stage plain architectures similar to MAE, consisting of 15 and 2 blocks of vanilla self-attention layers, respectively. The results indicate that removing multi-stage structures from either the encoder or decoder negatively impacts performance, with the hierarchical encoder being more beneficial than the decoder. Additionally, skip connections and local spatial attention significantly enhance the network by providing complementary information and local inductive bias.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hAi0PMz9T7.Ut8ESfYp1.01", "instruction": "Make the last sentence slightly shorter.", "revised_paragraph": "Revised paragraph: \"Conventional TCP CC adopts a heuristic-based approach where the heuristic functions are manually crafted to adjust the traffic rate in a deterministic manner. Some proposals use packet loss as a signal for network congestion, e.g., Cubic [4], Reno [20], and NewReno [6], while others rely on delay variation, e.g., Vegas [5]. Other CC designs combine packet loss and delay [21, 22]. Recently, various CC techniques specialized for data-center networks have also been proposed [2, 3, 23].\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hAi0PMz9T7.Ut8ESfYp1.01", "instruction": "Make the paragraph slightly shorter.", "revised_paragraph": "Revised paragraph: \"Conventional TCP congestion control (CC) employs a heuristic-based approach with manually crafted functions to adjust traffic rates deterministically. Some methods, like Cubic, Reno, and NewReno, use packet loss as a signal for congestion, while others, such as Vegas, rely on delay variations. Additionally, some CC designs integrate both packet loss and delay. Recently, specialized CC techniques for data-center networks have also been proposed.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.01", "instruction": "Make the beginning of this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"This work addresses the gap in studying state space topological properties as completeness thresholds for cost-optimal planning with zero-cost actions. We identify the sublist diameter as a practically bounded completeness threshold and introduce the subset diameter as a new threshold, demonstrating that no tighter threshold can be computed without utilizing cost information, the initial state, or the goal. To test the practical utility of these thresholds, we develop a SAT compilation for cost-optimal planning, implementing it in an anytime planning as satisfiability algorithm with a fixed horizon at the completeness threshold. The algorithm begins with an upper bound on total cost and refines it in each iteration. Experiments reveal that the algorithm often computes plans with costs better than the initial estimates and frequently matches optimal costs. Additionally, it successfully proves the optimality of certain costs for multiple instances that the widely used LM-cut (Pommerening and Helmert 2012) planning heuristic could not.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.01", "instruction": "Make the beginning of this paragraph shorter.", "revised_paragraph": "Revised paragraph: \"This work addresses the gap in studying the suitability of various state space topological properties as completeness thresholds for cost optimal planning with 0-cost actions. We identify the sublist diameter as a practically bounded completeness threshold and introduce the subset diameter as a new completeness threshold, demonstrating that no tighter threshold can be computed without exploiting cost information, the initial state, or the goal. To test the practical utility of these completeness thresholds, we devise a SAT compilation for cost optimal planning, which is applied in an anytime planning as satisfiability algorithm with a fixed horizon set to the completeness threshold. This algorithm begins with an upper bound on total cost and iteratively improves this bound. Experiments reveal that the algorithm computes plans with costs better than the initial estimates, often achieving the optimal cost, and it proves optimality for several instances that the widely used LM-cut (Pommerening and Helmert 2012) heuristic could not.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "9wfZbn73om.FhHH15YtKt.01", "instruction": "Make the sentence precise.", "revised_paragraph": "Revised paragraph: \"In summary, our contributions are: 1) the introduction of a novel (\u03c3, \u03b4)-measure for quantifying data augmentation; 2) the development of a theoretical framework for contrastive self-supervised learning (SSL) that identifies alignment, divergence, and concentration as critical for generalization ability; 3) the provable verification that both the InfoNCE loss and cross-correlation loss meet the criteria for alignment and divergence; and 4) the empirical demonstration that concentration concerning the proposed augmented distance correlates significantly with downstream performance.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "9wfZbn73om.FhHH15YtKt.01", "instruction": "Improve english in this text.", "revised_paragraph": "Revised paragraph: \"In summary, our contributions are as follows: 1) we propose a novel (\u03c3, \u03b4)-measure to quantify data augmentation; 2) we present a theoretical framework for contrastive semi-supervised learning (SSL), suggesting that alignment, divergence, and concentration are key factors influencing generalization ability; 3) we provide provable evidence that both the InfoNCE loss and the cross-correlation loss meet the criteria for alignment and divergence; and 4) we empirically demonstrate that concentration with respect to the proposed augmented distance is closely related to downstream performance.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.13", "instruction": "Replace some words in the paragraph", "revised_paragraph": "Revised paragraph: \"We noted the primary effect of W (F2, 22 = 25.3, p < 0.001, \u03b7\u00b2p = 0.967) (Figure 4 (i)). Pairwise comparisons indicated that the error rates rose as W diminished. The remaining parameters did not exhibit significant main effects. No noteworthy interaction was detected.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.13", "instruction": "Slightly revise for readability.", "revised_paragraph": "Revised paragraph: \"We observed a significant main effect of W (F(2, 22) = 25.3, p < 0.001, \u03b7\u00b2_p = 0.967) (Figure 4(i)). Pairwise comparisons indicated that error rates increased as W decreased. No main effects were found for the other parameters, and no significant interactions were observed.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "F3z0hchpGy.xeuzrNJiNW.03", "instruction": "Rephrase the paragraph", "revised_paragraph": "Revised paragraph: \"Since all figures in the FAUST dataset share similar meshing and orientation, disrupting gauge equivariance in the upper layers can prove advantageous. Weiler and Cesa (2019) demonstrate that symmetry can be disrupted by considering non-invariant features as invariant when inputting them into the final 1 \u00d7 1 convolution. This approach allows for equivariance at lower levels while facilitating orientation sensitivity at the higher layers.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "F3z0hchpGy.xeuzrNJiNW.03", "instruction": "Rephrase the first sentence.", "revised_paragraph": "Revised paragraph: \"Given that all figures in the FAUST data set share similar meshing and orientation, disrupting gauge equivariance in the deeper layers may prove advantageous. As demonstrated by Weiler and Cesa (2019), symmetry may be disrupted by considering non-invariant features as invariant inputs to the final 1 \u00d7 1 convolution. This architectural approach retains equivariance at lower levels while permitting orientation sensitivity in higher layers.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "p8yrWJS4W.eHA5NswPr.00", "instruction": "Fix formatting issues and simplify the wording of the paragraph.", "revised_paragraph": "Revised paragraph: \"In practice, we do not have access to the true distribution \\( p_w \\). Instead, we typically use a corpus \\( \\{ w_{p_w}^{(n)} \\}_{n=1}^N \\), which we assume consists of independent and identically distributed (i.i.d.) samples from \\( p_w \\). To address this limitation, we often derive a statistical estimator \\( \\hat{\\Delta} \\) to approximate \\( \\Delta \\) based on this corpus. There are two main strategies for constructing such estimators: Monte Carlo estimation and plug-in estimation. **3.2. Monte Carlo Estimation** Our assumption of i.i.d. samples \\( w_{p_w} \\sim p_w \\) enables us to create a Monte Carlo estimator for certain divergences. We begin with the forward Kullback-Leibler (KL) divergence, which is relevant for both \\( \\Delta \\rightarrow \\) and \\( \\Delta_{\\text{exp}} \\):\u201d", "type_approach": "instruction-annot_1"}
{"id_paragraph": "p8yrWJS4W.eHA5NswPr.00", "instruction": "Fix the caplocks problem. Slightly shorten the paragraph.", "revised_paragraph": "Revised paragraph: \"In practice, we do not have access to the true distribution pw. Instead, we typically work with a corpus {w_p, w_n}^N_{n=1}, where we assume instances are sampled i.i.d. from pw. A common approach to address this issue is to derive a statistical estimator \u014c that uses this corpus to approximate \u2206. Two prevalent strategies for building such estimators are Monte Carlo estimation and plug-in estimation. The i.i.d. assumption regarding samples w_p \u223c pw enables us to derive a Monte Carlo estimator for certain divergences, starting with the forward KL divergence, which is relevant for both \u2206 \u2192 and \u2206 exp.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "nkOpNqg-ip.OwJsIhe_p.00", "instruction": "Edit some formulations to sound more neutral.", "revised_paragraph": "Revised paragraph: It is noteworthy that the baselines employed to evaluate the performance of AutoML tools are predominantly other AutoML tools, with few \u201csimple\u201d baselines included. A straightforward baseline could involve replicating the steps taken by a human data scientist, suggesting that such an approach warrants consideration as a baseline. In the absence of these simple baselines, the understanding of how AutoML tools improve upon ad-hoc techniques is limited, resulting in a focus primarily on their relative performance to one another. To our knowledge, the only study that incorporates such baselines is Thornton et al. (2013), which utilizes the Exhaustive-Default (\u201cEx-def\u201d) baseline that identifies the default parametrized model yielding the best results in cross-validation. They also examine a grid search; however, this method may not be practical in all scenarios.", "type_approach": "instruction-annot_1"}
{"id_paragraph": "nkOpNqg-ip.OwJsIhe_p.00", "instruction": "Make the beginning of the paragraph shorter.", "revised_paragraph": "Revised paragraph: \"The baselines used to assess AutoML tool performance are typically limited to other AutoML tools, neglecting simpler baselines. For instance, a basic baseline could involve mimicking the steps a human data scientist would take, warranting its consideration. Without these baselines, we fail to understand how AutoML tools enhance ad-hoc techniques, only comparing them against each other. To our knowledge, the only study incorporating such baselines is Thornton et al. (2013), which utilizes the Exhaustive-Default (\u201cEx-def\u201d) baseline\u2014selecting the best default parametrized model from cross-validation. They also mention grid search, though this method is impractical.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.14", "instruction": "Modify the first sentence", "revised_paragraph": "Revised paragraph: \"A significant main effect of Position was identified, with Position = Inside exhibiting a longer movement time compared to Position = Outside. We observed a significant interaction of I \u00d7 Position. At I = 0, the movement time increased in comparison to the condition with no notch by approximately 11.8% for Position = Inside and approximately 4.93% for Position = Outside.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "hegI87bI5S.fL6Q48sfx8.14", "instruction": "Revise this text to make it more readable and direct.", "revised_paragraph": "Revised paragraph: \"We found a main effect of Position, with movements taking longer when the Position was Inside compared to Outside. Additionally, there was a significant interaction between I and Position. When I = 0, movement time increased by approximately 11.8% for the Inside position and by about 4.93% for the Outside position compared to the condition with no notch.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.03", "instruction": "I want to get rid of unnecessary details.", "revised_paragraph": "Revised paragraph: \"Existing methods for computing the recurrence diameter have a doubly exponential worst-case running time (Kroening and Strichman 2003; Abdulaziz and Berger 2021) and are limited to small abstractions for upper bounds on topological properties. Additionally, there is no compositional algorithm that uses abstractions' recurrence diameters to compute upper bounds (Abdulaziz 2017)[Chapter 3, Theorem 2]. Consequently, the lack of a practical computation method renders the recurrence diameter ineffective as a completeness threshold.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "BkxG1CvhWf.wcpE7maMLZ4.03", "instruction": "Remove the references in the second half of the paragraph. Reorder the last sentence to improve readability.", "revised_paragraph": "Revised paragraph: \"Practically, the existing methods to compute the recurrence diameter have a doubly exponential worst-case running time, and they are only useful when applied to small abstractions in the context of compositionally computing upper bounds on other topological properties. Furthermore, there is not a compositional algorithm that can compute upper bounds on the recurrence diameter using abstractions\u2019 recurrence diameters. Accordingly, the absence of a practical way to compute or tightly bound the recurrence diameter renders it ineffective as a completeness threshold.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "Byyb66j52G.hR5KKRfhQm.14", "instruction": "Use clearer expression, use concise words.", "revised_paragraph": "Revised paragraph: \"Delayed Augmentation. This study investigates generalization when augmentation is introduced late, specifically at 10M and 20M. As illustrated in Figures 2(d) and 2(e), generalization improves significantly after implementing augmentation at these stages. Despite the late introduction of augmentation, it still enhances generalization, irrespective of timing. Golatkar et al. [9] demonstrate that delayed augmentation is less effective than consistent augmentation during the entire training process in supervised learning. However, in our findings, the performance of (10, 25) with delayed augmentation is comparable to (0, 25), which uses augmentation throughout the training, differing from the outcomes in supervised learning. When augmentation proves particularly beneficial, as seen in Figure 2(e), delayed augmentation struggles to match the performance of earlier strategies shown in Figure 2(f). This disparity arises because reinforcement learning (RL) gradually enhances the policy and trajectory due to the Markov property. Additionally, since RL has a limited sample size compared to supervised learning, starting augmentation from the outset is crucial when it aids training.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "Byyb66j52G.hR5KKRfhQm.14", "instruction": "Revise this paragraph to use clearer and more precise words.", "revised_paragraph": "Revised paragraph: \"Delayed Augmentation. We investigate the impact of implementing augmentation at later stages, specifically at 10 million (10M) and 20 million (20M) iterations. As illustrated in Figure 2(d) and Figure 2(e), generalization performance significantly improves after the introduction of augmentation at both 10M and 20M. Although augmentation is applied later in the training process, it consistently enhances generalization regardless of the timing. Golatkar et al. [9] argue that delayed augmentation does not achieve the same effectiveness as applying augmentation throughout the entire training period in supervised learning. However, in our findings, models using delayed augmentation at (10, 25) demonstrate generalization comparable to those using continuous augmentation at (0, 25), which is atypical compared to supervised learning. Notably, when augmentation substantially benefits training, as seen in Figure 2(e), delayed augmentation struggles to maintain pace with earlier augmentation models, depicted in Figure 2(f), since reinforcement learning (RL) improves policy and trajectory over time due to the Markov property. Additionally, RL is constrained by a limited number of samples, making the initial implementation of augmentation more critical compared to supervised learning when augmentation proves to be beneficial.\"", "type_approach": "instruction-annot_2"}
