{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"depth": 1, "intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"depth": 1, "intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"depth": 1, "intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "revisions": [{"depth": 1, "intention": "Rewritting_heavy", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "instruction": "Rearrange the structure to make the structure clearer.", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "instruction": "Rewrite this paragraph completely to make it clearer.", "revised_paragraph": "\"We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The first step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"depth": 1, "intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"depth": 1, "intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"depth": 1, "intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "revisions": [{"depth": 1, "intention": "Content_deletion", "revised_paragraph": "\"Generalization of meta learning. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "instruction": "Remove a redundant sentence. Use clearer expression.", "revised_paragraph": "\"Generalization of meta learning. The excess risk, as a metric of generalization ability of gradient-based meta learning has been analyzed recently [3,4,9,14,18,42]. The generalization of meta learninghas been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning [13,16]. Information theoretical bounds have been proposed in [10,26], which bound thegeneralization error in terms of mutual information between the input training data and the output of the meta-learning algorithms. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "instruction": "Improve the English and remove the second sentence.\t", "revised_paragraph": ": \"Generalization of meta learning. The excess risk, as a metric of generalization ability of gradient-based meta learning has been analyzed recently [3,4,9,14,18,42]. The generalization of meta learning has been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based meta learning [13,16]. Information theoretical bounds have been proposed in [10,26], which bound thegeneralization error in terms of mutual information between the input training data and the output of the meta-learning algorithms. Recently, overparameterized meta learning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity for the method of moment estimator.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "instruction": "Make expression concise, add conjuction, include all citations.", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "instruction": "Rewrite this paragraph to make it clearer.", "revised_paragraph": "\"This definition of  (  ;  ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015), the update of the direction of , i.e.,  / ||  || 22, reflects how much the update on  changes the model f in order to fit the batch of samples.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "instruction": "Make expression concrete, correct typos.", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermark learning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "instruction": "Improve the English of this paragraph.", "revised_paragraph": "\"The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermark learning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8. 60 and 4. 70 improvements in FPR95 for the softmax and the free energy scoring.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "revisions": [{"depth": 1, "intention": "Rewritting_medium", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "instruction": "Improved the writing and reformulate the third sentence", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "instruction": "Improved the writing and reformulate the third sentence", "revised_paragraph": "\"Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 , 180 , 270  and flipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48  48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014) with  1 =0.9,  2 =0.999, and (cid:15) = 10  8. We set the initial learning rate as 10  4 and then decrease it to half every 2  10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"intention": "Content_deletion", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"intention": "Rewritting_light", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"intention": "Content_deletion", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"intention": "Rewritting_light", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"intention": "Content_deletion", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"intention": "Content_deletion", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"depth": 1, "intention": "Content_deletion", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"depth": 2, "intention": "Rewritting_light", "revised_paragraph": "\"\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\"\""}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"depth": 1, "intention": "Content_deletion", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"depth": 2, "intention": "Rewritting_light", "revised_paragraph": "\"\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\"\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"depth": 2, "intention": "Content_deletion", "revised_paragraph": "\"\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\"\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\""}, {"depth": 2, "intention": "Content_deletion", "revised_paragraph": "\"\"Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\"\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "instruction": "Rewrite this paragraph in a more formal style and remove any unnecessary details.", "revised_paragraph": "\"Visualization of Pruning Process.\" To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\"", "type_approach": "instruction-annot_1"}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "instruction": "Rewrite this paragraph in a more formal style and remove the last sentence", "revised_paragraph": "Visualization of Pruning Process. To figuratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of filters in two layers of EDSR baseline during SRP training. The filters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned filters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept filters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover itself, akin to the compensation effect in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.\"", "type_approach": "instruction-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"intention": "Rewritting_light", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the list all at once is not feasible due to the intractably large number of possible lists. Therefore, the key is to build the list gradually, one action at a time. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}], "type_approach": "separate-labels-union"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"intention": "Rewritting_light", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the list all at once is not feasible due to the intractably large number of possible lists. Therefore, the key is to build the list gradually, one action at a time. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}], "type_approach": "separate-labels-intersection"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"intention": "Rewritting_light", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the list all at once is not feasible due to the intractably large number of possible lists. Therefore, the key is to build the list gradually, one action at a time. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}], "type_approach": "separate-labels-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"intention": "Rewritting_light", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the list all at once is not feasible due to the intractably large number of possible lists. Therefore, the key is to build the list gradually, one action at a time. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}], "type_approach": "separate-labels-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"depth": 1, "intention": "Rewritting_light", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the list all at once is not feasible due to the intractably large number of possible lists. Therefore, the key is to build the list gradually, one action at a time. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"depth": 2, "intention": "Concision", "revised_paragraph": "\"\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the list all at once is not feasible due to the intractably large number of possible lists. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. Algorithm 1 provides complete details on listwise RL."}], "type_approach": "iterative-labels-union"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"depth": 1, "intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"depth": 2, "intention": "Rewritting_light", "revised_paragraph": "\"\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\"\""}], "type_approach": "iterative-labels-intersection"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"depth": 1, "intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"depth": 2, "intention": "Rewritting_light", "revised_paragraph": "\"\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\"\""}], "type_approach": "iterative-labels-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "revisions": [{"depth": 1, "intention": "Concision", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\""}, {"depth": 2, "intention": "Rewritting_light", "revised_paragraph": "\"\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. Algorithm 1 provides complete details on listwise AGILE.\"\""}], "type_approach": "iterative-labels-annot_2"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "instruction": "Make first sentence more concise. Rewrite phrases, prefer short formulations and avoid we.", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a).", "type_approach": "instruction-annot_1"}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "instruction": "Make first sentence more concise. Rewrite phrases, prefer short formulations and avoid we.", "revised_paragraph": "\"Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a).", "type_approach": "instruction-annot_2"}
