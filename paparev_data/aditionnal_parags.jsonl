{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.03", "parag_1": "We implement the fusion module as a multi-modal transfer module (MMTM) (Joze et al., 2020). The ﬁrst step in MMTM is to squeeze feature maps from each uni-modal branch to vector representations via global average pooling  over spatial dimensions. Next we concatenate these representations and applya linear transformation to obtain cross-modal context representation. We predict channel-wise weights for each modality based this context representation through two independent fully-connected layers. Finally, these weights are used tore-calibrate the channel-wise features per modality.", "parag_2": "We implement every fusion module by a multi-modal transfer module (MMTM) (Joze et al., 2020). Each MMTM connects two layers from the two uni-modal branches. There is ﬁrst the global average pooling applied over spatial dimensions to transform feature maps into a vector. We concatenate the two vectors and apply linear transformation. We refer to its output as context representation. Next, for each uni-modal branch, we implement a fully connected layer on the context representation and get a vector with a dimension of the number of feature maps. Feature maps are re-scaled by this vector before passing to the next layer of the uni-modal branch.", "annot_1": {"annotation": ["Rewritting_heavy"], "instruction": ["Rearrange the structure to make the structure clearer."], "annotator": "Jiahao_Huang"}, "annot_2": {"annotation": ["Rewritting_heavy"], "instruction": ["Rewrite this paragraph completely to make it clearer."], "annotator": "Florian_Boudin"}}
{"id_paragraph": "x8CcXI4Ei.4yg90qT46L.01", "parag_1": "Generalization of meta learning. The excess risk , as a metric of generalization ability of gradientbased meta learning has been analyzed recently [3,4,9,14,18,42]. The generalization of meta learninghas been studied in [27] in the context of mixed linear regression, where the focus is on investigatingwhen abundant tasks with small data can compensate for lack of tasks with big data. Generalization performance has also been studied in a relevant but different setting - representation based metalearning [13,16]. Information theoretical bounds have been proposed in [10,26], which bound thegeneralization error in terms of mutual information between the input training data and the output of the meta-learning algorithms. The PAC-Bayes framework has been extended to meta learning to provide a PAC-Bayes meta-population risk bound [1,15,19,34]. These works mostly focus on the case where the meta learning model is underparameterized; that is, the total number of meta training data from all tasks is larger than the dimension of the model parameter. Recently, overparameterized metalearning has attracted much attention. Bernacchia [6] suggests that in overparameterized MAML,negative learning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [39] shows that the optimal representation in representation-based meta learning isoverparameterized and provides sample complexity  for the method of moment estimator.", "parag_2": "Generalization of meta learning. The excess risk , as a metric of generalization ability of nestedmeta learning has been analyzed recently [3,4,9,13,17,41]. Generalization performance has also been studied in a relevant but different setting - representation based meta learning [12,15]. Informationtheoretical generalization bounds have been proposed in [10,25], which bound the generalization errorin terms of mutual information between the input training data and the output of the meta-learning algorithms. The PAC-Bayes framework has been extended to meta learning to provide a PAC-Bayes meta-population risk bound [1,14,18,32]. These works mostly focus on the case where the meta learning model is underparameterized; that is, the total number of meta training data from all tasks is larger than the dimension of the model parameter. Recently, overparameterized meta learninghas attracted much attention. Bernacchia [6] suggests that in overparameterized MAML, negativelearning rate in the inner loop is optimal during meta training for linear models with Gaussian data. Sun et al. [37] shows that the optimal representation in representation-based meta linear regression isoverparameterized and provides sample complexity bounds for the method of moment estimator.", "annot_1": {"annotation": ["Content_deletion"], "instruction": ["Remove a redundant sentence. Use clearer expression."], "annotator": "Jiahao_Huang"}, "annot_2": {"annotation": ["Content_deletion"], "instruction": ["Improve the English and remove the second sentence.\t"], "annotator": "Florian_Boudin"}}
{"id_paragraph": "7_CwM-IzWd.zcm6f5HDI.09", "parag_1": "This deﬁnition of µ ( θ ; · ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021). Previous studies found that when normalization techniques are applied, such as batch normalization (Ioffe & Szegedy, 2015),  the update of the direction of  θ ,  i.e., θ / || θ || 22 , reﬂects how much the update on θ changes the model f in order to ﬁt the batch of samples.", "parag_2": "This deﬁnition of µ ( θ ; i ) is inspired by discussion on the effective update of parameters in the literature (Van Laarhoven, 2017; Zhang et al., 2019; Brock et al., 2021; Hoffer et al., 2018). When normalization techniques, such as batch normalization (Ioffe & Szegedy, 2015), are applied to the DNNs, the key property of the weight vector, θ , is its direction, i.e., θ / || θ || 22 . Thus, we measure the update on θ using the norm of its gradient normalized by its norm.", "annot_1": {"annotation": ["Rewritting_medium"], "instruction": ["Make expression concise, add conjuction, include all citations."], "annotator": "Jiahao_Huang"}, "annot_2": {"annotation": ["Rewritting_medium"], "instruction": ["Rewrite this paragraph to make it clearer."], "annotator": "Florian_Boudin"}}
{"id_paragraph": "sK_VkqBv2X.rJUJNwFOc.03", "parag_1": "The experimental results are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common watermarklearning setup (common) can already lead to improved performance in near-OOD detection compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shiftingaugmentations (perm and rotate) can further boost the detection power of the models, leading to atmost 8 . 60 and 4 . 70 improvements in FPR95 for the softmax and the free energy scoring.", "parag_2": "The near OOD experiments are summarized in Table 5, where we take CIFAR- 10 as ID data and CIFAR- 100 as OOD data. Here, the common learning setup (common) already leads to improved performance compared to the cases without watermarking (w/o watermark). Moreover, watermarking with shifting augmentations (permute and rotate) can further boost the detection power of the models, leading to at most 8 . 60 and 4 . 70 improvements in FPR95 for the softmax and the free energy scoring.", "annot_1": {"annotation": ["Rewritting_light"], "instruction": ["Make expression concrete, correct typos."], "annotator": "Jiahao_Huang"}, "annot_2": {"annotation": ["Rewritting_light"], "instruction": ["Improve the English of this paragraph."], "annotator": "Florian_Boudin"}}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.16", "parag_1": "Training Settings. Following (Zhang et al., 2018b), we perform data augmentation on the training images, which are randomly rotated by 90 ◦ , 180 ◦ , 270 ◦ and ﬂipped horizontally. Each training batch consists of 16 LR color patches, whose sizeis 48 × 48. Our SRP model is trained by ADAM optimizer (Kingma & Ba, 2014)  with β 1 =0.9, β 2 =0.999, and (cid:15) = 10 − 8 . We set the initial learning rate as 10 − 4 and then decrease it to half every 2 × 10 5 iterations of back-propagation. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU 2 .", "parag_2": "Training Settings. Following (Zhang et al., 2018b), data augmentation is used in training – training images are randomly rotated by 90 ◦ , 180 ◦ , 270 ◦ and ﬂipped horizontally. Image patches (patch size 48 × 48) are cropped out to form each training batch. Adam optimizer (Kingma & Ba, 2014) is adopted for training with β 1 =0.9, β 2 =0.999, and (cid:15) = 10 − 8 . Initial learning rate is set to 10 − 4 and then decayed by factor 0 . 5 every 2 × 10 5 iterations. We use PyTorch (Paszke et al., 2017) to implement our models with a Tesla V100 GPU † .", "annot_1": {"annotation": ["Rewritting_medium"], "instruction": ["Improved the writing and reformulate the third sentence"], "annotator": "Juan_Junqueras"}, "annot_2": {"annotation": ["Rewritting_medium"], "instruction": ["Improved the writing and reformulate the third sentence"], "annotator": "Jonas_Luhrs"}}
{"id_paragraph": "aomiOZE_m2.rxb2TiQ6bq.18", "parag_1": "Visualization of Pruning Process . To ﬁguratively understand how SRP works, in Fig. 3 we plot the mean L 1 -norm of ﬁlters in two layers of EDSR baseline during SRP training. The ﬁlters are split into two groups, pruned and kept. As seen, the mean L 1 -norm of the pruned ﬁlters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept ﬁlters arise themselves (we do not have any regularization term to encourage them to grow larger). It means the network learns to recover  itself , akin to the compensation effect  in human brain (Duffau et al., 2003). We provide more visualization results in the appendix.", "parag_2": "Visualization of Pruning Process . In Fig. 3, we visualize the pruning process by plotting the mean L 1 -norm of ﬁlters in two layers of EDSR baseline during SRP training. The ﬁlters are split into two groups, kept ﬁlters and pruned ﬁlters. As is shown in the ﬁgure, the mean L 1 -norm of the pruned ﬁlters goes down gradually because the penalty grows stronger and stronger, driving them towards zero. Interestingly, note the L 1 -norms of the kept ﬁlters arise themselves. Recall that there is no explicit regularization to promote them to grow. In other words, the network learns to recover by itself , akin to the compensation effect found in human brain (Duffau et al., 2003).", "annot_1": {"annotation": ["Rewritting_light","Content_deletion"], "instruction": ["Rewrite this paragraph in a more formal style and remove any unnecessary details."], "annotator": "Juan_Junqueras"}, "annot_2": {"annotation": ["Rewritting_light","Content_deletion"], "instruction": ["Rewrite this paragraph in a more formal style and remove the last sentence"], "annotator": "Jonas_Luhrs"}}
{"id_paragraph": "MXi6uEx-hp.rdZfFcGyf9.06", "parag_1": "Listwise RL (CDQN) : To solve the combinatorial action space problem of listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the  list all at once is not feasible due to the intractably large number of possible lists. Therefore, the key is to build the list incrementally, one action at a time. Thus, each list index can be treated as an individual non-combinatorial action which can be trained with RL. We replace the Q-network of CDQN with AGILE in order to accommodate a varying action space. We share the weights of the cascaded Q-networks. Algorithm 1 provides complete details on  listwise AGILE.", "parag_2": "Listwise RL (CDQN) : For tasks with listwise actions, we follow the Cascaded DQN (CDQN) framework of Chen et al. (2019a). The main challenge is that building the action list all at once is not feasible due to a combinatorial number of possible list-actions. Therefore, the key is to build the list incrementally, one action at a time. Thus, each list index can be treated as an individual action decision trained with independent Q-networks. We replace the Q-network of CDQN with AGILE to support a varying action space. Sharing the weights of the cascaded Q-networks led to better performance. Algorithm 1 provides complete details on CDQN for listwise AGILE.", "annot_1": {"annotation": ["Concision", "Rewritting_light"], "instruction": ["Make first sentence more concise. Rewrite phrases, prefer short formulations and avoid we."], "annotator": "Julian_Schnitzler"}, "annot_2": {"annotation": ["Concision", "Rewritting_light"], "instruction": ["Make first sentence more concise. Rewrite phrases, prefer short formulations and avoid we."], "annotator": "Juan_Junqueras"}}
